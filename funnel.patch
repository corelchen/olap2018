diff --git a/cmake/find_llvm.cmake b/cmake/find_llvm.cmake
index e540c3eaf..d9805b033 100644
--- a/cmake/find_llvm.cmake
+++ b/cmake/find_llvm.cmake
@@ -1,8 +1,5 @@
-# Broken in macos. TODO: update clang, re-test, enable
-if (NOT APPLE)
-    option (ENABLE_EMBEDDED_COMPILER "Set to TRUE to enable support for 'compile' option for query execution" 1)
-    option (USE_INTERNAL_LLVM_LIBRARY "Use bundled or system LLVM library. Default: system library for quicker developer builds." ${APPLE})
-endif ()
+option (ENABLE_EMBEDDED_COMPILER "Set to TRUE to enable support for 'compile' option for query execution" 1)
+option (USE_INTERNAL_LLVM_LIBRARY "Use bundled or system LLVM library. Default: system library for quicker developer builds." ${APPLE})
 
 if (ENABLE_EMBEDDED_COMPILER)
     if (USE_INTERNAL_LLVM_LIBRARY AND NOT EXISTS "${ClickHouse_SOURCE_DIR}/contrib/llvm/llvm/CMakeLists.txt")
diff --git a/contrib/boost-cmake/README b/contrib/boost-cmake/README
index 69b266c7c..7eec9460a 100644
--- a/contrib/boost-cmake/README
+++ b/contrib/boost-cmake/README
@@ -3,7 +3,7 @@ Motivation
 
 For reproducible build, we need to control, what exactly version of boost we build,
  because different versions of boost obviously have slightly different behaviour.
-You may already have installed arbitrary version of boost on your system, to build another projects.
+You may already have installed arbitary version of boost on your system, to build another projects.
 
 We need to have all libraries with C++ interface to be located in tree and to be build together.
 This is needed to allow quickly changing build options, that could introduce changes in ABI of that libraries.
diff --git a/contrib/libsparsehash/README b/contrib/libsparsehash/README
index 527cfa1d3..8c806b42a 100644
--- a/contrib/libsparsehash/README
+++ b/contrib/libsparsehash/README
@@ -1,7 +1,7 @@
 This directory contains several hash-map implementations, similar in
 API to SGI's hash_map class, but with different performance
 characteristics.  sparse_hash_map uses very little space overhead, 1-2
-bits per entry.  dense_hash_map is very fast, particularly on lookup.
+bits per entry.  dense_hash_map is very fast, particulary on lookup.
 (sparse_hash_set and dense_hash_set are the set versions of these
 routines.)  On the other hand, these classes have requirements that
 may not make them appropriate for all applications.
diff --git a/dbms/CMakeLists.txt b/dbms/CMakeLists.txt
index 0cb5824e1..9f2e2b5be 100644
--- a/dbms/CMakeLists.txt
+++ b/dbms/CMakeLists.txt
@@ -84,6 +84,7 @@ add_headers_and_sources(dbms src/Storages/Kafka)
 add_headers_and_sources(dbms src/Storages/MergeTree)
 add_headers_and_sources(dbms src/Client)
 add_headers_and_sources(dbms src/Formats)
+add_headers_and_sources(dbms src/Compression)
 
 list (APPEND clickhouse_common_io_sources ${CONFIG_BUILD})
 list (APPEND clickhouse_common_io_headers ${CONFIG_VERSION} ${CONFIG_COMMON})
diff --git a/dbms/cmake/version.cmake b/dbms/cmake/version.cmake
index e62a8e479..d0e8bdaea 100644
--- a/dbms/cmake/version.cmake
+++ b/dbms/cmake/version.cmake
@@ -2,10 +2,10 @@
 set(VERSION_REVISION 54409 CACHE STRING "")
 set(VERSION_MAJOR 18 CACHE STRING "")
 set(VERSION_MINOR 14 CACHE STRING "")
-set(VERSION_PATCH 9 CACHE STRING "")
-set(VERSION_GITHASH 457f8fd495b2812940e69c15ab5b499cd863aae4 CACHE STRING "")
-set(VERSION_DESCRIBE v18.14.9-testing CACHE STRING "")
-set(VERSION_STRING 18.14.9 CACHE STRING "")
+set(VERSION_PATCH 6 CACHE STRING "")
+set(VERSION_GITHASH 899f41741cf9824d94bf6562c21b90c62ea4fee0 CACHE STRING "")
+set(VERSION_DESCRIBE v18.14.6-testing CACHE STRING "")
+set(VERSION_STRING 18.14.6 CACHE STRING "")
 # end of autochange
 
 set(VERSION_EXTRA "" CACHE STRING "")
diff --git a/dbms/programs/benchmark/Benchmark.cpp b/dbms/programs/benchmark/Benchmark.cpp
index 161321f4d..b9dc202a5 100644
--- a/dbms/programs/benchmark/Benchmark.cpp
+++ b/dbms/programs/benchmark/Benchmark.cpp
@@ -110,8 +110,6 @@ private:
     /// Don't execute new queries after timelimit or SIGINT or exception
     std::atomic<bool> shutdown{false};
 
-    std::atomic<size_t> queries_executed{0};
-
     struct Stats
     {
         Stopwatch watch;
@@ -240,12 +238,10 @@ private:
             size_t query_index = randomize ? distribution(generator) : i % queries.size();
 
             if (!tryPushQueryInteractively(queries[query_index], interrupt_listener))
-            {
-                shutdown = true;
                 break;
-            }
         }
 
+        shutdown = true;
         pool.wait();
         info_total.watch.stop();
 
@@ -278,12 +274,11 @@ private:
                 {
                     extracted = queue.tryPop(query, 100);
 
-                    if (shutdown || (max_iterations && queries_executed == max_iterations))
+                    if (shutdown)
                         return;
                 }
 
                 execute(connection, query);
-                ++queries_executed;
             }
         }
         catch (...)
@@ -424,20 +419,20 @@ int mainEntryClickHouseBenchmark(int argc, char ** argv)
 
         boost::program_options::options_description desc("Allowed options");
         desc.add_options()
-            ("help",                                                            "produce help message")
-            ("concurrency,c", value<unsigned>()->default_value(1),              "number of parallel queries")
-            ("delay,d",       value<double>()->default_value(1),                "delay between intermediate reports in seconds (set 0 to disable reports)")
-            ("stage",         value<std::string>()->default_value("complete"),  "request query processing up to specified stage")
-            ("iterations,i",  value<size_t>()->default_value(0),                "amount of queries to be executed")
-            ("timelimit,t",   value<double>()->default_value(0.),               "stop launch of queries after specified time limit")
-            ("randomize,r",   value<bool>()->default_value(false),              "randomize order of execution")
-            ("json",          value<std::string>()->default_value(""),          "write final report to specified file in JSON format")
-            ("host,h",        value<std::string>()->default_value("localhost"), "")
-            ("port",          value<UInt16>()->default_value(9000),             "")
-            ("user",          value<std::string>()->default_value("default"),   "")
-            ("password",      value<std::string>()->default_value(""),          "")
-            ("database",      value<std::string>()->default_value("default"),   "")
-            ("stacktrace",                                                      "print stack traces of exceptions")
+            ("help",                                                                 "produce help message")
+            ("concurrency,c",    value<unsigned>()->default_value(1),                 "number of parallel queries")
+            ("delay,d",         value<double>()->default_value(1),                     "delay between intermediate reports in seconds (set 0 to disable reports)")
+            ("stage",            value<std::string>()->default_value("complete"),     "request query processing up to specified stage")
+            ("iterations,i",    value<size_t>()->default_value(0),                    "amount of queries to be executed")
+            ("timelimit,t",        value<double>()->default_value(0.),                 "stop launch of queries after specified time limit")
+            ("randomize,r",        value<bool>()->default_value(false),                "randomize order of execution")
+            ("json",            value<std::string>()->default_value(""),            "write final report to specified file in JSON format")
+            ("host,h",            value<std::string>()->default_value("localhost"),     "")
+            ("port",             value<UInt16>()->default_value(9000),                 "")
+            ("user",             value<std::string>()->default_value("default"),        "")
+            ("password",        value<std::string>()->default_value(""),             "")
+            ("database",        value<std::string>()->default_value("default"),     "")
+            ("stacktrace",                                                            "print stack traces of exceptions")
 
         #define DECLARE_SETTING(TYPE, NAME, DEFAULT, DESCRIPTION) (#NAME, boost::program_options::value<std::string> (), DESCRIPTION)
             APPLY_FOR_SETTINGS(DECLARE_SETTING)
diff --git a/dbms/programs/client/Client.cpp b/dbms/programs/client/Client.cpp
index bf57d072f..73ea56405 100644
--- a/dbms/programs/client/Client.cpp
+++ b/dbms/programs/client/Client.cpp
@@ -844,7 +844,7 @@ private:
         }
         else if (print_time_to_stderr)
         {
-            std::cerr << watch.elapsedSeconds() << "\n";
+            std::cerr << "[Log] Cost seconds -> " << watch.elapsedSeconds() << "\n";
         }
 
         return true;
diff --git a/dbms/programs/obfuscator/Obfuscator.cpp b/dbms/programs/obfuscator/Obfuscator.cpp
index f7eb7c522..d5c6b6bac 100644
--- a/dbms/programs/obfuscator/Obfuscator.cpp
+++ b/dbms/programs/obfuscator/Obfuscator.cpp
@@ -500,7 +500,7 @@ private:
         return CRC32Hash()(StringRef(reinterpret_cast<const char *>(begin), (end - begin) * sizeof(CodePoint)));
     }
 
-    /// By the way, we don't have to use actual Unicode numbers. We use just arbitrary bijective mapping.
+    /// By the way, we don't have to use actual Unicode numbers. We use just arbitary bijective mapping.
     CodePoint readCodePoint(const char *& pos, const char * end)
     {
         size_t length = UTF8::seqLength(*pos);
@@ -954,7 +954,7 @@ try
         ("structure,S", po::value<std::string>(), "structure of the initial table (list of column and type names)")
         ("input-format", po::value<std::string>(), "input format of the initial table data")
         ("output-format", po::value<std::string>(), "default output format")
-        ("seed", po::value<std::string>(), "seed (arbitrary string), must be random string with at least 10 bytes length")
+        ("seed", po::value<std::string>(), "seed (arbitary string), must be random string with at least 10 bytes length")
         ("limit", po::value<UInt64>(), "if specified - stop after generating that number of rows")
         ("silent", po::value<bool>()->default_value(false), "don't print information messages to stderr")
         ("order", po::value<UInt64>()->default_value(5), "order of markov model to generate strings")
diff --git a/dbms/programs/odbc-bridge/ColumnInfoHandler.cpp b/dbms/programs/odbc-bridge/ColumnInfoHandler.cpp
index f59abd5f5..51302410a 100644
--- a/dbms/programs/odbc-bridge/ColumnInfoHandler.cpp
+++ b/dbms/programs/odbc-bridge/ColumnInfoHandler.cpp
@@ -94,8 +94,7 @@ void ODBCColumnsInfoHandler::handleRequest(Poco::Net::HTTPServerRequest & reques
     {
         schema_name = params.get("schema");
         LOG_TRACE(log, "Will fetch info for table '" << schema_name + "." + table_name << "'");
-    }
-    else
+    } else
         LOG_TRACE(log, "Will fetch info for table '" << table_name << "'");
     LOG_TRACE(log, "Got connection str '" << connection_string << "'");
 
diff --git a/dbms/programs/odbc-bridge/README.md b/dbms/programs/odbc-bridge/README.md
index 70b413c9c..91a6e4767 100644
--- a/dbms/programs/odbc-bridge/README.md
+++ b/dbms/programs/odbc-bridge/README.md
@@ -5,8 +5,8 @@ was possible segfaults or another faults in ODBC implementations, which can
 crash whole clickhouse-server process.
 
 This tool works via HTTP, not via pipes, shared memory, or TCP because:
-- It's simpler to implement
-- It's simpler to debug
+- It's simplier to implement
+- It's simplier to debug
 - jdbc-bridge can be implemented in the same way
 
 ## Usage
diff --git a/dbms/programs/odbc-bridge/validateODBCConnectionString.h b/dbms/programs/odbc-bridge/validateODBCConnectionString.h
index 8d2a23ca8..f0f93b1de 100644
--- a/dbms/programs/odbc-bridge/validateODBCConnectionString.h
+++ b/dbms/programs/odbc-bridge/validateODBCConnectionString.h
@@ -6,10 +6,10 @@
 namespace DB
 {
 
-/** Passing arbitrary connection string to ODBC Driver Manager is insecure, for the following reasons:
+/** Passing arbitary connection string to ODBC Driver Manager is insecure, for the following reasons:
   * 1. Driver Manager like unixODBC has multiple bugs like buffer overflow.
   * 2. Driver Manager can interpret some parameters as a path to library for dlopen or a file to read,
-  *    thus allows arbitrary remote code execution.
+  *    thus allows arbitary remote code execution.
   *
   * This function will throw exception if connection string has insecure parameters.
   * It may also modify connection string to harden it.
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionGroupArrayInsertAt.h b/dbms/src/AggregateFunctions/AggregateFunctionGroupArrayInsertAt.h
index 5ad93c042..b998ee972 100644
--- a/dbms/src/AggregateFunctions/AggregateFunctionGroupArrayInsertAt.h
+++ b/dbms/src/AggregateFunctions/AggregateFunctionGroupArrayInsertAt.h
@@ -107,7 +107,7 @@ public:
     void add(AggregateDataPtr place, const IColumn ** columns, size_t row_num, Arena *) const override
     {
         /// TODO Do positions need to be 1-based for this function?
-        size_t position = columns[1]->getUInt(row_num);
+        size_t position = columns[1]->get64(row_num);
 
         /// If position is larger than size to which array will be cutted - simply ignore value.
         if (length_to_resize && position >= length_to_resize)
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionMinMaxAny.h b/dbms/src/AggregateFunctions/AggregateFunctionMinMaxAny.h
index 51d1e8d1d..a2dbd21a3 100644
--- a/dbms/src/AggregateFunctions/AggregateFunctionMinMaxAny.h
+++ b/dbms/src/AggregateFunctions/AggregateFunctionMinMaxAny.h
@@ -607,7 +607,7 @@ struct AggregateFunctionAnyLastData : Data
 
 /** Implement 'heavy hitters' algorithm.
   * Selects most frequent value if its frequency is more than 50% in each thread of execution.
-  * Otherwise, selects some arbitrary value.
+  * Otherwise, selects some arbitary value.
   * http://www.cs.umd.edu/~samir/498/karp.pdf
   */
 template <typename Data>
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionNothing.h b/dbms/src/AggregateFunctions/AggregateFunctionNothing.h
index 3a98807bb..060561396 100644
--- a/dbms/src/AggregateFunctions/AggregateFunctionNothing.h
+++ b/dbms/src/AggregateFunctions/AggregateFunctionNothing.h
@@ -10,7 +10,7 @@ namespace DB
 {
 
 
-/** Aggregate function that takes arbitrary number of arbitrary arguments and does nothing.
+/** Aggregate function that takes arbitary number of arbitary arguments and does nothing.
   */
 class AggregateFunctionNothing final : public IAggregateFunctionHelper<AggregateFunctionNothing>
 {
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp b/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp
index 4b8222963..4031854f2 100644
--- a/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp
+++ b/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp
@@ -10,6 +10,7 @@ namespace DB
 namespace
 {
 
+template <bool repeated = false>
 AggregateFunctionPtr createAggregateFunctionWindowFunnel(const std::string & name, const DataTypes & arguments, const Array & params)
 {
     if (params.size() != 1)
@@ -21,14 +22,15 @@ AggregateFunctionPtr createAggregateFunctionWindowFunnel(const std::string & nam
     if (arguments.size() > AggregateFunctionWindowFunnelData::max_events + 1)
         throw Exception("Too many event arguments for aggregate function " + name, ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);
 
-    return std::make_shared<AggregateFunctionWindowFunnel>(arguments, params);
+    return std::make_shared<AggregateFunctionWindowFunnel<repeated>>(arguments, params);
 }
 
 }
 
 void registerAggregateFunctionWindowFunnel(AggregateFunctionFactory & factory)
 {
-    factory.registerFunction("windowFunnel", createAggregateFunctionWindowFunnel, AggregateFunctionFactory::CaseInsensitive);
+    factory.registerFunction("windowFunnel", createAggregateFunctionWindowFunnel<false>, AggregateFunctionFactory::CaseInsensitive);
+    factory.registerFunction("windowRepeatedFunnel", createAggregateFunctionWindowFunnel<true>, AggregateFunctionFactory::CaseInsensitive);
 }
 
 }
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.h b/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.h
index c9a4e6b32..a8d5fe1df 100644
--- a/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.h
+++ b/dbms/src/AggregateFunctions/AggregateFunctionWindowFunnel.h
@@ -18,8 +18,8 @@ namespace DB
 {
 namespace ErrorCodes
 {
-    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
-    extern const int TOO_MANY_ARGUMENTS_FOR_FUNCTION;
+extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
+extern const int TOO_MANY_ARGUMENTS_FOR_FUNCTION;
 }
 
 struct ComparePairFirst final
@@ -131,12 +131,13 @@ struct AggregateFunctionWindowFunnelData
 
 /** Calculates the max event level in a sliding window.
   * The max size of events is 32, that's enough for funnel analytics
-  *
+  * repeated indicates is there any repeated conditions, default to false
   * Usage:
   * - windowFunnel(window)(timestamp, cond1, cond2, cond3, ....)
   */
+template <bool repeated = false>
 class AggregateFunctionWindowFunnel final
-    : public IAggregateFunctionDataHelper<AggregateFunctionWindowFunnelData, AggregateFunctionWindowFunnel>
+        : public IAggregateFunctionDataHelper<AggregateFunctionWindowFunnelData, AggregateFunctionWindowFunnel<repeated>>
 {
 private:
     UInt32 window;
@@ -183,7 +184,7 @@ private:
 public:
     String getName() const override
     {
-        return "windowFunnel";
+        return !repeated ? "windowFunnel" : "windowRepeatedFunnel";
     }
 
     AggregateFunctionWindowFunnel(const DataTypes & arguments, const Array & params)
@@ -191,7 +192,7 @@ public:
         const auto time_arg = arguments.front().get();
         if (!WhichDataType(time_arg).isDateTime() && !WhichDataType(time_arg).isUInt32())
             throw Exception{"Illegal type " + time_arg->getName() + " of first argument of aggregate function " + getName()
-                + ", must be DateTime or UInt32"};
+                    + ", must be DateTime or UInt32"};
 
         for (const auto i : ext::range(1, arguments.size()))
         {
@@ -199,7 +200,7 @@ public:
             if (!isUInt8(cond_arg))
                 throw Exception{"Illegal type " + cond_arg->getName() + " of argument " + toString(i + 1) + " of aggregate function "
                         + getName() + ", must be UInt8",
-                    ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};
+                        ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};
         }
 
         events_size = arguments.size() - 1;
@@ -214,14 +215,30 @@ public:
 
     void add(AggregateDataPtr place, const IColumn ** columns, const size_t row_num, Arena *) const override
     {
-        const auto timestamp = static_cast<const ColumnVector<UInt32> *>(columns[0])->getData()[row_num];
-        // reverse iteration and stable sorting are needed for events that are qualified by more than one condition.
-        for (auto i = events_size; i > 0; --i)
+        if constexpr(!repeated)
         {
-            auto event = static_cast<const ColumnVector<UInt8> *>(columns[i])->getData()[row_num];
-            if (event)
-                this->data(place).add(timestamp, i);
+            for (const auto i : ext::range(1, events_size + 1))
+            {
+                auto event = static_cast<const ColumnVector<UInt8> *>(columns[i])->getData()[row_num];
+                if (event)
+                {
+                    this->data(place).add(static_cast<const ColumnVector<UInt32> *>(columns[0])->getData()[row_num], i);
+                    break;
+                }
+            }
+        }
+        else
+        {
+            for (size_t i = events_size; i >= 1; --i)
+            {
+                auto event = static_cast<const ColumnVector<UInt8> *>(columns[i])->getData()[row_num];
+                if (event)
+                {
+                    this->data(place).add(static_cast<const ColumnVector<UInt32> *>(columns[0])->getData()[row_num], i);
+                }
+            }
         }
+
     }
 
     void merge(AggregateDataPtr place, ConstAggregateDataPtr rhs, Arena *) const override
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionXFunnel.cpp b/dbms/src/AggregateFunctions/AggregateFunctionXFunnel.cpp
new file mode 100644
index 000000000..4ff3297ed
--- /dev/null
+++ b/dbms/src/AggregateFunctions/AggregateFunctionXFunnel.cpp
@@ -0,0 +1,36 @@
+#include <AggregateFunctions/AggregateFunctionFactory.h>
+#include <AggregateFunctions/AggregateFunctionXFunnel.h>
+#include <AggregateFunctions/Helpers.h>
+#include <AggregateFunctions/FactoryHelpers.h>
+
+
+namespace DB
+{
+
+namespace
+{
+
+template <bool repeated = false>
+AggregateFunctionPtr createAggregateFunctionXFunnel(const std::string & name, const DataTypes & arguments, const Array & params)
+{
+    if (params.size() < 3 || params.size() > 4)
+        throw Exception{"Aggregate function " + name + " requires 3 - 4 parameter.", ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH};
+
+    if (arguments.size() < 2)
+        throw Exception("Aggregate function " + name + " requires one column array argument and at least one event condition.", ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);
+
+    if (arguments.size() > AggregateFunctionXFunnelData::max_events + 1)
+        throw Exception("Too many event arguments for aggregate function " + name, ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);
+
+    return std::make_shared<AggregateFunctionXFunnel<repeated>>(arguments, params);
+}
+
+}
+
+void registerAggregateFunctionXFunnel(AggregateFunctionFactory & factory)
+{
+    factory.registerFunction("xFunnel", createAggregateFunctionXFunnel<false>, AggregateFunctionFactory::CaseInsensitive);
+    factory.registerFunction("xRepeatedFunnel", createAggregateFunctionXFunnel<true>, AggregateFunctionFactory::CaseInsensitive);
+}
+
+}
diff --git a/dbms/src/AggregateFunctions/AggregateFunctionXFunnel.h b/dbms/src/AggregateFunctions/AggregateFunctionXFunnel.h
new file mode 100644
index 000000000..f257a354a
--- /dev/null
+++ b/dbms/src/AggregateFunctions/AggregateFunctionXFunnel.h
@@ -0,0 +1,542 @@
+#pragma once
+
+#include <iostream>
+#include <sstream>
+#include <unordered_set>
+#include <Columns/ColumnArray.h>
+#include <Columns/ColumnTuple.h>
+#include <Columns/ColumnsNumber.h>
+#include <Common/ArenaAllocator.h>
+#include <Common/typeid_cast.h>
+#include <DataTypes/DataTypeArray.h>
+#include <DataTypes/DataTypeDateTime.h>
+#include <DataTypes/DataTypeTuple.h>
+#include <DataTypes/DataTypesNumber.h>
+#include <IO/ReadHelpers.h>
+#include <IO/WriteHelpers.h>
+
+#include <common/logger_useful.h>
+
+#include <Core/Field.h>
+#include <boost/algorithm/string/split.hpp>
+#include <ext/range.h>
+
+#include <AggregateFunctions/IAggregateFunction.h>
+
+namespace DB
+{
+namespace ErrorCodes
+{
+extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
+extern const int TOO_MANY_ARGUMENTS_FOR_FUNCTION;
+}
+
+struct TimestampEvent
+{
+    UInt32 timestamp;
+    UInt16 day;
+    Tuple tuple;
+    UInt8 event;
+
+    TimestampEvent(const UInt32 & timestamp_, const UInt16 & day_, Tuple & tuple_, const UInt8 & event_)
+        : timestamp{timestamp_}, day{day_}, tuple{std::move(tuple_)}, event{event_}
+    {
+    }
+};
+
+struct ComparePairFirst final
+{
+    bool operator()(const TimestampEvent & lhs, const TimestampEvent & rhs) const
+    {
+        return lhs.timestamp < rhs.timestamp;
+    }
+};
+
+
+static constexpr size_t bytes_on_stack = 4096;
+//using TimestampEvents = PODArray<TimestampEvent, bytes_on_stack>;
+using TimestampEvents = std::vector<TimestampEvent>;
+using validataFunc = std::function<bool(const Tuple & tuple, const Tuple & pre_tuple)>;
+using Comparator = ComparePairFirst;
+
+struct AggregateFunctionXFunnelData
+{
+    static constexpr auto max_events = 32;
+
+    bool sorted = true;
+    TimestampEvents events_list;
+
+
+    size_t size() const
+    {
+        return events_list.size();
+    }
+
+    void add(const UInt32 & timestamp, const UInt16 & day, Tuple & node, UInt8 event)
+    {
+        // Since most events should have already been sorted by timestamp.
+        if (sorted && events_list.size() > 0 && events_list.back().timestamp > timestamp)
+        {
+            sorted = false;
+        }
+        events_list.emplace_back(timestamp, day, node, event);
+    }
+
+    void merge(const AggregateFunctionXFunnelData & other)
+    {
+        const auto size = events_list.size();
+        //        events_list.insert(std::begin(other.events_list), std::end(other.events_list));
+        events_list.insert(events_list.end(), other.events_list.begin(), other.events_list.end());
+
+        /// either sort whole container or do so partially merging ranges afterwards
+        if (!sorted && !other.sorted)
+            std::stable_sort(std::begin(events_list), std::end(events_list), Comparator{});
+        else
+        {
+            const auto begin = std::begin(events_list);
+            const auto middle = std::next(begin, size);
+            const auto end = std::end(events_list);
+
+            if (!sorted)
+                std::stable_sort(begin, middle, Comparator{});
+
+            if (!other.sorted)
+                std::stable_sort(middle, end, Comparator{});
+
+            std::inplace_merge(begin, middle, end, Comparator{});
+        }
+
+        sorted = true;
+    }
+
+    void sort()
+    {
+        if (!sorted)
+        {
+            std::stable_sort(std::begin(events_list), std::end(events_list), Comparator{});
+            sorted = true;
+        }
+    }
+
+    void serialize(WriteBuffer & buf) const
+    {
+        writeBinary(sorted, buf);
+        writeBinary(events_list.size(), buf);
+
+        for (const auto & events : events_list)
+        {
+            writeBinary(events.timestamp, buf);
+            writeBinary(events.day, buf);
+            auto & tuple = events.tuple;
+            writeBinary(tuple, buf);
+            writeBinary(events.event, buf);
+        }
+    }
+
+    void deserialize(ReadBuffer & buf)
+    {
+        readBinary(sorted, buf);
+
+        size_t size;
+        readBinary(size, buf);
+
+        /// TODO Protection against huge size
+
+        events_list.clear();
+        events_list.reserve(size);
+
+        UInt32 timestamp;
+        UInt16 day;
+        UInt8 event;
+
+        for (size_t i = 0; i < size; ++i)
+        {
+            readBinary(timestamp, buf);
+            readBinary(day, buf);
+
+            Tuple node;
+            readTuple(node, buf);
+
+            readBinary(event, buf);
+            events_list.emplace_back(timestamp, day, node, event);
+        }
+    }
+};
+
+
+/** Calculates the max event level in a sliding window.
+  * The max size of events is 32, that's enough for funnel analytics
+  * repeated indicates is there any repeated conditions, default to false
+  * Usage:
+  * - xFunnel(windowSize, 2, rule)( (timestamp, col1, col2....) , cond1, cond2, cond3, ....)
+  */
+template <bool repeated = false>
+class AggregateFunctionXFunnel final : public IAggregateFunctionDataHelper<AggregateFunctionXFunnelData, AggregateFunctionXFunnel<repeated>>
+{
+private:
+    UInt32 window;
+    UInt8 events_size;
+    UInt8 max_output_idx;
+    String rule_arg{""};
+    size_t tuple_size;
+
+    DataTypes dataTypes;
+
+    using validataIdxFuncs = std::vector<std::pair<UInt8, validataFunc>>;
+    std::vector<validataIdxFuncs> validators;
+
+    using Indexs = std::vector<UInt16>;
+    using DayIndexs = std::vector<Indexs>;
+
+    // 多维数组，第一维度表示某天开头的所有可能事件流序列
+    using DayIndexsPattern = std::vector<std::vector<DayIndexs>>;
+
+    /// 匹配算法
+    ALWAYS_INLINE DayIndexs getFunnelIndexArray(const AggregateFunctionXFunnelData & data) const
+    {
+        if (data.size() == 0)
+            return {};
+
+        const_cast<AggregateFunctionXFunnelData &>(data).sort();
+
+        // 返回多个漏斗匹配事件流，按天分组
+        UInt16 first_day = data.events_list.front().day;
+        int count = data.events_list.back().day - first_day + 1;
+        DayIndexs res(count);
+        DayIndexsPattern patterns(count, std::vector<DayIndexs>(events_size));
+
+//        LOG_TRACE(&Logger::get("xFunnel"), "list=>" << data.events_list.size() <<  ",day_size" << count << "~" << "min=>" << first_day << "max=>" <<  data.events_list.back().day);
+
+
+        bool ok;
+        int tmp_idx;
+
+        /// let's fuck the BT rules
+        /// 最大目标一样的， 从右边开始，越靠近最大目标的
+        /// 类似多叉树， 没满足最后目标求最大深度最右节点序列， 满足最后目标后，只需要第一个
+
+        for (UInt16 idx = 0; idx < data.events_list.size(); idx++)
+        {
+            const auto & event = data.events_list[idx];
+            const auto & timestamp = event.timestamp;
+            const auto & tuple = event.tuple;
+            const auto & event_idx = event.event;
+            const auto & day = event.day;
+
+
+            if (event_idx == 0)
+            {
+                if (!res[day - first_day].empty())
+                {
+                    continue;
+                }
+                Indexs tmp = Indexs{idx};
+                patterns[day - first_day][0].push_back(std::move(tmp));
+            }
+
+            else
+            {
+                for (UInt16 day_idx = 0; day_idx < patterns.size(); ++day_idx)
+                {
+                    if (!res[day_idx].empty() || day < first_day + day_idx)
+                        continue;
+
+                    //tmp_idx 表示 该天 event_idx = 0 的时间最大值，如果这个值也不满足时间窗口，则我们可以取多叉树最右值
+                    tmp_idx = patterns[day_idx].front().empty() ? -1 : patterns[day_idx].front().back().front();
+                    if (tmp_idx != -1 && data.events_list[tmp_idx].timestamp + window < timestamp)
+                    {
+                        for (size_t event = events_size - 1; event > 0; --event)
+                        {
+                            if (!patterns[day_idx][event - 1].empty())
+                            {
+                                res[day_idx] = patterns[day_idx][event - 1].back();
+                                count --;
+                                break;
+                            }
+                        }
+                        continue;
+                    }
+
+                    for (const auto & pre_indexes : patterns[day_idx][event_idx - 1])
+                    {
+                        if (pre_indexes.size() < 1)
+                            break;
+
+                        tmp_idx = pre_indexes.front();
+                        if (data.events_list[tmp_idx].timestamp + window >= timestamp)
+                        {
+                            ok = true;
+
+                            const auto & idx_funcs = validators[event_idx];
+                            for (const auto & idxFunc : idx_funcs)
+                            {
+                                const auto & pre_tuple = data.events_list[pre_indexes[idxFunc.first]].tuple;
+                                if (!idxFunc.second(tuple, pre_tuple))
+                                {
+                                    ok = false;
+                                    break;
+                                }
+                            }
+
+                            if (ok)
+                            {
+                                //如果是最后一个，只需要取最后一个！！
+                                Indexs current_indexes = pre_indexes;
+                                current_indexes.push_back(idx);
+                                patterns[day_idx][event_idx].push_back(std::move(current_indexes));
+                            }
+                        }
+                    }
+                    /// now we reach the final goal~ but we should continue ...
+                    if (event_idx + 1 == events_size && res[day_idx].empty() && !patterns[day_idx][event_idx].empty())
+                    {
+                        res[day_idx] = patterns[day_idx][event_idx].back();
+                        count--;
+                    }
+                }
+            }
+        }
+
+
+        /// for the days never reach final goal, let's gather them all.
+        if (count != 0)
+        {
+            for (UInt16 day_idx = 0; day_idx < patterns.size(); ++day_idx)
+            {
+                if (!res[day_idx].empty())
+                    continue;
+
+                /// return the rightest event sequences
+                for (size_t event = events_size - 1; event > 0; --event)
+                {
+                    if (!patterns[day_idx][event - 1].empty()) {
+                        res[day_idx] = patterns[day_idx][event - 1].back();
+//                        LOG_TRACE(&Logger::get("xFunnel"), "filling=> " << day_idx << "size=>" << res[day_idx].size());
+                        break;
+                    }
+                }
+            }
+        }
+        return std::move(res);
+    }
+
+public:
+    String getName() const override
+    {
+        return !repeated ? "xFunnel" : "xRepeatedFunnel";
+    }
+
+    AggregateFunctionXFunnel(const DataTypes & arguments, const Array & params)
+    {
+        window = params.at(0).safeGet<UInt64>();
+
+        // [min_ouput_idx, max_output_idx]
+        max_output_idx = params.at(1).safeGet<UInt64>() - 1;
+        if (max_output_idx < 1)
+            throw Exception{"Invalid number of " + toString(max_output_idx+1) + ", must greater than 2"};
+
+        const auto col_arg = arguments[0].get();
+        auto tuple_args = typeid_cast<const DataTypeTuple *>(col_arg);
+        if (!tuple_args)
+            throw Exception{
+            "Illegal type " + col_arg->getName() + " of first argument of aggregate function " + getName() + ", must be tuple"};
+        tuple_size = tuple_args->getElements().size();
+        events_size = arguments.size() - 1;
+        validators.resize(events_size);
+
+        for (const auto & i : ext::range(1, arguments.size()))
+        {
+            const auto & cond_arg = arguments[i].get();
+            if (!typeid_cast<const DataTypeUInt8 *>(cond_arg))
+                throw Exception{"Illegal type " + cond_arg->getName() + " of argument " + toString(i + 1) + " of aggregate function "
+                        + getName() + ", must be UInt8",
+                        ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};
+        }
+
+        // (timestamp, day, a,b,c,d,...) ) =>  (timestamp, day, a) for max_output_idx == 3
+        const auto & tupleType = typeid_cast<const DataTypeTuple *>(arguments[0].get());
+        const auto & time_arg = static_cast<const DataTypeUInt32 *>(tupleType->getElements()[0].get());
+        if (!time_arg)
+            throw Exception{"Illegal type " + time_arg->getName() + " 1st of first tuple argument of aggregate function " + getName()
+                    + ", must be DateTime or UInt32"};
+
+        const auto & day_arg = static_cast<const DataTypeUInt16 *>(tupleType->getElements()[1].get());
+        if (!day_arg)
+            throw Exception{"Illegal type " + day_arg->getName() + " 2st of first tuple argument of aggregate function " + getName()
+                    + ", must be Date or UInt16"};
+
+
+        for (const auto & idx : ext::range(0, max_output_idx + 1))
+        {
+            dataTypes.emplace_back(tupleType->getElements()[idx]);
+        }
+
+        if (params.size() > 2)
+        {
+            rule_arg = params.at(2).safeGet<String>();
+            if (!rule_arg.empty())
+                initRule();
+        }
+        LOG_TRACE(&Logger::get("xFunnel"), "tuple_size " << tuple_size << " rule_args " << rule_arg << " window " << window);
+    }
+
+    // 初始化rule规则
+    ALWAYS_INLINE void initRule()
+    {
+        std::vector<String> ruleStrs;
+        // eg 1.1=2.1,3.2=2.2
+        boost::split(ruleStrs, rule_arg, [](char c) { return c == ','; });
+        if (ruleStrs.size() < 1)
+            return;
+
+        for (String & ruleStr : ruleStrs)
+        {
+            std::vector<String> ruleKvs;
+            boost::split(ruleKvs, ruleStr, [](char c) { return c == '='; });
+
+            // eg [1.1,2.2]
+            if (ruleKvs.size() != 2)
+                throw Exception{"Illegal rule " + ruleStr, ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};
+
+            std::vector<String> ruleKeys;
+            std::vector<String> ruleValues;
+            boost::split(ruleKeys, ruleKvs[0], [](char c) { return c == '.'; });
+            boost::split(ruleValues, ruleKvs[1], [](char c) { return c == '.'; });
+
+            int previous_event_level = std::atoi(ruleKeys[0].c_str());
+            int previous_label_idx = std::atoi(ruleKeys[1].c_str());
+
+            int current_event_level = std::atoi(ruleValues[0].c_str());
+            int current_label_idx = std::atoi(ruleValues[1].c_str());
+
+            if (previous_event_level < current_event_level)
+            {
+                addRule(previous_event_level - 1, previous_label_idx - 3, current_event_level - 1, current_label_idx - 3);
+            }
+            else
+            {
+                addRule(current_event_level - 1, current_label_idx - 3, previous_event_level - 1, previous_label_idx - 3);
+            }
+        }
+    }
+
+    ALWAYS_INLINE void addRule(const size_t & previous_event_level,
+                               const size_t & previous_label_idx,
+                               const size_t & current_event_level,
+                               const size_t & current_label_idx)
+    {
+        auto func = [=](const Tuple & current_label, const Tuple & pre_label) {
+            const TupleBackend & current_arr = current_label.toUnderType();
+            const TupleBackend & previous_arr = pre_label.toUnderType();
+
+            return current_arr[current_label_idx] == previous_arr[previous_label_idx];
+        };
+        validators[current_event_level].push_back(std::make_pair(previous_event_level, func));
+    }
+
+    DataTypePtr getReturnType() const override
+    {
+        return std::make_shared<DataTypeArray>(std::make_shared<DataTypeArray>(std::make_shared<DataTypeTuple>(dataTypes)));
+    }
+
+    void add(AggregateDataPtr place, const IColumn ** columns, const size_t row_num, Arena *) const override
+    {
+        if constexpr (!repeated)
+        {
+            for (const auto idx : ext::range(1, events_size + 1))
+            {
+                auto event = static_cast<const ColumnVector<UInt8> *>(columns[idx])->getData()[row_num];
+                if (event)
+                {
+                    auto labelCol = static_cast<const ColumnTuple *>(columns[0]);
+
+                    /// 这里将timestamp和day抽离出来
+                    Tuple tuple = Tuple(TupleBackend(tuple_size - 2));
+                    auto & arr = tuple.toUnderType();
+                    for (const auto i : ext::range(2, tuple_size))
+                    {
+                        labelCol->getColumn(i).get(row_num, arr[i - 2]);
+                    }
+                    this->data(place).add(labelCol->getColumn(0).getInt(row_num), labelCol->getColumn(1).getInt(row_num), tuple, idx - 1);
+                    break;
+                }
+            }
+        }
+        else
+        {
+            for (auto idx = events_size; idx >= 1; --idx)
+            {
+                auto event = static_cast<const ColumnVector<UInt8> *>(columns[idx])->getData()[row_num];
+                if (event)
+                {
+                    auto labelCol = static_cast<const ColumnTuple *>(columns[0]);
+
+                    /// 这里将timestamp和day抽离出来
+                    Tuple tuple = Tuple(TupleBackend(tuple_size - 2));
+                    auto & arr = tuple.toUnderType();
+                    for (const auto i : ext::range(2, tuple_size))
+                    {
+                        labelCol->getColumn(i).get(row_num, arr[i - 2]);
+                    }
+                    this->data(place).add(labelCol->getColumn(0).getInt(row_num), labelCol->getColumn(1).getInt(row_num), tuple, idx - 1);
+                }
+            }
+        }
+    }
+
+    void merge(AggregateDataPtr place, ConstAggregateDataPtr rhs, Arena *) const override
+    {
+        this->data(place).merge(this->data(rhs));
+    }
+
+    void serialize(ConstAggregateDataPtr place, WriteBuffer & buf) const override
+    {
+        this->data(place).serialize(buf);
+    }
+
+    void deserialize(AggregateDataPtr place, ReadBuffer & buf, Arena *) const override
+    {
+        this->data(place).deserialize(buf);
+    }
+
+    void insertResultInto(ConstAggregateDataPtr place, IColumn & to) const override
+    {
+        const auto & funnel_index_array = getFunnelIndexArray(this->data(place));
+        ColumnArray & arr_to = static_cast<ColumnArray &>(to);
+        ColumnArray::Offsets & offsets_to = arr_to.getOffsets();
+
+        int count = funnel_index_array.size();
+        for (const auto & funnel_index : funnel_index_array)
+        {
+            if (funnel_index.empty())
+            {
+                count --;
+                continue;
+            }
+            //static_cast<ColumnUInt8 &>(static_cast<ColumnArray &>(to).getData()).getData();
+            auto & arr_tuple_to = static_cast<ColumnArray &>(arr_to.getData());
+            auto & offset_tuple_to = arr_tuple_to.getOffsets();
+
+            for (const auto & index : funnel_index)
+            {
+                auto & timestamp_event = this->data(place).events_list[index];
+                auto & tuple_data = static_cast<ColumnTuple &>(arr_tuple_to.getData());
+
+                tuple_data.getColumn(0).insert(static_cast<UInt64>(timestamp_event.timestamp));
+                tuple_data.getColumn(1).insert(static_cast<UInt64>(timestamp_event.day));
+                for (size_t idx = 2; idx <= max_output_idx; idx++)
+                {
+                    tuple_data.getColumn(idx).insert(timestamp_event.tuple.toUnderType()[idx - 2]);
+                }
+            }
+            offset_tuple_to.push_back(offset_tuple_to.size() == 0 ? funnel_index.size() : offset_tuple_to.back() + funnel_index.size());
+        }
+        offsets_to.push_back(offsets_to.size() == 0 ? count : offsets_to.back() + count);
+    }
+
+    const char * getHeaderFilePath() const override
+    {
+        return __FILE__;
+    }
+};
+}
diff --git a/dbms/src/AggregateFunctions/QuantileExact.h b/dbms/src/AggregateFunctions/QuantileExact.h
index 7ac639b8f..57c93f98f 100644
--- a/dbms/src/AggregateFunctions/QuantileExact.h
+++ b/dbms/src/AggregateFunctions/QuantileExact.h
@@ -70,11 +70,19 @@ struct QuantileExact
         if (!array.empty())
         {
             size_t n = level < 1
-                ? level * array.size()
-                : (array.size() - 1);
+                       ? level * array.size()
+                       : (array.size() - 1);
 
             std::nth_element(array.begin(), array.begin() + n, array.end());    /// NOTE You can think of the radix-select algorithm.
-            return array[n];
+
+            //TODO https://stackoverflow.com/questions/1719070/what-is-the-right-approach-when-using-stl-container-for-median-calculation
+            // some hacks
+            auto med = array[n];
+            if (!(array.size() & 1) && level == 0.5) {
+                auto max_it = std::max_element(array.begin(), array.begin()+n);
+                return (*max_it + med) / 2.0;
+            }
+            return med;
         }
 
         return std::numeric_limits<Value>::quiet_NaN();
diff --git a/dbms/src/AggregateFunctions/registerAggregateFunctions.cpp b/dbms/src/AggregateFunctions/registerAggregateFunctions.cpp
index 3517ad57a..bd7ffff70 100644
--- a/dbms/src/AggregateFunctions/registerAggregateFunctions.cpp
+++ b/dbms/src/AggregateFunctions/registerAggregateFunctions.cpp
@@ -15,6 +15,7 @@ void registerAggregateFunctionGroupArrayInsertAt(AggregateFunctionFactory &);
 void registerAggregateFunctionsQuantile(AggregateFunctionFactory &);
 void registerAggregateFunctionsSequenceMatch(AggregateFunctionFactory &);
 void registerAggregateFunctionWindowFunnel(AggregateFunctionFactory &);
+void registerAggregateFunctionXFunnel(AggregateFunctionFactory &);
 void registerAggregateFunctionsMinMaxAny(AggregateFunctionFactory &);
 void registerAggregateFunctionsStatisticsStable(AggregateFunctionFactory &);
 void registerAggregateFunctionsStatisticsSimple(AggregateFunctionFactory &);
@@ -49,6 +50,7 @@ void registerAggregateFunctions()
         registerAggregateFunctionsQuantile(factory);
         registerAggregateFunctionsSequenceMatch(factory);
         registerAggregateFunctionWindowFunnel(factory);
+        registerAggregateFunctionXFunnel(factory);
         registerAggregateFunctionsMinMaxAny(factory);
         registerAggregateFunctionsStatisticsStable(factory);
         registerAggregateFunctionsStatisticsSimple(factory);
diff --git a/dbms/src/CMakeLists.txt b/dbms/src/CMakeLists.txt
index 30bd7c134..a9ccdb559 100644
--- a/dbms/src/CMakeLists.txt
+++ b/dbms/src/CMakeLists.txt
@@ -14,3 +14,4 @@ add_subdirectory (Client)
 add_subdirectory (TableFunctions)
 add_subdirectory (Analyzers)
 add_subdirectory (Formats)
+add_subdirectory (Compression)
diff --git a/dbms/src/Client/Connection.cpp b/dbms/src/Client/Connection.cpp
index 727fa6fb5..1ed186085 100644
--- a/dbms/src/Client/Connection.cpp
+++ b/dbms/src/Client/Connection.cpp
@@ -231,14 +231,6 @@ void Connection::getServerVersion(String & name, UInt64 & version_major, UInt64
     revision = server_revision;
 }
 
-UInt64 Connection::getServerRevision()
-{
-    if (!connected)
-        connect();
-
-    return server_revision;
-}
-
 const String & Connection::getServerTimezone()
 {
     if (!connected)
@@ -357,7 +349,7 @@ void Connection::sendQuery(
     {
         ClientInfo client_info_to_send;
 
-        if (!client_info || client_info->empty())
+        if (!client_info)
         {
             /// No client info passed - means this query initiated by me.
             client_info_to_send.query_kind = ClientInfo::QueryKind::INITIAL_QUERY;
diff --git a/dbms/src/Client/Connection.h b/dbms/src/Client/Connection.h
index d8229fc34..ad98df3cc 100644
--- a/dbms/src/Client/Connection.h
+++ b/dbms/src/Client/Connection.h
@@ -106,7 +106,6 @@ public:
     void setDefaultDatabase(const String & database);
 
     void getServerVersion(String & name, UInt64 & version_major, UInt64 & version_minor, UInt64 & version_patch, UInt64 & revision);
-    UInt64 getServerRevision();
 
     const String & getServerTimezone();
     const String & getServerDisplayName();
diff --git a/dbms/src/Client/ConnectionPoolWithFailover.cpp b/dbms/src/Client/ConnectionPoolWithFailover.cpp
index d71d1f083..8c2f1a07b 100644
--- a/dbms/src/Client/ConnectionPoolWithFailover.cpp
+++ b/dbms/src/Client/ConnectionPoolWithFailover.cpp
@@ -153,9 +153,13 @@ ConnectionPoolWithFailover::tryGetEntry(
     {
         result.entry = pool.get(settings, /* force_connected = */ false);
 
-        UInt64 server_revision = 0;
+        String server_name;
+        UInt64 server_version_major;
+        UInt64 server_version_minor;
+        UInt64 server_version_patch;
+        UInt64 server_revision;
         if (table_to_check)
-            server_revision = result.entry->getServerRevision();
+            result.entry->getServerVersion(server_name, server_version_major, server_version_minor, server_version_patch, server_revision);
 
         if (!table_to_check || server_revision < DBMS_MIN_REVISION_WITH_TABLES_STATUS)
         {
diff --git a/dbms/src/Client/MultiplexedConnections.cpp b/dbms/src/Client/MultiplexedConnections.cpp
index 5228f9b85..3e88a20ca 100644
--- a/dbms/src/Client/MultiplexedConnections.cpp
+++ b/dbms/src/Client/MultiplexedConnections.cpp
@@ -87,36 +87,28 @@ void MultiplexedConnections::sendQuery(
     if (sent_query)
         throw Exception("Query already sent.", ErrorCodes::LOGICAL_ERROR);
 
-    Settings modified_settings = settings;
-
-    for (auto & replica : replica_states)
+    if (replica_states.size() > 1)
     {
-        if (!replica.connection)
-            throw Exception("MultiplexedConnections: Internal error", ErrorCodes::LOGICAL_ERROR);
+        Settings query_settings = settings;
+        query_settings.parallel_replicas_count = replica_states.size();
 
-        if (replica.connection->getServerRevision() < DBMS_MIN_REVISION_WITH_CURRENT_AGGREGATION_VARIANT_SELECTION_METHOD)
+        for (size_t i = 0; i < replica_states.size(); ++i)
         {
-            /// Disable two-level aggregation due to version incompatibility.
-            modified_settings.group_by_two_level_threshold = 0;
-            modified_settings.group_by_two_level_threshold_bytes = 0;
-        }
-    }
+            Connection * connection = replica_states[i].connection;
+            if (connection == nullptr)
+                throw Exception("MultiplexedConnections: Internal error", ErrorCodes::LOGICAL_ERROR);
 
-    size_t num_replicas = replica_states.size();
-    if (num_replicas > 1)
-    {
-        /// Use multiple replicas for parallel query processing.
-        modified_settings.parallel_replicas_count = num_replicas;
-        for (size_t i = 0; i < num_replicas; ++i)
-        {
-            modified_settings.parallel_replica_offset = i;
-            replica_states[i].connection->sendQuery(query, query_id, stage, &modified_settings, client_info, with_pending_data);
+            query_settings.parallel_replica_offset = i;
+            connection->sendQuery(query, query_id, stage, &query_settings, client_info, with_pending_data);
         }
     }
     else
     {
-        /// Use single replica.
-        replica_states[0].connection->sendQuery(query, query_id, stage, &modified_settings, client_info, with_pending_data);
+        Connection * connection = replica_states[0].connection;
+        if (connection == nullptr)
+            throw Exception("MultiplexedConnections: Internal error", ErrorCodes::LOGICAL_ERROR);
+
+        connection->sendQuery(query, query_id, stage, &settings, client_info, with_pending_data);
     }
 
     sent_query = true;
diff --git a/dbms/src/Columns/ColumnAggregateFunction.cpp b/dbms/src/Columns/ColumnAggregateFunction.cpp
index 8f8a44be8..b50bb81f8 100644
--- a/dbms/src/Columns/ColumnAggregateFunction.cpp
+++ b/dbms/src/Columns/ColumnAggregateFunction.cpp
@@ -373,7 +373,7 @@ const char * ColumnAggregateFunction::deserializeAndInsertFromArena(const char *
 
     /** We will read from src_arena.
       * There is no limit for reading - it is assumed, that we can read all that we need after src_arena pointer.
-      * Buf ReadBufferFromMemory requires some bound. We will use arbitrary big enough number, that will not overflow pointer.
+      * Buf ReadBufferFromMemory requires some bound. We will use arbitary big enough number, that will not overflow pointer.
       * NOTE Technically, this is not compatible with C++ standard,
       *  as we cannot legally compare pointers after last element + 1 of some valid memory region.
       *  Probably this will not work under UBSan.
diff --git a/dbms/src/Columns/ColumnConst.h b/dbms/src/Columns/ColumnConst.h
index c9038cdf7..bd40246b1 100644
--- a/dbms/src/Columns/ColumnConst.h
+++ b/dbms/src/Columns/ColumnConst.h
@@ -15,7 +15,7 @@ namespace ErrorCodes
 
 
 /** ColumnConst contains another column with single element,
-  *  but looks like a column with arbitrary amount of same elements.
+  *  but looks like a column with arbitary amount of same elements.
   */
 class ColumnConst final : public COWPtrHelper<IColumn, ColumnConst>
 {
diff --git a/dbms/src/Columns/ColumnDecimal.cpp b/dbms/src/Columns/ColumnDecimal.cpp
index 092ee74d6..01ec69b7e 100644
--- a/dbms/src/Columns/ColumnDecimal.cpp
+++ b/dbms/src/Columns/ColumnDecimal.cpp
@@ -1,3 +1,6 @@
+#include <cmath>
+#include <ext/bit_cast.h>
+
 #include <Common/Exception.h>
 #include <Common/Arena.h>
 #include <Common/SipHash.h>
@@ -50,7 +53,7 @@ UInt64 ColumnDecimal<T>::get64(size_t n) const
 {
     if constexpr (sizeof(T) > sizeof(UInt64))
         throw Exception(String("Method get64 is not supported for ") + getFamilyName(), ErrorCodes::NOT_IMPLEMENTED);
-    return static_cast<typename T::NativeType>(data[n]);
+    return ext::bit_cast<UInt64>(data[n]);
 }
 
 template <typename T>
@@ -117,14 +120,6 @@ MutableColumnPtr ColumnDecimal<T>::cloneResized(size_t size) const
     return std::move(res);
 }
 
-template <typename T>
-void ColumnDecimal<T>::insertData(const char * src, size_t /*length*/)
-{
-    T tmp;
-    memcpy(&tmp, src, sizeof(T));
-    data.emplace_back(tmp);
-}
-
 template <typename T>
 void ColumnDecimal<T>::insertRangeFrom(const IColumn & src, size_t start, size_t length)
 {
diff --git a/dbms/src/Columns/ColumnDecimal.h b/dbms/src/Columns/ColumnDecimal.h
index 523064167..e2e52bfe8 100644
--- a/dbms/src/Columns/ColumnDecimal.h
+++ b/dbms/src/Columns/ColumnDecimal.h
@@ -89,7 +89,7 @@ public:
     void reserve(size_t n) override { data.reserve(n); }
 
     void insertFrom(const IColumn & src, size_t n) override { data.push_back(static_cast<const Self &>(src).getData()[n]); }
-    void insertData(const char * pos, size_t /*length*/) override;
+    void insertData(const char * pos, size_t /*length*/) override { data.push_back(*reinterpret_cast<const T *>(pos)); }
     void insertDefault() override { data.push_back(T()); }
     void insert(const Field & x) override { data.push_back(DB::get<typename NearestFieldType<T>::Type>(x)); }
     void insertRangeFrom(const IColumn & src, size_t start, size_t length) override;
@@ -112,7 +112,6 @@ public:
     bool getBool(size_t n) const override { return bool(data[n]); }
     Int64 getInt(size_t n) const override { return Int64(data[n] * scale); }
     UInt64 get64(size_t n) const override;
-    bool isDefaultAt(size_t n) const override { return data[n] == 0; }
 
     ColumnPtr filter(const IColumn::Filter & filt, ssize_t result_size_hint) const override;
     ColumnPtr permute(const IColumn::Permutation & perm, size_t limit) const override;
diff --git a/dbms/src/Columns/IColumn.h b/dbms/src/Columns/IColumn.h
index 82a686625..034bf9d5c 100644
--- a/dbms/src/Columns/IColumn.h
+++ b/dbms/src/Columns/IColumn.h
@@ -92,7 +92,7 @@ public:
     }
 
     /** If column is numeric, return value of n-th element, casted to UInt64.
-      * For NULL values of Nullable column it is allowed to return arbitrary value.
+      * For NULL values of Nullable column it is allowed to return arbitary value.
       * Otherwise throw an exception.
       */
     virtual UInt64 getUInt(size_t /*n*/) const
@@ -105,7 +105,6 @@ public:
         throw Exception("Method getInt is not supported for " + getName(), ErrorCodes::NOT_IMPLEMENTED);
     }
 
-    virtual bool isDefaultAt(size_t n) const { return get64(n) == 0; }
     virtual bool isNullAt(size_t /*n*/) const { return false; }
 
     /** If column is numeric, return value of n-th element, casted to bool.
@@ -175,7 +174,7 @@ public:
     virtual const char * deserializeAndInsertFromArena(const char * pos) = 0;
 
     /// Update state of hash function with value of n-th element.
-    /// On subsequent calls of this method for sequence of column values of arbitrary types,
+    /// On subsequent calls of this method for sequence of column values of arbitary types,
     ///  passed bytes to hash must identify sequence of values unambiguously.
     virtual void updateHashWithValue(size_t n, SipHash & hash) const = 0;
 
diff --git a/dbms/src/Common/ErrorCodes.cpp b/dbms/src/Common/ErrorCodes.cpp
index 4e724c995..7dd03459c 100644
--- a/dbms/src/Common/ErrorCodes.cpp
+++ b/dbms/src/Common/ErrorCodes.cpp
@@ -396,6 +396,8 @@ namespace ErrorCodes
     extern const int MULTIPLE_ASSIGNMENTS_TO_COLUMN = 419;
     extern const int CANNOT_UPDATE_COLUMN = 420;
     extern const int CANNOT_ADD_DIFFERENT_AGGREGATE_STATES = 421;
+    extern const int ILLEGAL_SYNTAX_FOR_CODEC_TYPE = 422;
+    extern const int UNKNOWN_CODEC = 423;
 
     extern const int KEEPER_EXCEPTION = 999;
     extern const int POCO_EXCEPTION = 1000;
diff --git a/dbms/src/Common/XDBCBridgeHelper.h b/dbms/src/Common/XDBCBridgeHelper.h
index 8cb025df7..93e4a0d69 100644
--- a/dbms/src/Common/XDBCBridgeHelper.h
+++ b/dbms/src/Common/XDBCBridgeHelper.h
@@ -254,8 +254,7 @@ struct ODBCBridgeMixin
 
     static void startBridge(const Poco::Util::AbstractConfiguration & config, Poco::Logger * log, const Poco::Timespan & http_timeout)
     {
-        /// Path to executable folder
-        Poco::Path path{config.getString("application.dir", "/usr/bin")};
+        Poco::Path path{config.getString("application.dir", "")};
 
         path.setFileName(
 #if CLICKHOUSE_SPLIT_BINARY
@@ -265,6 +264,9 @@ struct ODBCBridgeMixin
 #endif
         );
 
+        if (!Poco::File(path).exists())
+            throw Exception("clickhouse binary (" + path.toString() + ") is not found", ErrorCodes::EXTERNAL_EXECUTABLE_NOT_FOUND);
+
         std::stringstream command;
 
         command << path.toString() <<
diff --git a/dbms/tests/queries/0_stateless/00698_validate_array_sizes_for_nested_kshvakov.reference b/dbms/src/Compression/CMakeLists.txt
similarity index 100%
rename from dbms/tests/queries/0_stateless/00698_validate_array_sizes_for_nested_kshvakov.reference
rename to dbms/src/Compression/CMakeLists.txt
diff --git a/dbms/src/Compression/CompressionCodecLZ4.cpp b/dbms/src/Compression/CompressionCodecLZ4.cpp
new file mode 100644
index 000000000..610b81a69
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecLZ4.cpp
@@ -0,0 +1,50 @@
+#include <Compression/CompressionCodecLZ4.h>
+#include <lz4.h>
+#include <lz4hc.h>
+#include <IO/CompressedStream.h>
+#include <Compression/CompressionFactory.h>
+#include "CompressionCodecLZ4.h"
+
+
+namespace DB
+{
+
+char CompressionCodecLZ4::getMethodByte()
+{
+    return static_cast<char>(CompressionMethodByte::LZ4);
+}
+
+void CompressionCodecLZ4::getCodecDesc(String & codec_desc)
+{
+    codec_desc = "LZ4";
+}
+
+void CompressionCodecLZ4::compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size)
+{
+    static constexpr size_t header_size = 1 + sizeof(UInt32) + sizeof(UInt32);
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wold-style-cast"
+    compressed_buf.resize(header_size + LZ4_COMPRESSBOUND(uncompressed_size));
+#pragma GCC diagnostic pop
+
+    compressed_size = header_size + LZ4_compress_default(
+        uncompressed_buf,
+        &compressed_buf[header_size],
+        uncompressed_size,
+        LZ4_COMPRESSBOUND(uncompressed_size));
+
+    UInt32 compressed_size_32 = compressed_size;
+    UInt32 uncompressed_size_32 = uncompressed_size;
+    unalignedStore(&compressed_buf[1], compressed_size_32);
+    unalignedStore(&compressed_buf[5], uncompressed_size_32);
+}
+
+void registerCodecLZ4(CompressionCodecFactory & factory)
+{
+    factory.registerSimpleCompressionCodec("LZ4", static_cast<char>(CompressionMethodByte::LZ4), [&](){
+        return std::make_shared<CompressionCodecLZ4>();
+    });
+}
+
+}
diff --git a/dbms/src/Compression/CompressionCodecLZ4.h b/dbms/src/Compression/CompressionCodecLZ4.h
new file mode 100644
index 000000000..9ba8da3c3
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecLZ4.h
@@ -0,0 +1,21 @@
+#pragma once
+
+#include <IO/WriteBuffer.h>
+#include <Compression/ICompressionCodec.h>
+#include <IO/BufferWithOwnMemory.h>
+#include <Parsers/StringRange.h>
+
+namespace DB
+{
+
+class CompressionCodecLZ4 : public ICompressionCodec
+{
+public:
+    char getMethodByte() override;
+
+    void getCodecDesc(String & codec_desc) override;
+
+    void compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size) override;
+};
+
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/CompressionCodecMultiple.cpp b/dbms/src/Compression/CompressionCodecMultiple.cpp
new file mode 100644
index 000000000..b866aebbc
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecMultiple.cpp
@@ -0,0 +1,50 @@
+#include <Compression/CompressionCodecMultiple.h>
+#include <IO/CompressedStream.h>
+#include "CompressionCodecMultiple.h"
+
+
+namespace DB
+{
+
+CompressionCodecMultiple::CompressionCodecMultiple(Codecs codecs)
+    : codecs(codecs)
+{
+    /// TODO initialize inner_methods_code
+    for (size_t idx = 0; idx < codecs.size(); idx++)
+    {
+        if (idx != 0)
+            codec_desc = codec_desc + ',';
+
+        const auto codec = codecs[idx];
+        String inner_codec_desc;
+        codec->getCodecDesc(inner_codec_desc);
+        codec_desc = codec_desc + inner_codec_desc;
+    }
+}
+
+char CompressionCodecMultiple::getMethodByte()
+{
+    return static_cast<char>(CompressionMethodByte::Multiple);
+}
+
+void CompressionCodecMultiple::getCodecDesc(String & codec_desc_)
+{
+    codec_desc_ = codec_desc;
+}
+
+void CompressionCodecMultiple::compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size)
+{
+    char reserve = 1;
+
+    PODArray<char> un_compressed_buf(uncompressed_size);
+    un_compressed_buf.emplace_back(reserve);
+    un_compressed_buf.insert(uncompressed_buf, uncompressed_buf + uncompressed_size);
+
+    for (auto & codec : codecs)
+    {
+        codec->compress(&un_compressed_buf[1], uncompressed_size, compressed_buf, compressed_size);
+        uncompressed_size = compressed_size;
+        compressed_buf.swap(un_compressed_buf);
+    }
+}
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/CompressionCodecMultiple.h b/dbms/src/Compression/CompressionCodecMultiple.h
new file mode 100644
index 000000000..2fc0aef2e
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecMultiple.h
@@ -0,0 +1,25 @@
+#pragma once
+
+#include <Compression/ICompressionCodec.h>
+
+namespace DB
+{
+
+class CompressionCodecMultiple final : public ICompressionCodec
+{
+public:
+    CompressionCodecMultiple(Codecs codecs);
+
+    char getMethodByte() override;
+
+    void getCodecDesc(String & codec_desc) override;
+
+    void compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size) override;
+
+private:
+    Codecs codecs;
+    String codec_desc;
+
+};
+
+}
diff --git a/dbms/src/Compression/CompressionCodecNone.cpp b/dbms/src/Compression/CompressionCodecNone.cpp
new file mode 100644
index 000000000..49e4e4f01
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecNone.cpp
@@ -0,0 +1,41 @@
+#include <Compression/CompressionCodecNone.h>
+#include <IO/CompressedStream.h>
+#include <Compression/CompressionFactory.h>
+
+
+namespace DB
+{
+
+char CompressionCodecNone::getMethodByte()
+{
+    return static_cast<char>(CompressionMethodByte::NONE);
+}
+
+void CompressionCodecNone::getCodecDesc(String & codec_desc)
+{
+    codec_desc = "NONE";
+}
+
+void CompressionCodecNone::compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size)
+{
+    static constexpr size_t header_size = 1 + sizeof(UInt32) + sizeof(UInt32);
+
+    compressed_size = header_size + uncompressed_size;
+    UInt32 uncompressed_size_32 = uncompressed_size;
+    UInt32 compressed_size_32 = compressed_size;
+
+    compressed_buf.resize(compressed_size);
+
+    unalignedStore(&compressed_buf[1], compressed_size_32);
+    unalignedStore(&compressed_buf[5], uncompressed_size_32);
+    memcpy(&compressed_buf[header_size], uncompressed_buf, uncompressed_size);
+}
+
+void registerCodecNone(CompressionCodecFactory & factory)
+{
+    factory.registerSimpleCompressionCodec("NONE", static_cast<char>(CompressionMethodByte::NONE), [&](){
+        return std::make_shared<CompressionCodecNone>();
+    });
+}
+
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/CompressionCodecNone.h b/dbms/src/Compression/CompressionCodecNone.h
new file mode 100644
index 000000000..7cc205139
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecNone.h
@@ -0,0 +1,21 @@
+#pragma once
+
+#include <IO/WriteBuffer.h>
+#include <Compression/ICompressionCodec.h>
+#include <IO/BufferWithOwnMemory.h>
+#include <Parsers/StringRange.h>
+
+namespace DB
+{
+
+class CompressionCodecNone : public ICompressionCodec
+{
+public:
+    char getMethodByte() override;
+
+    void getCodecDesc(String & codec_desc) override;
+
+    void compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size) override;
+};
+
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/CompressionCodecZSTD.cpp b/dbms/src/Compression/CompressionCodecZSTD.cpp
new file mode 100644
index 000000000..7509d4eb6
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecZSTD.cpp
@@ -0,0 +1,76 @@
+#include <Compression/CompressionCodecZSTD.h>
+#include <IO/CompressedStream.h>
+#include <Compression/CompressionFactory.h>
+#include <zstd.h>
+#include <Core/Field.h>
+#include <Parsers/IAST.h>
+#include <Parsers/ASTLiteral.h>
+#include <Common/typeid_cast.h>
+
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int CANNOT_COMPRESS;
+}
+
+char CompressionCodecZSTD::getMethodByte()
+{
+    return static_cast<char>(CompressionMethodByte::ZSTD);
+}
+
+void CompressionCodecZSTD::getCodecDesc(String & codec_desc)
+{
+    codec_desc = "ZSTD";
+}
+
+void CompressionCodecZSTD::compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size)
+{
+    static constexpr size_t header_size = 1 + sizeof(UInt32) + sizeof(UInt32);
+
+    compressed_buf.resize(header_size + ZSTD_compressBound(uncompressed_size));
+
+    size_t res = ZSTD_compress(
+        &compressed_buf[header_size],
+        compressed_buf.size() - header_size,
+        uncompressed_buf,
+        uncompressed_size,
+        level);
+
+    if (ZSTD_isError(res))
+        throw Exception("Cannot compress block with ZSTD: " + std::string(ZSTD_getErrorName(res)), ErrorCodes::CANNOT_COMPRESS);
+
+    compressed_size = header_size + res;
+
+    UInt32 compressed_size_32 = compressed_size;
+    UInt32 uncompressed_size_32 = uncompressed_size;
+
+    unalignedStore(&compressed_buf[1], compressed_size_32);
+    unalignedStore(&compressed_buf[5], uncompressed_size_32);
+}
+
+CompressionCodecZSTD::CompressionCodecZSTD(int level)
+    :level(level)
+{
+}
+
+void registerCodecZSTD(CompressionCodecFactory & factory)
+{
+    UInt8 method_code = static_cast<char>(CompressionMethodByte::ZSTD);
+    factory.registerCompressionCodec("ZSTD", method_code, [&](const ASTPtr & arguments) -> CompressionCodecPtr
+    {
+        int level = 0;
+        if (arguments && !arguments->children.empty())
+        {
+            const auto children = arguments->children;
+            const ASTLiteral * literal = static_cast<const ASTLiteral *>(children[0].get());
+            level = literal->value.safeGet<UInt64>();
+        }
+
+        return std::make_shared<CompressionCodecZSTD>(level);
+    });
+}
+
+}
diff --git a/dbms/src/Compression/CompressionCodecZSTD.h b/dbms/src/Compression/CompressionCodecZSTD.h
new file mode 100644
index 000000000..65a43be64
--- /dev/null
+++ b/dbms/src/Compression/CompressionCodecZSTD.h
@@ -0,0 +1,26 @@
+#pragma once
+
+#include <IO/WriteBuffer.h>
+#include <Compression/ICompressionCodec.h>
+#include <IO/BufferWithOwnMemory.h>
+#include <Parsers/StringRange.h>
+
+namespace DB
+{
+
+class CompressionCodecZSTD : public ICompressionCodec
+{
+public:
+    CompressionCodecZSTD(int level);
+
+    char getMethodByte() override;
+
+    void getCodecDesc(String & codec_desc) override;
+
+    void compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size) override;
+
+private:
+    int level;
+};
+
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/CompressionFactory.cpp b/dbms/src/Compression/CompressionFactory.cpp
new file mode 100644
index 000000000..9d10b54b0
--- /dev/null
+++ b/dbms/src/Compression/CompressionFactory.cpp
@@ -0,0 +1,110 @@
+#include <Compression/CompressionFactory.h>
+#include <Parsers/parseQuery.h>
+#include <Parsers/ParserCreateQuery.h>
+#include <Parsers/ASTFunction.h>
+#include <Parsers/ASTIdentifier.h>
+#include <Parsers/ASTLiteral.h>
+#include <Common/typeid_cast.h>
+#include <Poco/String.h>
+#include <IO/ReadBuffer.h>
+#include <Parsers/queryToString.h>
+#include <Compression/CompressionCodecMultiple.h>
+#include <Compression/CompressionCodecLZ4.h>
+#include <Compression/CompressionCodecNone.h>
+
+namespace DB
+{
+namespace ErrorCodes
+{
+    extern const int LOGICAL_ERROR;
+    extern const int ILLEGAL_SYNTAX_FOR_CODEC_TYPE;
+    extern const int UNEXPECTED_AST_STRUCTURE;
+    extern const int UNKNOWN_CODEC;
+    extern const int DATA_TYPE_CANNOT_HAVE_ARGUMENTS;
+}
+
+CompressionCodecPtr CompressionCodecFactory::getDefaultCodec() const
+{
+    return default_codec;
+}
+
+CompressionCodecPtr CompressionCodecFactory::get(const ASTPtr & ast) const
+{
+    if (const auto * func = typeid_cast<const ASTFunction *>(ast.get()))
+    {
+        if (func->parameters)
+            throw Exception("Compression codec cannot have multiple parenthesed parameters.", ErrorCodes::ILLEGAL_SYNTAX_FOR_CODEC_TYPE);
+
+        if (Poco::toLower(func->name) != "codec")
+            throw Exception("", ErrorCodes::UNKNOWN_CODEC);
+
+        Codecs codecs;
+        codecs.reserve(func->arguments->children.size());
+        for (const auto & inner_codec_ast : func->arguments->children)
+        {
+            if (const auto * family_name = typeid_cast<const ASTIdentifier *>(inner_codec_ast.get()))
+                codecs.emplace_back(getImpl(family_name->name, {}));
+            else if (const auto * ast_func = typeid_cast<const ASTFunction *>(inner_codec_ast.get()))
+                codecs.emplace_back(getImpl(ast_func->name, ast_func->arguments));
+            else
+                throw Exception("Unexpected AST element for compression codec.", ErrorCodes::UNEXPECTED_AST_STRUCTURE);
+        }
+
+        if (codecs.size() == 1)
+            return codecs.back();
+        else if (codecs.size() > 1)
+            return std::make_shared<CompressionCodecMultiple>(codecs);
+    }
+
+    throw Exception("Unknown codec expression : " + queryToString(ast), ErrorCodes::UNKNOWN_CODEC);
+}
+
+CompressionCodecPtr CompressionCodecFactory::getImpl(const String & family_name, const ASTPtr & arguments) const
+{
+    const auto family_and_creator = family_name_with_codec.find(family_name);
+
+    if (family_and_creator == family_name_with_codec.end())
+        throw Exception("Unknown codec family: " + family_name, ErrorCodes::UNKNOWN_CODEC);
+
+    return family_and_creator->second(arguments);
+}
+
+void CompressionCodecFactory::registerCompressionCodec(const String & family_name, UInt8 byte_code, Creator creator)
+{
+    if (creator == nullptr)
+        throw Exception("CompressionCodecFactory: the codec family " + family_name + " has been provided a null constructor",
+                        ErrorCodes::LOGICAL_ERROR);
+
+    if (!family_name_with_codec.emplace(family_name, creator).second)
+        throw Exception("CompressionCodecFactory: the codec family name '" + family_name + "' is not unique", ErrorCodes::LOGICAL_ERROR);
+
+    if (!family_code_with_codec.emplace(byte_code, creator).second)
+        throw Exception("CompressionCodecFactory: the codec family name '" + family_name + "' is not unique", ErrorCodes::LOGICAL_ERROR);
+}
+
+void CompressionCodecFactory::registerSimpleCompressionCodec(const String & family_name, UInt8 byte_code,
+                                                                 std::function<CompressionCodecPtr()> creator)
+{
+    registerCompressionCodec(family_name, byte_code, [family_name, creator](const ASTPtr & ast)
+    {
+        if (ast)
+            throw Exception("Data type " + family_name + " cannot have arguments", ErrorCodes::DATA_TYPE_CANNOT_HAVE_ARGUMENTS);
+        return creator();
+    });
+}
+
+void registerCodecLZ4(CompressionCodecFactory & factory);
+void registerCodecNone(CompressionCodecFactory & factory);
+void registerCodecZSTD(CompressionCodecFactory & factory);
+//void registerCodecDelta(CompressionCodecFactory & factory);
+
+CompressionCodecFactory::CompressionCodecFactory()
+{
+    default_codec = std::make_shared<CompressionCodecLZ4>();
+    registerCodecLZ4(*this);
+    registerCodecNone(*this);
+    registerCodecZSTD(*this);
+//    registerCodecDelta(*this);
+}
+
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/CompressionFactory.h b/dbms/src/Compression/CompressionFactory.h
new file mode 100644
index 000000000..ecc03053c
--- /dev/null
+++ b/dbms/src/Compression/CompressionFactory.h
@@ -0,0 +1,54 @@
+#pragma once
+
+#include <memory>
+#include <functional>
+#include <unordered_map>
+#include <ext/singleton.h>
+#include <Common/IFactoryWithAliases.h>
+#include <Compression/ICompressionCodec.h>
+#include <IO/CompressedStream.h>
+
+namespace DB
+{
+
+class ICompressionCodec;
+
+using CompressionCodecPtr = std::shared_ptr<ICompressionCodec>;
+
+class IAST;
+
+using ASTPtr = std::shared_ptr<IAST>;
+
+/** Creates a codec object by name of compression algorithm family and parameters.
+ */
+class CompressionCodecFactory final : public ext::singleton<CompressionCodecFactory>
+{
+protected:
+    using Creator = std::function<CompressionCodecPtr(const ASTPtr & parameters)>;
+    using SimpleCreator = std::function<CompressionCodecPtr()>;
+    using CompressionCodecsDictionary = std::unordered_map<String, Creator>;
+    using CompressionCodecsCodeDictionary = std::unordered_map<UInt8, Creator>;
+public:
+
+    CompressionCodecPtr getDefaultCodec() const;
+
+    CompressionCodecPtr get(const ASTPtr & ast) const;
+
+    void registerCompressionCodec(const String & family_name, UInt8 byte_code, Creator creator);
+
+    void registerSimpleCompressionCodec(const String & family_name, UInt8 byte_code, SimpleCreator creator);
+
+protected:
+    CompressionCodecPtr getImpl(const String & family_name, const ASTPtr & arguments) const;
+
+private:
+    CompressionCodecsDictionary family_name_with_codec;
+    CompressionCodecsCodeDictionary family_code_with_codec;
+    CompressionCodecPtr default_codec;
+
+    CompressionCodecFactory();
+
+    friend class ext::singleton<CompressionCodecFactory>;
+};
+
+}
\ No newline at end of file
diff --git a/dbms/src/Compression/ICompressionCodec.cpp b/dbms/src/Compression/ICompressionCodec.cpp
new file mode 100644
index 000000000..e4419ea28
--- /dev/null
+++ b/dbms/src/Compression/ICompressionCodec.cpp
@@ -0,0 +1,71 @@
+#include <Compression/ICompressionCodec.h>
+#include <IO/LZ4_decompress_faster.h>
+#include "ICompressionCodec.h"
+
+
+namespace DB
+{
+
+ReadBufferPtr ICompressionCodec::liftCompressed(ReadBuffer & origin)
+{
+    return std::make_shared<LazyLiftCompressedReadBuffer>(*this, origin);
+}
+
+WriteBufferPtr ICompressionCodec::liftCompressed(WriteBuffer & origin)
+{
+    return std::make_shared<LiftedCompressedWriteBuffer>(*this, origin);
+}
+
+LazyLiftCompressedReadBuffer::LazyLiftCompressedReadBuffer(ICompressionCodec & /*codec*/, ReadBuffer & /*origin*/)
+{
+}
+
+bool LazyLiftCompressedReadBuffer::nextImpl()
+{
+//    size_t size_decompressed;
+//    size_t size_compressed_without_checksum;
+//    size_compressed = readCompressedData(size_decompressed, size_compressed_without_checksum);
+//    if (!size_compressed)
+//        return false;
+//
+//    memory.resize(size_decompressed + LZ4::ADDITIONAL_BYTES_AT_END_OF_BUFFER);
+//    working_buffer = Buffer(memory.data(), &memory[size_decompressed]);
+//
+//    decompress(working_buffer.begin(), size_decompressed, size_compressed_without_checksum);
+//
+    return true;
+}
+
+LiftedCompressedWriteBuffer::~LiftedCompressedWriteBuffer()
+{
+    try
+    {
+        next();
+    }
+    catch (...)
+    {
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+    }
+}
+
+void LiftedCompressedWriteBuffer::nextImpl()
+{
+    if (!offset())
+        return;
+
+    size_t compressed_size = 0;
+    size_t uncompressed_size = offset();
+    compressed_buffer.emplace_back(compression_codec.getMethodByte());
+    compression_codec.compress(working_buffer.begin(), uncompressed_size, compressed_buffer, compressed_size);
+    CityHash_v1_0_2::uint128 checksum = CityHash_v1_0_2::CityHash128(compressed_buffer.data(), compressed_size);
+
+    out.write(reinterpret_cast<const char *>(&checksum), sizeof(checksum));
+    out.write(compressed_buffer.data(), compressed_size);
+}
+
+LiftedCompressedWriteBuffer::LiftedCompressedWriteBuffer(ICompressionCodec & compression_codec, WriteBuffer & out, size_t buf_size)
+    : BufferWithOwnMemory<WriteBuffer>(buf_size), out(out), compression_codec(compression_codec)
+{
+}
+
+}
diff --git a/dbms/src/Compression/ICompressionCodec.h b/dbms/src/Compression/ICompressionCodec.h
new file mode 100644
index 000000000..ea1818c65
--- /dev/null
+++ b/dbms/src/Compression/ICompressionCodec.h
@@ -0,0 +1,64 @@
+#pragma once
+
+#include <memory>
+#include <Core/Field.h>
+#include <IO/ReadBuffer.h>
+#include <IO/WriteBuffer.h>
+#include <IO/BufferWithOwnMemory.h>
+#include <Common/PODArray.h>
+#include <DataTypes/IDataType.h>
+#include <boost/noncopyable.hpp>
+
+namespace DB
+{
+
+class ICompressionCodec;
+
+using CompressionCodecPtr = std::shared_ptr<ICompressionCodec>;
+using Codecs = std::vector<CompressionCodecPtr>;
+
+class LiftedCompressedWriteBuffer : public BufferWithOwnMemory<WriteBuffer>
+{
+public:
+    LiftedCompressedWriteBuffer(ICompressionCodec & compression_codec, WriteBuffer & out, size_t buf_size = DBMS_DEFAULT_BUFFER_SIZE);
+
+    ~LiftedCompressedWriteBuffer() override;
+
+private:
+    void nextImpl() override;
+
+
+private:
+    WriteBuffer & out;
+    ICompressionCodec & compression_codec;
+    PODArray<char> compressed_buffer;
+};
+
+class LazyLiftCompressedReadBuffer : public BufferWithOwnMemory<ReadBuffer>
+{
+private:
+    bool nextImpl() override;
+public:
+    LazyLiftCompressedReadBuffer(ICompressionCodec & codec, ReadBuffer & origin);
+};
+
+/**
+*
+*/
+class ICompressionCodec : private boost::noncopyable
+{
+public:
+    virtual ~ICompressionCodec() = default;
+
+    ReadBufferPtr liftCompressed(ReadBuffer & origin);
+
+    WriteBufferPtr liftCompressed(WriteBuffer & origin);
+
+    virtual char getMethodByte() = 0;
+
+    virtual void getCodecDesc(String & codec_desc) = 0;
+
+    virtual void compress(char * uncompressed_buf, size_t uncompressed_size, PODArray<char> & compressed_buf, size_t & compressed_size) = 0;
+};
+
+}
diff --git a/dbms/src/Core/Block.h b/dbms/src/Core/Block.h
index fa0038e41..27b107eeb 100644
--- a/dbms/src/Core/Block.h
+++ b/dbms/src/Core/Block.h
@@ -19,7 +19,7 @@ namespace DB
   * This is unit of data processing.
   * Also contains metadata - data types of columns and their names
   *  (either original names from a table, or generated names during temporary calculations).
-  * Allows to insert, remove columns in arbitrary position, to change order of columns.
+  * Allows to insert, remove columns in arbitary position, to change order of columns.
   */
 
 class Context;
diff --git a/dbms/src/Core/Defines.h b/dbms/src/Core/Defines.h
index cf7a0b621..d7a2ca419 100644
--- a/dbms/src/Core/Defines.h
+++ b/dbms/src/Core/Defines.h
@@ -47,10 +47,6 @@
 #define DBMS_MIN_REVISION_WITH_SERVER_DISPLAY_NAME 54372
 #define DBMS_MIN_REVISION_WITH_VERSION_PATCH 54401
 #define DBMS_MIN_REVISION_WITH_SERVER_LOGS 54406
-/// Minimum revision with exactly the same set of aggregation methods and rules to select them.
-/// Two-level (bucketed) aggregation is incompatible if servers are inconsistent in these rules
-/// (keys will be placed in different buckets and result will not be fully aggregated).
-#define DBMS_MIN_REVISION_WITH_CURRENT_AGGREGATION_VARIANT_SELECTION_METHOD 54408
 
 /// Version of ClickHouse TCP protocol. Set to git tag with latest protocol change.
 #define DBMS_TCP_PROTOCOL_VERSION 54226
diff --git a/dbms/src/Core/Field.cpp b/dbms/src/Core/Field.cpp
index 9a7fc60f4..b948a8340 100644
--- a/dbms/src/Core/Field.cpp
+++ b/dbms/src/Core/Field.cpp
@@ -142,7 +142,80 @@ namespace DB
 
 namespace DB
 {
-    inline void readBinary(Tuple & x_def, ReadBuffer & buf)
+    //TODO inline
+    void readBinary(Tuple & x_def, ReadBuffer & buf)
+    {
+        auto & x = x_def.toUnderType();
+        size_t size;
+        DB::readBinary(size, buf);
+
+        for (size_t index = 0; index < size; ++index)
+        {
+            UInt8 type;
+            DB::readBinary(type, buf);
+
+            switch (type)
+            {
+                case Field::Types::Null:
+                {
+                    x.push_back(DB::Field());
+                    break;
+                }
+                case Field::Types::UInt64:
+                {
+                    UInt64 value;
+                    DB::readVarUInt(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+                case Field::Types::UInt128:
+                {
+                    UInt128 value;
+                    DB::readBinary(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+                case Field::Types::Int64:
+                {
+                    Int64 value;
+                    DB::readVarInt(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+                case Field::Types::Float64:
+                {
+                    Float64 value;
+                    DB::readFloatBinary(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+                case Field::Types::String:
+                {
+                    std::string value;
+                    DB::readStringBinary(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+                case Field::Types::Array:
+                {
+                    Array value;
+                    DB::readBinary(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+                case Field::Types::Tuple:
+                {
+                    Tuple value;
+                    DB::readBinary(value, buf);
+                    x.push_back(value);
+                    break;
+                }
+            };
+        }
+    }
+
+    void readTuple(Tuple & x_def, ReadBuffer & buf)
+
     {
         auto & x = x_def.toUnderType();
         size_t size;
diff --git a/dbms/src/Core/Field.h b/dbms/src/Core/Field.h
index ca8bd2fb2..0b39e1a13 100644
--- a/dbms/src/Core/Field.h
+++ b/dbms/src/Core/Field.h
@@ -638,6 +638,7 @@ void writeText(const Array & x, WriteBuffer & buf);
 inline void writeQuoted(const Array &, WriteBuffer &) { throw Exception("Cannot write Array quoted.", ErrorCodes::NOT_IMPLEMENTED); }
 
 void readBinary(Tuple & x, ReadBuffer & buf);
+void readTuple(Tuple & x, ReadBuffer & buf);
 
 inline void readText(Tuple &, ReadBuffer &) { throw Exception("Cannot read Tuple.", ErrorCodes::NOT_IMPLEMENTED); }
 inline void readQuoted(Tuple &, ReadBuffer &) { throw Exception("Cannot read Tuple.", ErrorCodes::NOT_IMPLEMENTED); }
diff --git a/dbms/src/Core/callOnTypeIndex.h b/dbms/src/Core/callOnTypeIndex.h
index 8f4424ec0..e531e64d1 100644
--- a/dbms/src/Core/callOnTypeIndex.h
+++ b/dbms/src/Core/callOnTypeIndex.h
@@ -16,7 +16,7 @@ struct TypePair
 
 
 
-template <typename T, bool _int, bool _float, bool _decimal, bool _datetime, typename F>
+template <typename T, bool _int, bool _float, bool _dec, typename F>
 bool callOnBasicType(TypeIndex number, F && f)
 {
     if constexpr (_int)
@@ -40,7 +40,7 @@ bool callOnBasicType(TypeIndex number, F && f)
         }
     }
 
-    if constexpr (_decimal)
+    if constexpr (_dec)
     {
         switch (number)
         {
@@ -63,51 +63,40 @@ bool callOnBasicType(TypeIndex number, F && f)
         }
     }
 
-    if constexpr (_datetime)
-    {
-        switch (number)
-        {
-            case TypeIndex::Date:         return f(TypePair<T, UInt16>());
-            case TypeIndex::DateTime:     return f(TypePair<T, UInt32>());
-            default:
-                break;
-        }
-    }
-
     return false;
 }
 
 /// Unroll template using TypeIndex
-template <bool _int, bool _float, bool _decimal, bool _datetime, typename F>
+template <bool _int, bool _float, bool _dec, typename F>
 inline bool callOnBasicTypes(TypeIndex type_num1, TypeIndex type_num2, F && f)
 {
     if constexpr (_int)
     {
         switch (type_num1)
         {
-            case TypeIndex::UInt8: return callOnBasicType<UInt8, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::UInt16: return callOnBasicType<UInt16, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::UInt32: return callOnBasicType<UInt32, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::UInt64: return callOnBasicType<UInt64, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            //case TypeIndex::UInt128: return callOnBasicType<UInt128, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-
-            case TypeIndex::Int8: return callOnBasicType<Int8, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Int16: return callOnBasicType<Int16, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Int32: return callOnBasicType<Int32, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Int64: return callOnBasicType<Int64, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Int128: return callOnBasicType<Int128, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
+            case TypeIndex::UInt8: return callOnBasicType<UInt8, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::UInt16: return callOnBasicType<UInt16, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::UInt32: return callOnBasicType<UInt32, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::UInt64: return callOnBasicType<UInt64, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            //case TypeIndex::UInt128: return callOnBasicType<UInt128, _int, _float, _dec>(type_num2, std::forward<F>(f));
+
+            case TypeIndex::Int8: return callOnBasicType<Int8, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Int16: return callOnBasicType<Int16, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Int32: return callOnBasicType<Int32, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Int64: return callOnBasicType<Int64, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Int128: return callOnBasicType<Int128, _int, _float, _dec>(type_num2, std::forward<F>(f));
             default:
                 break;
         }
     }
 
-    if constexpr (_decimal)
+    if constexpr (_dec)
     {
         switch (type_num1)
         {
-            case TypeIndex::Decimal32: return callOnBasicType<Decimal32, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Decimal64: return callOnBasicType<Decimal64, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Decimal128: return callOnBasicType<Decimal128, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
+            case TypeIndex::Decimal32: return callOnBasicType<Decimal32, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Decimal64: return callOnBasicType<Decimal64, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Decimal128: return callOnBasicType<Decimal128, _int, _float, _dec>(type_num2, std::forward<F>(f));
             default:
                 break;
         }
@@ -117,19 +106,8 @@ inline bool callOnBasicTypes(TypeIndex type_num1, TypeIndex type_num2, F && f)
     {
         switch (type_num1)
         {
-            case TypeIndex::Float32: return callOnBasicType<Float32, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::Float64: return callOnBasicType<Float64, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            default:
-                break;
-        }
-    }
-
-    if constexpr (_datetime)
-    {
-        switch (type_num1)
-        {
-            case TypeIndex::Date: return callOnBasicType<UInt16, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
-            case TypeIndex::DateTime: return callOnBasicType<UInt32, _int, _float, _decimal, _datetime>(type_num2, std::forward<F>(f));
+            case TypeIndex::Float32: return callOnBasicType<Float32, _int, _float, _dec>(type_num2, std::forward<F>(f));
+            case TypeIndex::Float64: return callOnBasicType<Float64, _int, _float, _dec>(type_num2, std::forward<F>(f));
             default:
                 break;
         }
diff --git a/dbms/src/DataStreams/GraphiteRollupSortedBlockInputStream.cpp b/dbms/src/DataStreams/GraphiteRollupSortedBlockInputStream.cpp
index a2740dabd..294f98d20 100644
--- a/dbms/src/DataStreams/GraphiteRollupSortedBlockInputStream.cpp
+++ b/dbms/src/DataStreams/GraphiteRollupSortedBlockInputStream.cpp
@@ -132,7 +132,7 @@ void GraphiteRollupSortedBlockInputStream::merge(MutableColumns & merged_columns
 
         is_first = false;
 
-        time_t next_row_time = next_cursor->all_columns[time_column_num]->getUInt(next_cursor->pos);
+        time_t next_row_time = next_cursor->all_columns[time_column_num]->get64(next_cursor->pos);
         /// Is new key before rounding.
         bool is_new_key = new_path || next_row_time != current_time;
 
diff --git a/dbms/src/DataStreams/SummingSortedBlockInputStream.cpp b/dbms/src/DataStreams/SummingSortedBlockInputStream.cpp
index d2af46f19..00f3f55c3 100644
--- a/dbms/src/DataStreams/SummingSortedBlockInputStream.cpp
+++ b/dbms/src/DataStreams/SummingSortedBlockInputStream.cpp
@@ -216,7 +216,7 @@ void SummingSortedBlockInputStream::insertCurrentRowIfNeeded(MutableColumns & me
                     if (desc.column_numbers.size() == 1)
                     {
                         // Flag row as non-empty if at least one column number if non-zero
-                        current_row_is_zero = current_row_is_zero && desc.merged_column->isDefaultAt(desc.merged_column->size() - 1);
+                        current_row_is_zero = current_row_is_zero && desc.merged_column->get64(desc.merged_column->size() - 1) == 0;
                     }
                     else
                     {
diff --git a/dbms/src/DataTypes/DataTypeDateTime.cpp b/dbms/src/DataTypes/DataTypeDateTime.cpp
index 519f419e8..d012d45d2 100644
--- a/dbms/src/DataTypes/DataTypeDateTime.cpp
+++ b/dbms/src/DataTypes/DataTypeDateTime.cpp
@@ -172,7 +172,6 @@ static DataTypePtr create(const ASTPtr & arguments)
 void registerDataTypeDateTime(DataTypeFactory & factory)
 {
     factory.registerDataType("DateTime", create, DataTypeFactory::CaseInsensitive);
-    factory.registerAlias("TIMESTAMP", "DateTime", DataTypeFactory::CaseInsensitive);
 }
 
 
diff --git a/dbms/src/DataTypes/IDataType.h b/dbms/src/DataTypes/IDataType.h
index 727d80540..e3f02c9b4 100644
--- a/dbms/src/DataTypes/IDataType.h
+++ b/dbms/src/DataTypes/IDataType.h
@@ -304,7 +304,7 @@ public:
     virtual bool shouldAlignRightInPrettyFormats() const { return false; }
 
     /** Does formatted value in any text format can contain anything but valid UTF8 sequences.
-      * Example: String (because it can contain arbitrary bytes).
+      * Example: String (because it can contain arbitary bytes).
       * Counterexamples: numbers, Date, DateTime.
       * For Enum, it depends.
       */
diff --git a/dbms/src/DataTypes/NumberTraits.h b/dbms/src/DataTypes/NumberTraits.h
index 3ebe94200..099a7199a 100644
--- a/dbms/src/DataTypes/NumberTraits.h
+++ b/dbms/src/DataTypes/NumberTraits.h
@@ -1,9 +1,7 @@
 #pragma once
 
-#include <type_traits>
-
 #include <Core/Types.h>
-#include <Common/UInt128.h>
+#include <type_traits>
 
 
 namespace DB
@@ -148,7 +146,6 @@ template <typename A> struct ResultOfBitNot
   * UInt<x>,  Int<y>    ->   Int<max(x*2, y)>
   * Float<x>, [U]Int<y> -> Float<max(x, y*2)>
   * Decimal<x>, Decimal<y> -> Decimal<max(x,y)>
-  * UUID, UUID          -> UUID
   * UInt64 ,  Int<x>    -> Error
   * Float<x>, [U]Int64  -> Error
   */
@@ -171,9 +168,7 @@ struct ResultOfIf
                 ? max(sizeof(A), sizeof(B)) * 2
                 : max(sizeof(A), sizeof(B))>::Type;
 
-    using ConstructedWithUUID = std::conditional_t<std::is_same_v<A, UInt128> && std::is_same_v<B, UInt128>, A, ConstructedType>;
-
-    using Type = std::conditional_t<!IsDecimalNumber<A> && !IsDecimalNumber<B>, ConstructedWithUUID,
+    using Type = std::conditional_t<!IsDecimalNumber<A> && !IsDecimalNumber<B>, ConstructedType,
         std::conditional_t<IsDecimalNumber<A> && IsDecimalNumber<B>, std::conditional_t<(sizeof(A) > sizeof(B)), A, B>, Error>>;
 };
 
diff --git a/dbms/src/Databases/DatabaseOrdinary.cpp b/dbms/src/Databases/DatabaseOrdinary.cpp
index 7bb4ae298..43cb96ec0 100644
--- a/dbms/src/Databases/DatabaseOrdinary.cpp
+++ b/dbms/src/Databases/DatabaseOrdinary.cpp
@@ -1,6 +1,5 @@
 #include <iomanip>
 
-#include <Poco/Event.h>
 #include <Poco/DirectoryIterator.h>
 #include <common/logger_useful.h>
 
@@ -42,6 +41,7 @@ namespace ErrorCodes
 static constexpr size_t PRINT_MESSAGE_EACH_N_TABLES = 256;
 static constexpr size_t PRINT_MESSAGE_EACH_N_SECONDS = 5;
 static constexpr size_t METADATA_FILE_BUFFER_SIZE = 32768;
+static constexpr size_t TABLES_PARALLEL_LOAD_BUNCH_SIZE = 100;
 
 namespace detail
 {
@@ -149,9 +149,6 @@ void DatabaseOrdinary::loadTables(
                 ErrorCodes::INCORRECT_FILE_NAME);
     }
 
-    if (file_names.empty())
-        return;
-
     /** Tables load faster if they are loaded in sorted (by name) order.
       * Otherwise (for the ext4 filesystem), `DirectoryIterator` iterates through them in some order,
       *  which does not correspond to order tables creation and does not correspond to order of their location on disk.
@@ -163,27 +160,36 @@ void DatabaseOrdinary::loadTables(
 
     AtomicStopwatch watch;
     std::atomic<size_t> tables_processed {0};
-    Poco::Event all_tables_processed;
 
-    auto task_function = [&](const String & table)
+    auto task_function = [&](FileNames::const_iterator begin, FileNames::const_iterator end)
     {
-        /// Messages, so that it's not boring to wait for the server to load for a long time.
-        if ((tables_processed + 1) % PRINT_MESSAGE_EACH_N_TABLES == 0
-            || watch.compareAndRestart(PRINT_MESSAGE_EACH_N_SECONDS))
+        for (auto it = begin; it != end; ++it)
         {
-            LOG_INFO(log, std::fixed << std::setprecision(2) << tables_processed * 100.0 / total_tables << "%");
-            watch.restart();
-        }
+            const String & table = *it;
 
-        loadTable(context, metadata_path, *this, name, data_path, table, has_force_restore_data_flag);
+            /// Messages, so that it's not boring to wait for the server to load for a long time.
+            if ((++tables_processed) % PRINT_MESSAGE_EACH_N_TABLES == 0
+                || watch.compareAndRestart(PRINT_MESSAGE_EACH_N_SECONDS))
+            {
+                LOG_INFO(log, std::fixed << std::setprecision(2) << tables_processed * 100.0 / total_tables << "%");
+                watch.restart();
+            }
 
-        if (++tables_processed == total_tables)
-            all_tables_processed.set();
+            loadTable(context, metadata_path, *this, name, data_path, table, has_force_restore_data_flag);
+        }
     };
 
-    for (const auto & filename : file_names)
+    const size_t bunch_size = TABLES_PARALLEL_LOAD_BUNCH_SIZE;
+    size_t num_bunches = (total_tables + bunch_size - 1) / bunch_size;
+
+    for (size_t i = 0; i < num_bunches; ++i)
     {
-        auto task = std::bind(task_function, filename);
+        auto begin = file_names.begin() + i * bunch_size;
+        auto end = (i + 1 == num_bunches)
+            ? file_names.end()
+            : (file_names.begin() + (i + 1) * bunch_size);
+
+        auto task = std::bind(task_function, begin, end);
 
         if (thread_pool)
             thread_pool->schedule(task);
@@ -192,7 +198,7 @@ void DatabaseOrdinary::loadTables(
     }
 
     if (thread_pool)
-        all_tables_processed.wait();
+        thread_pool->wait();
 
     /// After all tables was basically initialized, startup them.
     startupTables(thread_pool);
@@ -206,38 +212,47 @@ void DatabaseOrdinary::startupTables(ThreadPool * thread_pool)
     AtomicStopwatch watch;
     std::atomic<size_t> tables_processed {0};
     size_t total_tables = tables.size();
-    Poco::Event all_tables_processed;
-
-    if (!total_tables)
-        return;
 
-    auto task_function = [&](const StoragePtr & table)
+    auto task_function = [&](Tables::iterator begin, Tables::iterator end)
     {
-        if ((tables_processed + 1) % PRINT_MESSAGE_EACH_N_TABLES == 0
-            || watch.compareAndRestart(PRINT_MESSAGE_EACH_N_SECONDS))
+        for (auto it = begin; it != end; ++it)
         {
-            LOG_INFO(log, std::fixed << std::setprecision(2) << tables_processed * 100.0 / total_tables << "%");
-            watch.restart();
+            if ((++tables_processed) % PRINT_MESSAGE_EACH_N_TABLES == 0
+                || watch.compareAndRestart(PRINT_MESSAGE_EACH_N_SECONDS))
+            {
+                LOG_INFO(log, std::fixed << std::setprecision(2) << tables_processed * 100.0 / total_tables << "%");
+                watch.restart();
+            }
+
+            it->second->startup();
         }
-
-        table->startup();
-
-        if (++tables_processed == total_tables)
-            all_tables_processed.set();
     };
 
-    for (const auto & name_storage : tables)
+    const size_t bunch_size = TABLES_PARALLEL_LOAD_BUNCH_SIZE;
+    size_t num_bunches = (total_tables + bunch_size - 1) / bunch_size;
+
+    auto begin = tables.begin();
+    for (size_t i = 0; i < num_bunches; ++i)
     {
-        auto task = std::bind(task_function, name_storage.second);
+        auto end = begin;
+
+        if (i + 1 == num_bunches)
+            end = tables.end();
+        else
+            std::advance(end, bunch_size);
+
+        auto task = std::bind(task_function, begin, end);
 
         if (thread_pool)
             thread_pool->schedule(task);
         else
             task();
+
+        begin = end;
     }
 
     if (thread_pool)
-        all_tables_processed.wait();
+        thread_pool->wait();
 }
 
 
diff --git a/dbms/src/Dictionaries/DictionarySourceFactory.cpp b/dbms/src/Dictionaries/DictionarySourceFactory.cpp
index bb778fbce..87ad425ba 100644
--- a/dbms/src/Dictionaries/DictionarySourceFactory.cpp
+++ b/dbms/src/Dictionaries/DictionarySourceFactory.cpp
@@ -155,8 +155,7 @@ DictionarySourcePtr DictionarySourceFactory::create(
     else if ("odbc" == source_type)
     {
 #if USE_POCO_SQLODBC || USE_POCO_DATAODBC
-        const auto & global_config = context.getConfigRef();
-        BridgeHelperPtr bridge = std::make_shared<XDBCBridgeHelper<ODBCBridgeMixin>>(global_config, context.getSettings().http_connection_timeout, config.getString(config_prefix + ".odbc.connection_string"));
+        BridgeHelperPtr bridge = std::make_shared<XDBCBridgeHelper<ODBCBridgeMixin>>(config, context.getSettings().http_connection_timeout, config.getString(config_prefix + ".odbc.connection_string"));
         return std::make_unique<XDBCDictionarySource>(dict_struct, config, config_prefix + ".odbc", sample_block, context, bridge);
 #else
         throw Exception{"Dictionary source of type `odbc` is disabled because poco library was built without ODBC support.",
diff --git a/dbms/src/Dictionaries/ExternalQueryBuilder.cpp b/dbms/src/Dictionaries/ExternalQueryBuilder.cpp
index bb420fdec..156c05932 100644
--- a/dbms/src/Dictionaries/ExternalQueryBuilder.cpp
+++ b/dbms/src/Dictionaries/ExternalQueryBuilder.cpp
@@ -14,28 +14,17 @@ namespace DB
 namespace ErrorCodes
 {
     extern const int UNSUPPORTED_METHOD;
-    extern const int LOGICAL_ERROR;
 }
 
 
 ExternalQueryBuilder::ExternalQueryBuilder(
-    const DictionaryStructure & dict_struct_,
-    const std::string & db_,
-    const std::string & table_,
-    const std::string & where_,
-    IdentifierQuotingStyle quoting_style_)
-    : dict_struct(dict_struct_), db(db_), where(where_), quoting_style(quoting_style_)
+    const DictionaryStructure & dict_struct,
+    const std::string & db,
+    const std::string & table,
+    const std::string & where,
+    IdentifierQuotingStyle quoting_style)
+    : dict_struct(dict_struct), db(db), table(table), where(where), quoting_style(quoting_style)
 {
-    if (auto pos = table_.find('.'); pos != std::string::npos)
-    {
-        schema = table_.substr(0, pos);
-        table = table_.substr(pos + 1);
-    }
-    else
-    {
-        schema = "";
-        table = table_;
-    }
 }
 
 
@@ -135,11 +124,6 @@ std::string ExternalQueryBuilder::composeLoadAllQuery() const
         writeQuoted(db, out);
         writeChar('.', out);
     }
-    if (!schema.empty())
-    {
-        writeQuoted(schema, out);
-        writeChar('.', out);
-    }
     writeQuoted(table, out);
 
     if (!where.empty())
@@ -203,12 +187,6 @@ std::string ExternalQueryBuilder::composeLoadIdsQuery(const std::vector<UInt64>
         writeQuoted(db, out);
         writeChar('.', out);
     }
-    if (!schema.empty())
-    {
-        writeQuoted(schema, out);
-        writeChar('.', out);
-    }
-
     writeQuoted(table, out);
 
     writeString(" WHERE ", out);
@@ -272,12 +250,6 @@ std::string ExternalQueryBuilder::composeLoadKeysQuery(
         writeQuoted(db, out);
         writeChar('.', out);
     }
-    if (!schema.empty())
-    {
-        writeQuoted(schema, out);
-        writeChar('.', out);
-    }
-
     writeQuoted(table, out);
 
     writeString(" WHERE ", out);
diff --git a/dbms/src/Dictionaries/ExternalQueryBuilder.h b/dbms/src/Dictionaries/ExternalQueryBuilder.h
index b2f790e07..aedf7b86a 100644
--- a/dbms/src/Dictionaries/ExternalQueryBuilder.h
+++ b/dbms/src/Dictionaries/ExternalQueryBuilder.h
@@ -18,20 +18,19 @@ class WriteBuffer;
 struct ExternalQueryBuilder
 {
     const DictionaryStructure & dict_struct;
-    std::string db;
-    std::string table;
-    std::string schema;
+    const std::string & db;
+    const std::string & table;
     const std::string & where;
 
     IdentifierQuotingStyle quoting_style;
 
 
     ExternalQueryBuilder(
-        const DictionaryStructure & dict_struct_,
-        const std::string & db_,
-        const std::string & table_,
-        const std::string & where_,
-        IdentifierQuotingStyle quoting_style_);
+        const DictionaryStructure & dict_struct,
+        const std::string & db,
+        const std::string & table,
+        const std::string & where,
+        IdentifierQuotingStyle quoting_style);
 
     /** Generate a query to load all data. */
     std::string composeLoadAllQuery() const;
diff --git a/dbms/src/Dictionaries/ExternalResultDescription.cpp b/dbms/src/Dictionaries/ExternalResultDescription.cpp
index a997cb4d0..4ac883d1c 100644
--- a/dbms/src/Dictionaries/ExternalResultDescription.cpp
+++ b/dbms/src/Dictionaries/ExternalResultDescription.cpp
@@ -5,7 +5,6 @@
 #include <DataTypes/DataTypeDate.h>
 #include <DataTypes/DataTypeDateTime.h>
 #include <DataTypes/DataTypeUUID.h>
-#include <DataTypes/DataTypeNullable.h>
 #include <Common/typeid_cast.h>
 
 
@@ -21,48 +20,57 @@ void ExternalResultDescription::init(const Block & sample_block_)
 {
     sample_block = sample_block_;
 
-    types.reserve(sample_block.columns());
+    const auto num_columns = sample_block.columns();
+    types.reserve(num_columns);
+    names.reserve(num_columns);
+    sample_columns.reserve(num_columns);
 
-    for (auto & elem : sample_block)
+    for (const auto idx : ext::range(0, num_columns))
     {
-        /// If default value for column was not provided, use default from data type.
-        if (elem.column->empty())
-            elem.column = elem.type->createColumnConstWithDefaultValue(1)->convertToFullColumnIfConst();
-
-        bool is_nullable = elem.type->isNullable();
-        DataTypePtr type_not_nullable = removeNullable(elem.type);
-        const IDataType * type = type_not_nullable.get();
+        const auto & column = sample_block.safeGetByPosition(idx);
+        const auto type = column.type.get();
 
         if (typeid_cast<const DataTypeUInt8 *>(type))
-            types.emplace_back(ValueType::UInt8, is_nullable);
+            types.push_back(ValueType::UInt8);
         else if (typeid_cast<const DataTypeUInt16 *>(type))
-            types.emplace_back(ValueType::UInt16, is_nullable);
+            types.push_back(ValueType::UInt16);
         else if (typeid_cast<const DataTypeUInt32 *>(type))
-            types.emplace_back(ValueType::UInt32, is_nullable);
+            types.push_back(ValueType::UInt32);
         else if (typeid_cast<const DataTypeUInt64 *>(type))
-            types.emplace_back(ValueType::UInt64, is_nullable);
+            types.push_back(ValueType::UInt64);
         else if (typeid_cast<const DataTypeInt8 *>(type))
-            types.emplace_back(ValueType::Int8, is_nullable);
+            types.push_back(ValueType::Int8);
         else if (typeid_cast<const DataTypeInt16 *>(type))
-            types.emplace_back(ValueType::Int16, is_nullable);
+            types.push_back(ValueType::Int16);
         else if (typeid_cast<const DataTypeInt32 *>(type))
-            types.emplace_back(ValueType::Int32, is_nullable);
+            types.push_back(ValueType::Int32);
         else if (typeid_cast<const DataTypeInt64 *>(type))
-            types.emplace_back(ValueType::Int64, is_nullable);
+            types.push_back(ValueType::Int64);
         else if (typeid_cast<const DataTypeFloat32 *>(type))
-            types.emplace_back(ValueType::Float32, is_nullable);
+            types.push_back(ValueType::Float32);
         else if (typeid_cast<const DataTypeFloat64 *>(type))
-            types.emplace_back(ValueType::Float64, is_nullable);
+            types.push_back(ValueType::Float64);
         else if (typeid_cast<const DataTypeString *>(type))
-            types.emplace_back(ValueType::String, is_nullable);
+            types.push_back(ValueType::String);
         else if (typeid_cast<const DataTypeDate *>(type))
-            types.emplace_back(ValueType::Date, is_nullable);
+            types.push_back(ValueType::Date);
         else if (typeid_cast<const DataTypeDateTime *>(type))
-            types.emplace_back(ValueType::DateTime, is_nullable);
+            types.push_back(ValueType::DateTime);
         else if (typeid_cast<const DataTypeUUID *>(type))
-            types.emplace_back(ValueType::UUID, is_nullable);
+            types.push_back(ValueType::UUID);
         else
             throw Exception{"Unsupported type " + type->getName(), ErrorCodes::UNKNOWN_TYPE};
+
+        names.emplace_back(column.name);
+        sample_columns.emplace_back(column.column);
+
+        /// If default value for column was not provided, use default from data type.
+        if (sample_columns.back()->empty())
+        {
+            MutableColumnPtr mutable_column = (*std::move(sample_columns.back())).mutate();
+            column.type->insertDefaultInto(*mutable_column);
+            sample_columns.back() = std::move(mutable_column);
+        }
     }
 }
 
diff --git a/dbms/src/Dictionaries/ExternalResultDescription.h b/dbms/src/Dictionaries/ExternalResultDescription.h
index 452770fe9..ff9426ade 100644
--- a/dbms/src/Dictionaries/ExternalResultDescription.h
+++ b/dbms/src/Dictionaries/ExternalResultDescription.h
@@ -25,11 +25,13 @@ struct ExternalResultDescription
         String,
         Date,
         DateTime,
-        UUID,
+        UUID
     };
 
     Block sample_block;
-    std::vector<std::pair<ValueType, bool /* is_nullable */>> types;
+    std::vector<ValueType> types;
+    std::vector<std::string> names;
+    Columns sample_columns;
 
     void init(const Block & sample_block_);
 };
diff --git a/dbms/src/Dictionaries/LibraryDictionarySource.cpp b/dbms/src/Dictionaries/LibraryDictionarySource.cpp
index a0505ee79..eaedfa717 100644
--- a/dbms/src/Dictionaries/LibraryDictionarySource.cpp
+++ b/dbms/src/Dictionaries/LibraryDictionarySource.cpp
@@ -94,16 +94,9 @@ namespace
             {
                 const auto & field = columns_received->data[col_n].data[row_n];
                 if (!field.data)
-                {
-                    /// sample_block contains null_value (from config) inside corresponding column
-                    const auto & col = sample_block.getByPosition(row_n);
-                    columns[row_n]->insertFrom(*(col.column), 0);
-                }
-                else
-                {
-                    const auto & size = field.size;
-                    columns[row_n]->insertData(static_cast<const char *>(field.data), size);
-                }
+                    continue;
+                const auto & size = field.size;
+                columns[row_n]->insertData(static_cast<const char *>(field.data), size);
             }
         }
 
diff --git a/dbms/src/Dictionaries/MongoDBBlockInputStream.cpp b/dbms/src/Dictionaries/MongoDBBlockInputStream.cpp
index 1f0f91a25..8bd4a28bb 100644
--- a/dbms/src/Dictionaries/MongoDBBlockInputStream.cpp
+++ b/dbms/src/Dictionaries/MongoDBBlockInputStream.cpp
@@ -14,7 +14,6 @@
 #include <Dictionaries/MongoDBBlockInputStream.h>
 #include <Columns/ColumnString.h>
 #include <Columns/ColumnsNumber.h>
-#include <Columns/ColumnNullable.h>
 #include <Common/FieldVisitors.h>
 #include <IO/WriteHelpers.h>
 #include <IO/ReadHelpers.h>
@@ -180,22 +179,13 @@ Block MongoDBBlockInputStream::readImpl()
 
             for (const auto idx : ext::range(0, size))
             {
-                const auto & name = description.sample_block.getByPosition(idx).name;
+                const auto & name = description.names[idx];
                 const Poco::MongoDB::Element::Ptr value = document->get(name);
 
                 if (value.isNull() || value->type() == Poco::MongoDB::ElementTraits<Poco::MongoDB::NullValue>::TypeId)
-                    insertDefaultValue(*columns[idx], *description.sample_block.getByPosition(idx).column);
+                    insertDefaultValue(*columns[idx], *description.sample_columns[idx]);
                 else
-                {
-                    if (description.types[idx].second)
-                    {
-                        ColumnNullable & column_nullable = static_cast<ColumnNullable &>(*columns[idx]);
-                        insertValue(column_nullable.getNestedColumn(), description.types[idx].first, *value, name);
-                        column_nullable.getNullMapData().emplace_back(0);
-                    }
-                    else
-                        insertValue(*columns[idx], description.types[idx].first, *value, name);
-                }
+                    insertValue(*columns[idx], description.types[idx], *value, name);
             }
         }
 
diff --git a/dbms/src/Dictionaries/MongoDBBlockInputStream.h b/dbms/src/Dictionaries/MongoDBBlockInputStream.h
index 3c964708c..dba3cc9e9 100644
--- a/dbms/src/Dictionaries/MongoDBBlockInputStream.h
+++ b/dbms/src/Dictionaries/MongoDBBlockInputStream.h
@@ -32,7 +32,7 @@ public:
 
     String getName() const override { return "MongoDB"; }
 
-    Block getHeader() const override { return description.sample_block.cloneEmpty(); }
+    Block getHeader() const override { return description.sample_block; }
 
 private:
     Block readImpl() override;
diff --git a/dbms/src/Dictionaries/MySQLBlockInputStream.cpp b/dbms/src/Dictionaries/MySQLBlockInputStream.cpp
index ecd6da11f..250329eae 100644
--- a/dbms/src/Dictionaries/MySQLBlockInputStream.cpp
+++ b/dbms/src/Dictionaries/MySQLBlockInputStream.cpp
@@ -4,7 +4,6 @@
 #include <Dictionaries/MySQLBlockInputStream.h>
 #include <Columns/ColumnsNumber.h>
 #include <Columns/ColumnString.h>
-#include <Columns/ColumnNullable.h>
 #include <IO/ReadHelpers.h>
 #include <IO/WriteHelpers.h>
 #include <ext/range.h>
@@ -83,18 +82,9 @@ Block MySQLBlockInputStream::readImpl()
         {
             const auto value = row[idx];
             if (!value.isNull())
-            {
-                if (description.types[idx].second)
-                {
-                    ColumnNullable & column_nullable = static_cast<ColumnNullable &>(*columns[idx]);
-                    insertValue(column_nullable.getNestedColumn(), description.types[idx].first, value);
-                    column_nullable.getNullMapData().emplace_back(0);
-                }
-                else
-                    insertValue(*columns[idx], description.types[idx].first, value);
-            }
+                insertValue(*columns[idx], description.types[idx], value);
             else
-                insertDefaultValue(*columns[idx], *description.sample_block.getByPosition(idx).column);
+                insertDefaultValue(*columns[idx], *description.sample_columns[idx]);
         }
 
         ++num_rows;
diff --git a/dbms/src/Dictionaries/MySQLBlockInputStream.h b/dbms/src/Dictionaries/MySQLBlockInputStream.h
index 7e082fdc2..9e760cd28 100644
--- a/dbms/src/Dictionaries/MySQLBlockInputStream.h
+++ b/dbms/src/Dictionaries/MySQLBlockInputStream.h
@@ -21,7 +21,7 @@ public:
 
     String getName() const override { return "MySQL"; }
 
-    Block getHeader() const override { return description.sample_block.cloneEmpty(); }
+    Block getHeader() const override { return description.sample_block; }
 
 private:
     Block readImpl() override;
diff --git a/dbms/src/Dictionaries/ODBCBlockInputStream.cpp b/dbms/src/Dictionaries/ODBCBlockInputStream.cpp
index 2bf1d0756..d22fd1b0e 100644
--- a/dbms/src/Dictionaries/ODBCBlockInputStream.cpp
+++ b/dbms/src/Dictionaries/ODBCBlockInputStream.cpp
@@ -2,7 +2,6 @@
 
 #include <Columns/ColumnsNumber.h>
 #include <Columns/ColumnString.h>
-#include <Columns/ColumnNullable.h>
 
 #include <IO/ReadHelpers.h>
 #include <IO/WriteHelpers.h>
@@ -92,18 +91,9 @@ Block ODBCBlockInputStream::readImpl()
             const Poco::Dynamic::Var & value = row[idx];
 
             if (!value.isEmpty())
-            {
-                if (description.types[idx].second)
-                {
-                    ColumnNullable & column_nullable = static_cast<ColumnNullable &>(*columns[idx]);
-                    insertValue(column_nullable.getNestedColumn(), description.types[idx].first, value);
-                    column_nullable.getNullMapData().emplace_back(0);
-                }
-                else
-                    insertValue(*columns[idx], description.types[idx].first, value);
-            }
+                insertValue(*columns[idx], description.types[idx], value);
             else
-                insertDefaultValue(*columns[idx], *description.sample_block.getByPosition(idx).column);
+                insertDefaultValue(*columns[idx], *description.sample_columns[idx]);
         }
 
         ++iterator;
diff --git a/dbms/src/Dictionaries/ODBCBlockInputStream.h b/dbms/src/Dictionaries/ODBCBlockInputStream.h
index b9f5543c2..8634b59e0 100644
--- a/dbms/src/Dictionaries/ODBCBlockInputStream.h
+++ b/dbms/src/Dictionaries/ODBCBlockInputStream.h
@@ -24,7 +24,7 @@ public:
 
     String getName() const override { return "ODBC"; }
 
-    Block getHeader() const override { return description.sample_block.cloneEmpty(); }
+    Block getHeader() const override { return description.sample_block; }
 
 private:
     Block readImpl() override;
diff --git a/dbms/src/Functions/FunctionsCoding.h b/dbms/src/Functions/FunctionsCoding.h
index 27424f735..d3ae5d86f 100644
--- a/dbms/src/Functions/FunctionsCoding.h
+++ b/dbms/src/Functions/FunctionsCoding.h
@@ -49,7 +49,7 @@ namespace ErrorCodes
   *          but only by whole bytes. For dates and datetimes - the same as for numbers.
   *          For example, hex(257) = '0101'.
   * unhex(string) - Returns a string, hex of which is equal to `string` with regard of case and discarding one leading zero.
-  *                 If such a string does not exist, could return arbitrary implementation specific value.
+  *                 If such a string does not exist, could return arbitary implementation specific value.
   *
   * bitmaskToArray(x) - Returns an array of powers of two in the binary form of x. For example, bitmaskToArray(50) = [2, 16, 32].
   */
diff --git a/dbms/src/Functions/FunctionsComparison.h b/dbms/src/Functions/FunctionsComparison.h
index 3f7f6f390..36132cbba 100644
--- a/dbms/src/Functions/FunctionsComparison.h
+++ b/dbms/src/Functions/FunctionsComparison.h
@@ -725,7 +725,7 @@ private:
             return true;
         };
 
-        if (!callOnBasicTypes<true, false, true, false>(left_number, right_number, call))
+        if (!callOnBasicTypes<true, false, true>(left_number, right_number, call))
             throw Exception("Wrong call for " + getName() + " with " + col_left.type->getName() + " and " + col_right.type->getName(),
                             ErrorCodes::LOGICAL_ERROR);
     }
diff --git a/dbms/src/Functions/FunctionsConditional.h b/dbms/src/Functions/FunctionsConditional.h
index a469a6bb7..573ae32b2 100644
--- a/dbms/src/Functions/FunctionsConditional.h
+++ b/dbms/src/Functions/FunctionsConditional.h
@@ -230,11 +230,6 @@ public:
     static FunctionPtr create(const Context &) { return std::make_shared<FunctionIf>(); }
 
 private:
-    template <typename T0, typename T1>
-    static constexpr bool allow_arrays =
-        !IsDecimalNumber<T0> && !IsDecimalNumber<T1> &&
-        !std::is_same_v<T0, UInt128> && !std::is_same_v<T1, UInt128>;
-
     template <typename T0, typename T1>
     static UInt32 decimalScale(Block & block [[maybe_unused]], const ColumnNumbers & arguments [[maybe_unused]])
     {
@@ -319,7 +314,7 @@ private:
     {
         if constexpr (std::is_same_v<NumberTraits::Error, typename NumberTraits::ResultOfIf<T0, T1>::Type>)
             return false;
-        else if constexpr (allow_arrays<T0, T1>)
+        else if constexpr (!IsDecimalNumber<T0> && !IsDecimalNumber<T1>)
         {
             using ResultType = typename NumberTraits::ResultOfIf<T0, T1>::Type;
 
@@ -375,7 +370,7 @@ private:
     {
         if constexpr (std::is_same_v<NumberTraits::Error, typename NumberTraits::ResultOfIf<T0, T1>::Type>)
             return false;
-        else if constexpr (allow_arrays<T0, T1>)
+        else if constexpr (!IsDecimalNumber<T0> && !IsDecimalNumber<T1>)
         {
             using ResultType = typename NumberTraits::ResultOfIf<T0, T1>::Type;
 
@@ -980,10 +975,9 @@ public:
         if (auto rigth_array = checkAndGetDataType<DataTypeArray>(arg_else.type.get()))
             right_id = rigth_array->getNestedType()->getTypeId();
 
-        bool executed_with_nums = callOnBasicTypes<true, true, true, true>(left_id, right_id, call);
+        bool executed_with_nums = callOnBasicTypes<true, true, true>(left_id, right_id, call);
 
         if (!( executed_with_nums
-            || executeTyped<UInt128, UInt128>(cond_col, block, arguments, result, input_rows_count)
             || executeString(cond_col, block, arguments, result)
             || executeGenericArray(cond_col, block, arguments, result)
             || executeTuple(block, arguments, result, input_rows_count)))
diff --git a/dbms/src/Functions/FunctionsConversion.h b/dbms/src/Functions/FunctionsConversion.h
index 9335e5eee..58e528a6e 100644
--- a/dbms/src/Functions/FunctionsConversion.h
+++ b/dbms/src/Functions/FunctionsConversion.h
@@ -340,7 +340,7 @@ struct ConvertImplGenericToString
         ColumnString::Chars_t & data_to = col_to->getChars();
         ColumnString::Offsets & offsets_to = col_to->getOffsets();
 
-        data_to.resize(size * 2); /// Using coefficient 2 for initial size is arbitrary.
+        data_to.resize(size * 2); /// Using coefficient 2 for initial size is arbitary.
         offsets_to.resize(size);
 
         WriteBufferFromVector<ColumnString::Chars_t> write_buffer(data_to);
diff --git a/dbms/src/Functions/FunctionsHashing.h b/dbms/src/Functions/FunctionsHashing.h
index 9ea16bc09..0bff34517 100644
--- a/dbms/src/Functions/FunctionsHashing.h
+++ b/dbms/src/Functions/FunctionsHashing.h
@@ -565,7 +565,7 @@ public:
             vec_to.assign(rows, static_cast<UInt64>(0xe28dbde7fe22e41c));
         }
 
-        /// The function supports arbitrary number of arguments of arbitrary types.
+        /// The function supports arbitary number of arguments of arbitary types.
 
         bool is_first_argument = true;
         for (size_t i = 0; i < arguments.size(); ++i)
diff --git a/dbms/src/Functions/FunctionsMath.h b/dbms/src/Functions/FunctionsMath.h
index 5ab1bf899..21404789a 100644
--- a/dbms/src/Functions/FunctionsMath.h
+++ b/dbms/src/Functions/FunctionsMath.h
@@ -162,7 +162,7 @@ private:
             return execute<Type>(block, col_vec, result);
         };
 
-        if (!callOnBasicType<void, true, true, true, false>(col.type->getTypeId(), call))
+        if (!callOnBasicType<void, true, true, true>(col.type->getTypeId(), call))
             throw Exception{"Illegal column " + col.column->getName() + " of argument of function " + getName(),
                 ErrorCodes::ILLEGAL_COLUMN};
     }
@@ -385,7 +385,7 @@ private:
         TypeIndex left_index = col_left.type->getTypeId();
         TypeIndex right_index = col_right.type->getTypeId();
 
-        if (!callOnBasicTypes<true, true, false, false>(left_index, right_index, call))
+        if (!callOnBasicTypes<true, true, false>(left_index, right_index, call))
             throw Exception{"Illegal column " + col_left.column->getName() + " of argument of function " + getName(),
                 ErrorCodes::ILLEGAL_COLUMN};
     }
diff --git a/dbms/src/Functions/arrayIndex.h b/dbms/src/Functions/arrayIndex.h
index 82e5f32fb..68589f56f 100644
--- a/dbms/src/Functions/arrayIndex.h
+++ b/dbms/src/Functions/arrayIndex.h
@@ -397,7 +397,7 @@ struct ArrayIndexStringImpl
     }
 };
 
-/// Catch-all implementation for arrays of arbitrary type.
+/// Catch-all implementation for arrays of arbitary type.
 /// To compare with constant value, create non-constant column with single element,
 /// and pass is_value_has_single_element_to_compare = true.
 template <typename IndexConv, bool is_value_has_single_element_to_compare>
@@ -555,7 +555,7 @@ public:
     }
 };
 
-/// Catch-all implementation for arrays of arbitrary type
+/// Catch-all implementation for arrays of arbitary type
 /// when the 2nd function argument is a NULL value.
 template <typename IndexConv>
 struct ArrayIndexGenericNullImpl
diff --git a/dbms/src/Functions/sleep.h b/dbms/src/Functions/sleep.h
index 1a6c90244..1dfdbf3b5 100644
--- a/dbms/src/Functions/sleep.h
+++ b/dbms/src/Functions/sleep.h
@@ -85,7 +85,7 @@ public:
             unsigned useconds = seconds * (variant == FunctionSleepVariant::PerBlock ? 1 : size) * 1e6;
 
             /// When sleeping, the query cannot be cancelled. For abitily to cancel query, we limit sleep time.
-            if (useconds > 3000000)   /// The choice is arbitrary
+            if (useconds > 3000000)   /// The choice is arbitary
                 throw Exception("The maximum sleep time is 3000000 microseconds. Requested: " + toString(useconds), ErrorCodes::TOO_SLOW);
 
             ::usleep(useconds);
diff --git a/dbms/src/Functions/timeSlot.cpp b/dbms/src/Functions/timeSlot.cpp
index c734bed82..f9763997b 100644
--- a/dbms/src/Functions/timeSlot.cpp
+++ b/dbms/src/Functions/timeSlot.cpp
@@ -8,7 +8,7 @@
 namespace DB
 {
 
-using FunctionTimeSlot = FunctionDateOrDateTimeToSomething<DataTypeDateTime, TimeSlotImpl>;
+using FunctionTimeSlot = FunctionDateOrDateTimeToSomething<DataTypeUInt32, TimeSlotImpl>;
 
 void registerFunctionTimeSlot(FunctionFactory & factory)
 {
diff --git a/dbms/src/Functions/timeSlots.cpp b/dbms/src/Functions/timeSlots.cpp
index 07afc2894..16ade60cb 100644
--- a/dbms/src/Functions/timeSlots.cpp
+++ b/dbms/src/Functions/timeSlots.cpp
@@ -121,19 +121,17 @@ public:
 
     size_t getNumberOfArguments() const override { return 2; }
 
-    DataTypePtr getReturnTypeImpl(const ColumnsWithTypeAndName & arguments) const override
+    DataTypePtr getReturnTypeImpl(const DataTypes & arguments) const override
     {
-        if (!WhichDataType(arguments[0].type).isDateTime())
-            throw Exception("Illegal type " + arguments[0].type->getName() + " of first argument of function " + getName() + ". Must be DateTime.",
+        if (!WhichDataType(arguments[0]).isDateTime())
+            throw Exception("Illegal type " + arguments[0]->getName() + " of first argument of function " + getName() + ". Must be DateTime.",
                 ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT);
 
-        if (!WhichDataType(arguments[1].type).isUInt32())
-            throw Exception("Illegal type " + arguments[1].type->getName() + " of second argument of function " + getName() + ". Must be UInt32.",
+        if (!WhichDataType(arguments[1]).isUInt32())
+            throw Exception("Illegal type " + arguments[1]->getName() + " of second argument of function " + getName() + ". Must be UInt32.",
                 ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT);
 
-        /// If time zone is specified for source data type, attach it to the resulting type.
-        /// Note that there is no explicit time zone argument for this function (we specify 2 as an argument number with explicit time zone).
-        return std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>(extractTimeZoneNameFromFunctionArguments(arguments, 2, 0)));
+        return std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>());
     }
 
     void executeImpl(Block & block, const ColumnNumbers & arguments, size_t result, size_t input_rows_count) override
diff --git a/dbms/src/IO/CompressedStream.h b/dbms/src/IO/CompressedStream.h
index 5a00db020..be8fb254f 100644
--- a/dbms/src/IO/CompressedStream.h
+++ b/dbms/src/IO/CompressedStream.h
@@ -47,6 +47,7 @@ enum class CompressionMethodByte : uint8_t
     NONE     = 0x02,
     LZ4      = 0x82,
     ZSTD     = 0x90,
+    Multiple = 0x91,
 };
 
 }
diff --git a/dbms/src/Interpreters/Aggregator.cpp b/dbms/src/Interpreters/Aggregator.cpp
index 4d99f7315..82654c30b 100644
--- a/dbms/src/Interpreters/Aggregator.cpp
+++ b/dbms/src/Interpreters/Aggregator.cpp
@@ -1132,9 +1132,6 @@ void Aggregator::convertToBlockImpl(
     if (data.empty())
         return;
 
-    if (key_columns.size() != params.keys_size)
-        throw Exception{"Aggregate. Unexpected key columns size.", ErrorCodes::LOGICAL_ERROR};
-
     if (final)
         convertToBlockImplFinal(method, data, key_columns, final_aggregate_columns);
     else
@@ -1154,7 +1151,7 @@ void NO_INLINE Aggregator::convertToBlockImplFinal(
 {
     for (const auto & value : data)
     {
-        method.insertKeyIntoColumns(value, key_columns, key_sizes);
+        method.insertKeyIntoColumns(value, key_columns, params.keys_size, key_sizes);
 
         for (size_t i = 0; i < params.aggregates_size; ++i)
             aggregate_functions[i]->insertResultInto(
@@ -1172,9 +1169,10 @@ void NO_INLINE Aggregator::convertToBlockImplNotFinal(
     MutableColumns & key_columns,
     AggregateColumnsData & aggregate_columns) const
 {
+
     for (auto & value : data)
     {
-        method.insertKeyIntoColumns(value, key_columns, key_sizes);
+        method.insertKeyIntoColumns(value, key_columns, params.keys_size, key_sizes);
 
         /// reserved, so push_back does not throw exceptions
         for (size_t i = 0; i < params.aggregates_size; ++i)
@@ -2087,7 +2085,7 @@ void Aggregator::mergeStream(const BlockInputStreamPtr & stream, AggregatedDataV
 
     /** `minus one` means the absence of information about the bucket
       * - in the case of single-level aggregation, as well as for blocks with "overflowing" values.
-      * If there is at least one block with a bucket number greater or equal than zero, then there was a two-level aggregation.
+      * If there is at least one block with a bucket number greater than zero, then there was a two-level aggregation.
       */
     auto max_bucket = bucket_to_blocks.rbegin()->first;
     size_t has_two_level = max_bucket >= 0;
diff --git a/dbms/src/Interpreters/Aggregator.h b/dbms/src/Interpreters/Aggregator.h
index 1d1a86e8a..f32c0e994 100644
--- a/dbms/src/Interpreters/Aggregator.h
+++ b/dbms/src/Interpreters/Aggregator.h
@@ -166,7 +166,7 @@ struct AggregationMethodOneNumber
 
     /** Insert the key from the hash table into columns.
       */
-    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, const Sizes & /*key_sizes*/)
+    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, size_t /*keys_size*/, const Sizes & /*key_sizes*/)
     {
         static_cast<ColumnVector<FieldType> *>(key_columns[0].get())->insertData(reinterpret_cast<const char *>(&value.first), sizeof(value.first));
     }
@@ -243,7 +243,7 @@ struct AggregationMethodString
         return StringRef(value.first.data, value.first.size);
     }
 
-    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, const Sizes &)
+    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, size_t, const Sizes &)
     {
         key_columns[0]->insertData(value.first.data, value.first.size);
     }
@@ -312,7 +312,7 @@ struct AggregationMethodFixedString
         return StringRef(value.first.data, value.first.size);
     }
 
-    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, const Sizes &)
+    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, size_t, const Sizes &)
     {
         key_columns[0]->insertData(value.first.data, value.first.size);
     }
@@ -580,7 +580,7 @@ struct AggregationMethodSingleLowCardinalityColumn : public SingleColumnMethod
     static const bool no_consecutive_keys_optimization = true;
     static const bool low_cardinality_optimization = true;
 
-    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, const Sizes & /*key_sizes*/)
+    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, size_t /*keys_size*/, const Sizes & /*key_sizes*/)
     {
         auto ref = Base::getValueRef(value);
         static_cast<ColumnLowCardinality *>(key_columns[0].get())->insertData(ref.data, ref.size);
@@ -783,10 +783,8 @@ struct AggregationMethodKeysFixed
     static const bool no_consecutive_keys_optimization = false;
     static const bool low_cardinality_optimization = false;
 
-    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, const Sizes & key_sizes)
+    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, size_t keys_size, const Sizes & key_sizes)
     {
-        size_t keys_size = key_columns.size();
-
         static constexpr auto bitmap_size = has_nullable_keys ? std::tuple_size<KeysNullMap<Key>>::value : 0;
         /// In any hash key value, column values to be read start just after the bitmap, if it exists.
         size_t pos = bitmap_size;
@@ -893,10 +891,10 @@ struct AggregationMethodSerialized
     static const bool no_consecutive_keys_optimization = true;
     static const bool low_cardinality_optimization = false;
 
-    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, const Sizes &)
+    static void insertKeyIntoColumns(const typename Data::value_type & value, MutableColumns & key_columns, size_t keys_size, const Sizes &)
     {
         auto pos = value.first.data;
-        for (size_t i = 0; i < key_columns.size(); ++i)
+        for (size_t i = 0; i < keys_size; ++i)
             pos = key_columns[i]->deserializeAndInsertFromArena(pos);
     }
 
@@ -1286,10 +1284,10 @@ public:
         Block intermediate_header;
 
         /// What to count.
-        const ColumnNumbers keys;
-        const AggregateDescriptions aggregates;
-        const size_t keys_size;
-        const size_t aggregates_size;
+        ColumnNumbers keys;
+        AggregateDescriptions aggregates;
+        size_t keys_size;
+        size_t aggregates_size;
 
         /// The settings of approximate calculation of GROUP BY.
         const bool overflow_row;    /// Do we need to put into AggregatedDataVariants::without_key aggregates for keys that are not in max_rows_to_group_by.
@@ -1346,6 +1344,9 @@ public:
         {
             intermediate_header = intermediate_header_;
         }
+
+        /// Calculate the column numbers in `keys` and `aggregates`.
+        void calculateColumnNumbers(const Block & block);
     };
 
     Aggregator(const Params & params_);
diff --git a/dbms/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp b/dbms/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp
deleted file mode 100644
index 4716ac095..000000000
--- a/dbms/src/Interpreters/ExecuteScalarSubqueriesVisitor.cpp
+++ /dev/null
@@ -1,130 +0,0 @@
-#include <Parsers/ASTFunction.h>
-#include <Parsers/ASTLiteral.h>
-#include <Parsers/ASTSubquery.h>
-#include <Parsers/ASTTablesInSelectQuery.h>
-#include <Parsers/ASTExpressionList.h>
-
-#include <Interpreters/Context.h>
-#include <Interpreters/QueryNormalizer.h>
-#include <Interpreters/InterpreterSelectWithUnionQuery.h>
-#include <Interpreters/ExecuteScalarSubqueriesVisitor.h>
-
-namespace DB
-{
-
-namespace ErrorCodes
-{
-    extern const int INCORRECT_RESULT_OF_SCALAR_SUBQUERY;
-    extern const int TOO_MANY_ROWS;
-}
-
-
-static ASTPtr addTypeConversion(std::unique_ptr<ASTLiteral> && ast, const String & type_name)
-{
-    auto func = std::make_shared<ASTFunction>();
-    ASTPtr res = func;
-    func->alias = ast->alias;
-    func->prefer_alias_to_column_name = ast->prefer_alias_to_column_name;
-    ast->alias.clear();
-    func->name = "CAST";
-    auto exp_list = std::make_shared<ASTExpressionList>();
-    func->arguments = exp_list;
-    func->children.push_back(func->arguments);
-    exp_list->children.emplace_back(ast.release());
-    exp_list->children.emplace_back(std::make_shared<ASTLiteral>(type_name));
-    return res;
-}
-
-void ExecuteScalarSubqueriesVisitor::visit(ASTSubquery * subquery, ASTPtr & ast, const DumpASTNode &) const
-{
-    Context subquery_context = context;
-    Settings subquery_settings = context.getSettings();
-    subquery_settings.max_result_rows = 1;
-    subquery_settings.extremes = 0;
-    subquery_context.setSettings(subquery_settings);
-
-    ASTPtr subquery_select = subquery->children.at(0);
-    BlockIO res = InterpreterSelectWithUnionQuery(
-        subquery_select, subquery_context, {}, QueryProcessingStage::Complete, subquery_depth + 1).execute();
-
-    Block block;
-    try
-    {
-        block = res.in->read();
-
-        if (!block)
-        {
-            /// Interpret subquery with empty result as Null literal
-            auto ast_new = std::make_unique<ASTLiteral>(Null());
-            ast_new->setAlias(ast->tryGetAlias());
-            ast = std::move(ast_new);
-            return;
-        }
-
-        if (block.rows() != 1 || res.in->read())
-            throw Exception("Scalar subquery returned more than one row", ErrorCodes::INCORRECT_RESULT_OF_SCALAR_SUBQUERY);
-    }
-    catch (const Exception & e)
-    {
-        if (e.code() == ErrorCodes::TOO_MANY_ROWS)
-            throw Exception("Scalar subquery returned more than one row", ErrorCodes::INCORRECT_RESULT_OF_SCALAR_SUBQUERY);
-        else
-            throw;
-    }
-
-    size_t columns = block.columns();
-    if (columns == 1)
-    {
-        auto lit = std::make_unique<ASTLiteral>((*block.safeGetByPosition(0).column)[0]);
-        lit->alias = subquery->alias;
-        lit->prefer_alias_to_column_name = subquery->prefer_alias_to_column_name;
-        ast = addTypeConversion(std::move(lit), block.safeGetByPosition(0).type->getName());
-    }
-    else
-    {
-        auto tuple = std::make_shared<ASTFunction>();
-        tuple->alias = subquery->alias;
-        ast = tuple;
-        tuple->name = "tuple";
-        auto exp_list = std::make_shared<ASTExpressionList>();
-        tuple->arguments = exp_list;
-        tuple->children.push_back(tuple->arguments);
-
-        exp_list->children.resize(columns);
-        for (size_t i = 0; i < columns; ++i)
-        {
-            exp_list->children[i] = addTypeConversion(
-                std::make_unique<ASTLiteral>((*block.safeGetByPosition(i).column)[0]),
-                block.safeGetByPosition(i).type->getName());
-        }
-    }
-}
-
-
-void ExecuteScalarSubqueriesVisitor::visit(ASTTableExpression *, ASTPtr &, const DumpASTNode &) const
-{
-    /// Don't descend into subqueries in FROM section.
-}
-
-void ExecuteScalarSubqueriesVisitor::visit(ASTFunction * func, ASTPtr & ast, const DumpASTNode &) const
-{
-    /// Don't descend into subqueries in arguments of IN operator.
-    /// But if an argument is not subquery, than deeper may be scalar subqueries and we need to descend in them.
-
-    if (functionIsInOrGlobalInOperator(func->name))
-    {
-        for (auto & child : ast->children)
-        {
-            if (child != func->arguments)
-                visit(child);
-            else
-                for (size_t i = 0, size = func->arguments->children.size(); i < size; ++i)
-                    if (i != 1 || !typeid_cast<ASTSubquery *>(func->arguments->children[i].get()))
-                        visit(func->arguments->children[i]);
-        }
-    }
-    else
-        visitChildren(ast);
-}
-
-}
diff --git a/dbms/src/Interpreters/ExecuteScalarSubqueriesVisitor.h b/dbms/src/Interpreters/ExecuteScalarSubqueriesVisitor.h
deleted file mode 100644
index 9688ac253..000000000
--- a/dbms/src/Interpreters/ExecuteScalarSubqueriesVisitor.h
+++ /dev/null
@@ -1,79 +0,0 @@
-#pragma once
-
-#include <Common/typeid_cast.h>
-#include <Parsers/DumpASTNode.h>
-
-namespace DB
-{
-
-class Context;
-class ASTSubquery;
-class ASTFunction;
-struct ASTTableExpression;
-
-
-/** Replace subqueries that return exactly one row
-    * ("scalar" subqueries) to the corresponding constants.
-    *
-    * If the subquery returns more than one column, it is replaced by a tuple of constants.
-    *
-    * Features
-    *
-    * A replacement occurs during query analysis, and not during the main runtime.
-    * This means that the progress indicator will not work during the execution of these requests,
-    *  and also such queries can not be aborted.
-    *
-    * But the query result can be used for the index in the table.
-    *
-    * Scalar subqueries are executed on the request-initializer server.
-    * The request is sent to remote servers with already substituted constants.
-    */
-class ExecuteScalarSubqueriesVisitor
-{
-public:
-    ExecuteScalarSubqueriesVisitor(const Context & context_, size_t subquery_depth_, std::ostream * ostr_ = nullptr)
-    :   context(context_),
-        subquery_depth(subquery_depth_),
-        visit_depth(0),
-        ostr(ostr_)
-    {}
-
-    void visit(ASTPtr & ast) const
-    {
-        DumpASTNode dump(*ast, ostr, visit_depth, "executeScalarSubqueries");
-
-        if (!tryVisit<ASTSubquery>(ast, dump) &&
-            !tryVisit<ASTTableExpression>(ast, dump) &&
-            !tryVisit<ASTFunction>(ast, dump))
-            visitChildren(ast);
-    }
-
-private:
-    const Context & context;
-    size_t subquery_depth;
-    mutable size_t visit_depth;
-    std::ostream * ostr;
-
-    void visit(ASTSubquery * subquery, ASTPtr & ast, const DumpASTNode & dump) const;
-    void visit(ASTFunction * func, ASTPtr & ast, const DumpASTNode &) const;
-    void visit(ASTTableExpression *, ASTPtr &, const DumpASTNode &) const;
-
-    void visitChildren(ASTPtr & ast) const
-    {
-        for (auto & child : ast->children)
-            visit(child);
-    }
-
-    template <typename T>
-    bool tryVisit(ASTPtr & ast, const DumpASTNode & dump) const
-    {
-        if (T * t = typeid_cast<T *>(ast.get()))
-        {
-            visit(t, ast, dump);
-            return true;
-        }
-        return false;
-    }
-};
-
-}
diff --git a/dbms/src/Interpreters/ExpressionAnalyzer.cpp b/dbms/src/Interpreters/ExpressionAnalyzer.cpp
index 7aa946cad..7285fabb2 100644
--- a/dbms/src/Interpreters/ExpressionAnalyzer.cpp
+++ b/dbms/src/Interpreters/ExpressionAnalyzer.cpp
@@ -37,7 +37,6 @@
 #include <Interpreters/ProjectionManipulation.h>
 #include <Interpreters/evaluateConstantExpression.h>
 #include <Interpreters/TranslateQualifiedNamesVisitor.h>
-#include <Interpreters/ExecuteScalarSubqueriesVisitor.h>
 
 #include <AggregateFunctions/AggregateFunctionFactory.h>
 #include <AggregateFunctions/parseAggregateFunctionParameters.h>
@@ -87,6 +86,8 @@ namespace ErrorCodes
     extern const int MULTIPLE_EXPRESSIONS_FOR_ALIAS;
     extern const int UNKNOWN_IDENTIFIER;
     extern const int CYCLIC_ALIASES;
+    extern const int INCORRECT_RESULT_OF_SCALAR_SUBQUERY;
+    extern const int TOO_MANY_ROWS;
     extern const int NOT_FOUND_COLUMN_IN_BLOCK;
     extern const int INCORRECT_ELEMENT_OF_SET;
     extern const int ALIAS_REQUIRED;
@@ -149,6 +150,16 @@ const std::unordered_set<String> possibly_injective_function_names
 namespace
 {
 
+bool functionIsInOperator(const String & name)
+{
+    return name == "in" || name == "notIn";
+}
+
+bool functionIsInOrGlobalInOperator(const String & name)
+{
+    return name == "in" || name == "notIn" || name == "globalIn" || name == "globalNotIn";
+}
+
 void removeDuplicateColumns(NamesAndTypesList & columns)
 {
     std::set<String> names;
@@ -893,13 +904,8 @@ void ExpressionAnalyzer::addAliasColumns()
 
 void ExpressionAnalyzer::executeScalarSubqueries()
 {
-    LogAST log;
-
     if (!select_query)
-    {
-        ExecuteScalarSubqueriesVisitor execute_scalar_subqueries_visitor(context, subquery_depth, log.stream());
-        execute_scalar_subqueries_visitor.visit(query);
-    }
+        executeScalarSubqueriesImpl(query);
     else
     {
         for (auto & child : query->children)
@@ -908,14 +914,143 @@ void ExpressionAnalyzer::executeScalarSubqueries()
             if (!typeid_cast<const ASTTableExpression *>(child.get())
                 && !typeid_cast<const ASTSelectQuery *>(child.get()))
             {
-                ExecuteScalarSubqueriesVisitor execute_scalar_subqueries_visitor(context, subquery_depth, log.stream());
-                execute_scalar_subqueries_visitor.visit(child);
+                executeScalarSubqueriesImpl(child);
             }
         }
     }
 }
 
 
+static ASTPtr addTypeConversion(std::unique_ptr<ASTLiteral> && ast, const String & type_name)
+{
+    auto func = std::make_shared<ASTFunction>();
+    ASTPtr res = func;
+    func->alias = ast->alias;
+    func->prefer_alias_to_column_name = ast->prefer_alias_to_column_name;
+    ast->alias.clear();
+    func->name = "CAST";
+    auto exp_list = std::make_shared<ASTExpressionList>();
+    func->arguments = exp_list;
+    func->children.push_back(func->arguments);
+    exp_list->children.emplace_back(ast.release());
+    exp_list->children.emplace_back(std::make_shared<ASTLiteral>(type_name));
+    return res;
+}
+
+
+void ExpressionAnalyzer::executeScalarSubqueriesImpl(ASTPtr & ast)
+{
+    /** Replace subqueries that return exactly one row
+      * ("scalar" subqueries) to the corresponding constants.
+      *
+      * If the subquery returns more than one column, it is replaced by a tuple of constants.
+      *
+      * Features
+      *
+      * A replacement occurs during query analysis, and not during the main runtime.
+      * This means that the progress indicator will not work during the execution of these requests,
+      *  and also such queries can not be aborted.
+      *
+      * But the query result can be used for the index in the table.
+      *
+      * Scalar subqueries are executed on the request-initializer server.
+      * The request is sent to remote servers with already substituted constants.
+      */
+
+    if (ASTSubquery * subquery = typeid_cast<ASTSubquery *>(ast.get()))
+    {
+        Context subquery_context = context;
+        Settings subquery_settings = context.getSettings();
+        subquery_settings.max_result_rows = 1;
+        subquery_settings.extremes = 0;
+        subquery_context.setSettings(subquery_settings);
+
+        ASTPtr subquery_select = subquery->children.at(0);
+        BlockIO res = InterpreterSelectWithUnionQuery(subquery_select, subquery_context, {}, QueryProcessingStage::Complete, subquery_depth + 1).execute();
+
+        Block block;
+        try
+        {
+            block = res.in->read();
+
+            if (!block)
+            {
+                /// Interpret subquery with empty result as Null literal
+                auto ast_new = std::make_unique<ASTLiteral>(Null());
+                ast_new->setAlias(ast->tryGetAlias());
+                ast = std::move(ast_new);
+                return;
+            }
+
+            if (block.rows() != 1 || res.in->read())
+                throw Exception("Scalar subquery returned more than one row", ErrorCodes::INCORRECT_RESULT_OF_SCALAR_SUBQUERY);
+        }
+        catch (const Exception & e)
+        {
+            if (e.code() == ErrorCodes::TOO_MANY_ROWS)
+                throw Exception("Scalar subquery returned more than one row", ErrorCodes::INCORRECT_RESULT_OF_SCALAR_SUBQUERY);
+            else
+                throw;
+        }
+
+        size_t columns = block.columns();
+        if (columns == 1)
+        {
+            auto lit = std::make_unique<ASTLiteral>((*block.safeGetByPosition(0).column)[0]);
+            lit->alias = subquery->alias;
+            lit->prefer_alias_to_column_name = subquery->prefer_alias_to_column_name;
+            ast = addTypeConversion(std::move(lit), block.safeGetByPosition(0).type->getName());
+        }
+        else
+        {
+            auto tuple = std::make_shared<ASTFunction>();
+            tuple->alias = subquery->alias;
+            ast = tuple;
+            tuple->name = "tuple";
+            auto exp_list = std::make_shared<ASTExpressionList>();
+            tuple->arguments = exp_list;
+            tuple->children.push_back(tuple->arguments);
+
+            exp_list->children.resize(columns);
+            for (size_t i = 0; i < columns; ++i)
+            {
+                exp_list->children[i] = addTypeConversion(
+                    std::make_unique<ASTLiteral>((*block.safeGetByPosition(i).column)[0]),
+                    block.safeGetByPosition(i).type->getName());
+            }
+        }
+    }
+    else
+    {
+        /** Don't descend into subqueries in FROM section.
+          */
+        if (!typeid_cast<ASTTableExpression *>(ast.get()))
+        {
+            /** Don't descend into subqueries in arguments of IN operator.
+              * But if an argument is not subquery, than deeper may be scalar subqueries and we need to descend in them.
+              */
+            ASTFunction * func = typeid_cast<ASTFunction *>(ast.get());
+
+            if (func && functionIsInOrGlobalInOperator(func->name))
+            {
+                for (auto & child : ast->children)
+                {
+                    if (child != func->arguments)
+                        executeScalarSubqueriesImpl(child);
+                    else
+                        for (size_t i = 0, size = func->arguments->children.size(); i < size; ++i)
+                            if (i != 1 || !typeid_cast<ASTSubquery *>(func->arguments->children[i].get()))
+                                executeScalarSubqueriesImpl(func->arguments->children[i]);
+                }
+            }
+            else
+                for (auto & child : ast->children)
+                    executeScalarSubqueriesImpl(child);
+        }
+    }
+}
+
+
 void ExpressionAnalyzer::optimizeGroupBy()
 {
     if (!(select_query && select_query->group_expression_list))
diff --git a/dbms/src/Interpreters/ExpressionJIT.cpp b/dbms/src/Interpreters/ExpressionJIT.cpp
index a88faf13c..71e4f6bce 100644
--- a/dbms/src/Interpreters/ExpressionJIT.cpp
+++ b/dbms/src/Interpreters/ExpressionJIT.cpp
@@ -322,13 +322,8 @@ class LLVMPreparedFunction : public PreparedFunctionImpl
 
 public:
     LLVMPreparedFunction(std::string name_, std::shared_ptr<LLVMContext> context)
-        : name(std::move(name_)), context(context)
-    {
-        auto it = context->symbols.find(name);
-        if (context->symbols.end() == it)
-            throw Exception("Cannot find symbol " + name + " in LLVMContext", ErrorCodes::LOGICAL_ERROR);
-        function = it->second;
-    }
+        : name(std::move(name_)), context(context), function(context->symbols.at(name))
+    {}
 
     String getName() const override { return name; }
 
@@ -515,15 +510,6 @@ LLVMFunction::LLVMFunction(const ExpressionActions::Actions & actions, std::shar
     compileFunctionToLLVMByteCode(context, *this);
 }
 
-llvm::Value * LLVMFunction::compile(llvm::IRBuilderBase & builder, ValuePlaceholders values) const
-{
-    auto it = subexpressions.find(name);
-    if (subexpressions.end() == it)
-        throw Exception("Cannot find subexpression " + name + " in LLVMFunction", ErrorCodes::LOGICAL_ERROR);
-    return it->second(builder, values);
-}
-
-
 PreparedFunctionPtr LLVMFunction::prepare(const Block &, const ColumnNumbers &, size_t) const { return std::make_shared<LLVMPreparedFunction>(name, context); }
 
 bool LLVMFunction::isDeterministic() const
@@ -705,6 +691,8 @@ void compileFunctions(ExpressionActions::Actions & actions, const Names & output
     static LLVMTargetInitializer initializer;
 
     auto dependents = getActionsDependents(actions, output_columns);
+    /// Initialize context as late as possible and only if needed
+    std::shared_ptr<LLVMContext> context;
     std::vector<ExpressionActions::Actions> fused(actions.size());
     for (size_t i = 0; i < actions.size(); ++i)
     {
@@ -720,7 +708,7 @@ void compileFunctions(ExpressionActions::Actions & actions, const Names & output
 
             auto hash_key = ExpressionActions::ActionsHash{}(fused[i]);
             {
-                std::lock_guard lock(mutex);
+                std::lock_guard<std::mutex> lock(mutex);
                 if (counter[hash_key]++ < min_count_to_compile)
                     continue;
             }
@@ -728,24 +716,26 @@ void compileFunctions(ExpressionActions::Actions & actions, const Names & output
             std::shared_ptr<LLVMFunction> fn;
             if (compilation_cache)
             {
-                std::tie(fn, std::ignore) = compilation_cache->getOrSet(hash_key, [&inlined_func=std::as_const(fused[i]), &sample_block] ()
+                /// Lock here, to be sure, that all functions will be compiled
+                std::lock_guard<std::mutex> lock(mutex);
+                /// Don't use getOrSet here, because sometimes we need to initialize context
+                fn = compilation_cache->get(hash_key);
+                if (!fn)
                 {
+                    if (!context)
+                        context = std::make_shared<LLVMContext>();
                     Stopwatch watch;
-                    std::shared_ptr<LLVMContext> context = std::make_shared<LLVMContext>();
-                    auto result_fn = std::make_shared<LLVMFunction>(inlined_func, context, sample_block);
-                    size_t used_memory = context->compileAllFunctionsToNativeCode();
-                    ProfileEvents::increment(ProfileEvents::CompileExpressionsBytes, used_memory);
+                    fn = std::make_shared<LLVMFunction>(fused[i], context, sample_block);
                     ProfileEvents::increment(ProfileEvents::CompileExpressionsMicroseconds, watch.elapsedMicroseconds());
-                    return result_fn;
-                });
+                    compilation_cache->set(hash_key, fn);
+                }
             }
             else
             {
-                std::shared_ptr<LLVMContext> context = std::make_shared<LLVMContext>();
+                if (!context)
+                    context = std::make_shared<LLVMContext>();
                 Stopwatch watch;
                 fn = std::make_shared<LLVMFunction>(fused[i], context, sample_block);
-                size_t used_memory = context->compileAllFunctionsToNativeCode();
-                ProfileEvents::increment(ProfileEvents::CompileExpressionsBytes, used_memory);
                 ProfileEvents::increment(ProfileEvents::CompileExpressionsMicroseconds, watch.elapsedMicroseconds());
             }
 
@@ -761,10 +751,20 @@ void compileFunctions(ExpressionActions::Actions & actions, const Names & output
             fused[*dep].insert(fused[*dep].end(), fused[i].begin(), fused[i].end());
     }
 
+    if (context)
+    {
+        /// Lock here, because other threads can get uncompilted functions from cache
+        std::lock_guard<std::mutex> lock(mutex);
+        size_t used_memory = context->compileAllFunctionsToNativeCode();
+        ProfileEvents::increment(ProfileEvents::CompileExpressionsBytes, used_memory);
+    }
+
     for (size_t i = 0; i < actions.size(); ++i)
     {
         if (actions[i].type == ExpressionAction::APPLY_FUNCTION && actions[i].is_function_compiled)
+        {
             actions[i].function = actions[i].function_base->prepare({}, {}, 0); /// Arguments are not used for LLVMFunction.
+        }
     }
 }
 
diff --git a/dbms/src/Interpreters/ExpressionJIT.h b/dbms/src/Interpreters/ExpressionJIT.h
index f5ec420f3..756ca5581 100644
--- a/dbms/src/Interpreters/ExpressionJIT.h
+++ b/dbms/src/Interpreters/ExpressionJIT.h
@@ -30,7 +30,7 @@ public:
 
     bool isCompilable() const override { return true; }
 
-    llvm::Value * compile(llvm::IRBuilderBase & builder, ValuePlaceholders values) const override;
+    llvm::Value * compile(llvm::IRBuilderBase & builder, ValuePlaceholders values) const override { return subexpressions.at(name)(builder, values); }
 
     String getName() const override { return name; }
 
diff --git a/dbms/src/Interpreters/InterpreterCreateQuery.cpp b/dbms/src/Interpreters/InterpreterCreateQuery.cpp
index 69e7ae63a..5ee605c28 100644
--- a/dbms/src/Interpreters/InterpreterCreateQuery.cpp
+++ b/dbms/src/Interpreters/InterpreterCreateQuery.cpp
@@ -41,6 +41,8 @@
 
 #include <Common/ZooKeeper/ZooKeeper.h>
 
+#include <Compression/CompressionFactory.h>
+
 
 namespace DB
 {
@@ -167,13 +169,16 @@ BlockIO InterpreterCreateQuery::createDatabase(ASTCreateQuery & create)
 
 
 using ColumnsAndDefaults = std::pair<NamesAndTypesList, ColumnDefaults>;
+using ColumnDefaultsAndCodecs = std::pair<ColumnDefaults, ColumnCodecs>;
+using ColumnsDefaultsAndCodecs = std::pair<NamesAndTypesList, ColumnDefaultsAndCodecs>;
 
 /// AST to the list of columns with types. Columns of Nested type are expanded into a list of real columns.
-static ColumnsAndDefaults parseColumns(const ASTExpressionList & column_list_ast, const Context & context)
+static ColumnsDefaultsAndCodecs parseColumns(const ASTExpressionList & column_list_ast, const Context & context)
 {
     /// list of table columns in correct order
     NamesAndTypesList columns{};
     ColumnDefaults defaults{};
+    ColumnCodecs codecs{};
 
     /// Columns requiring type-deduction or default_expression type-check
     std::vector<std::pair<NameAndTypePair *, ASTColumnDeclaration *>> defaulted_columns{};
@@ -217,6 +222,12 @@ static ColumnsAndDefaults parseColumns(const ASTExpressionList & column_list_ast
             else
                 default_expr_list->children.emplace_back(setAlias(col_decl.default_expression->clone(), col_decl.name));
         }
+
+        if (col_decl.codec)
+        {
+            auto codec = CompressionCodecFactory::instance().get(col_decl.codec);
+            codecs.emplace(col_decl.name, codec);
+        }
     }
 
     /// set missing types and wrap default_expression's in a conversion-function if necessary
@@ -266,14 +277,15 @@ static ColumnsAndDefaults parseColumns(const ASTExpressionList & column_list_ast
         }
     }
 
-    return {Nested::flatten(columns), defaults};
+    return {Nested::flatten(columns), {defaults, codecs}};
 }
 
 
-static NamesAndTypesList removeAndReturnColumns(ColumnsAndDefaults & columns_and_defaults, const ColumnDefaultKind kind)
+static NamesAndTypesList removeAndReturnColumns(ColumnsDefaultsAndCodecs & columns_and_defaults_and_codecs, const ColumnDefaultKind kind)
 {
-    auto & columns = columns_and_defaults.first;
-    auto & defaults = columns_and_defaults.second;
+    auto & columns = columns_and_defaults_and_codecs.first;
+    auto & defaults_and_codecs = columns_and_defaults_and_codecs.second;
+    auto & defaults = defaults_and_codecs.first;
 
     NamesAndTypesList removed{};
 
@@ -341,6 +353,18 @@ ASTPtr InterpreterCreateQuery::formatColumns(const ColumnsDescription & columns)
             column_declaration->default_expression = it->second.expression->clone();
         }
 
+        const auto ct = columns.codecs.find(column.name);
+        if (ct != std::end(columns.codecs))
+        {
+            String codec_desc;
+            ct->second->getCodecDesc(codec_desc);
+            codec_desc = "CODEC(" + codec_desc + ")";
+            auto pos = codec_desc.data();
+            const auto end = pos + codec_desc.size();
+            ParserIdentifierWithParameters codec_p;
+            column_declaration->codec = parseQuery(codec_p, pos, end, "column codec", 0);
+        }
+
         columns_list->children.push_back(column_declaration_ptr);
     }
 
@@ -352,11 +376,12 @@ ColumnsDescription InterpreterCreateQuery::getColumnsDescription(const ASTExpres
 {
     ColumnsDescription res;
 
-    auto && columns_and_defaults = parseColumns(columns, context);
-    res.materialized = removeAndReturnColumns(columns_and_defaults, ColumnDefaultKind::Materialized);
-    res.aliases = removeAndReturnColumns(columns_and_defaults, ColumnDefaultKind::Alias);
-    res.ordinary = std::move(columns_and_defaults.first);
-    res.defaults = std::move(columns_and_defaults.second);
+    auto && columns_and_defaults_and_codecs = parseColumns(columns, context);
+    res.materialized = removeAndReturnColumns(columns_and_defaults_and_codecs, ColumnDefaultKind::Materialized);
+    res.aliases = removeAndReturnColumns(columns_and_defaults_and_codecs, ColumnDefaultKind::Alias);
+    res.ordinary = std::move(columns_and_defaults_and_codecs.first);
+    res.defaults = std::move(columns_and_defaults_and_codecs.second.first);
+    res.codecs = std::move(columns_and_defaults_and_codecs.second.second);
 
     if (res.ordinary.size() + res.materialized.size() == 0)
         throw Exception{"Cannot CREATE table without physical columns", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED};
diff --git a/dbms/src/Interpreters/InterpreterSelectQuery.cpp b/dbms/src/Interpreters/InterpreterSelectQuery.cpp
index ddac4a076..71f39e909 100644
--- a/dbms/src/Interpreters/InterpreterSelectQuery.cpp
+++ b/dbms/src/Interpreters/InterpreterSelectQuery.cpp
@@ -417,22 +417,6 @@ InterpreterSelectQuery::AnalysisResult InterpreterSelectQuery::analyzeExpression
 
     res.subqueries_for_sets = query_analyzer->getSubqueriesForSets();
 
-    /// Check that PREWHERE doesn't contain unusual actions. Unusual actions are that can change number of rows.
-    if (res.prewhere_info)
-    {
-        auto check_actions = [](const ExpressionActionsPtr & actions)
-        {
-            if (actions)
-                for (const auto & action : actions->getActions())
-                    if (action.type == ExpressionAction::Type::JOIN || action.type == ExpressionAction::Type::ARRAY_JOIN)
-                        throw Exception("PREWHERE cannot contain ARRAY JOIN or JOIN action", ErrorCodes::ILLEGAL_PREWHERE);
-        };
-
-        check_actions(res.prewhere_info->prewhere_actions);
-        check_actions(res.prewhere_info->alias_actions);
-        check_actions(res.prewhere_info->remove_columns_actions);
-    }
-
     return res;
 }
 
@@ -691,10 +675,10 @@ static void getLimitLengthAndOffset(ASTSelectQuery & query, size_t & length, siz
     }
 }
 
-
 void InterpreterSelectQuery::executeFetchColumns(
     QueryProcessingStage::Enum processing_stage, Pipeline & pipeline, const PrewhereInfoPtr & prewhere_info)
 {
+
     const Settings & settings = context.getSettingsRef();
 
     /// Actions to calculate ALIAS if required.
@@ -822,6 +806,7 @@ void InterpreterSelectQuery::executeFetchColumns(
         }
     }
 
+
     /// Limitation on the number of columns to read.
     /// It's not applied in 'only_analyze' mode, because the query could be analyzed without removal of unnecessary columns.
     if (!only_analyze && settings.max_columns_to_read && required_columns.size() > settings.max_columns_to_read)
diff --git a/dbms/src/Interpreters/QueryNormalizer.cpp b/dbms/src/Interpreters/QueryNormalizer.cpp
index a1c699736..c76655286 100644
--- a/dbms/src/Interpreters/QueryNormalizer.cpp
+++ b/dbms/src/Interpreters/QueryNormalizer.cpp
@@ -21,6 +21,16 @@ namespace ErrorCodes
 }
 
 
+namespace
+{
+
+bool functionIsInOrGlobalInOperator(const String & name)
+{
+    return name == "in" || name == "notIn" || name == "globalIn" || name == "globalNotIn";
+}
+
+}
+
 QueryNormalizer::QueryNormalizer(ASTPtr & query, const QueryNormalizer::Aliases & aliases,
                                  const Settings & settings, const Names & all_column_names,
                                  const TableNamesAndColumnNames & table_names_and_column_names)
diff --git a/dbms/src/Interpreters/QueryNormalizer.h b/dbms/src/Interpreters/QueryNormalizer.h
index f5192dadd..ba0acd9ce 100644
--- a/dbms/src/Interpreters/QueryNormalizer.h
+++ b/dbms/src/Interpreters/QueryNormalizer.h
@@ -1,6 +1,5 @@
 #pragma once
 
-#include <Core/Names.h>
 #include <Parsers/IAST.h>
 #include <Interpreters/Settings.h>
 #include <Interpreters/evaluateQualified.h>
@@ -8,17 +7,6 @@
 namespace DB
 {
 
-inline bool functionIsInOperator(const String & name)
-{
-    return name == "in" || name == "notIn";
-}
-
-inline bool functionIsInOrGlobalInOperator(const String & name)
-{
-    return functionIsInOperator(name) || name == "globalIn" || name == "globalNotIn";
-}
-
-
 using TableNameAndColumnNames = std::pair<DatabaseAndTableWithAlias, Names>;
 using TableNamesAndColumnNames = std::vector<TableNameAndColumnNames>;
 
diff --git a/dbms/src/Interpreters/Settings.h b/dbms/src/Interpreters/Settings.h
index 6eb85e9c4..1cc816e24 100644
--- a/dbms/src/Interpreters/Settings.h
+++ b/dbms/src/Interpreters/Settings.h
@@ -277,7 +277,7 @@ struct Settings
     M(SettingBool, log_query_settings, true, "Log query settings into the query_log.") \
     M(SettingBool, log_query_threads, true, "Log query threads into system.query_thread_log table. This setting have effect only when 'log_queries' is true.") \
     M(SettingString, send_logs_level, "none", "Send server text logs with specified minumum level to client. Valid values: 'trace', 'debug', 'information', 'warning', 'error', 'none'") \
-    M(SettingBool, enable_optimize_predicate_expression, 0, "If it is set to true, optimize predicates to subqueries.") \
+    M(SettingBool, enable_optimize_predicate_expression, 1, "If it is set to true, optimize predicates to subqueries.") \
     \
     M(SettingUInt64, low_cardinality_max_dictionary_size, 8192, "Maximum size (in rows) of shared global dictionary for LowCardinality type.") \
     M(SettingBool, low_cardinality_use_single_dictionary_for_part, false, "LowCardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries.") \
diff --git a/dbms/src/Parsers/ASTColumnDeclaration.h b/dbms/src/Parsers/ASTColumnDeclaration.h
index 308e9b665..cfe2650d7 100644
--- a/dbms/src/Parsers/ASTColumnDeclaration.h
+++ b/dbms/src/Parsers/ASTColumnDeclaration.h
@@ -15,6 +15,7 @@ public:
     ASTPtr type;
     String default_specifier;
     ASTPtr default_expression;
+    ASTPtr codec;
 
     String getID() const override { return "ColumnDeclaration_" + name; }
 
@@ -29,6 +30,12 @@ public:
             res->children.push_back(res->type);
         }
 
+        if (codec)
+        {
+            res->codec=codec->clone();
+            res->children.push_back(res->codec);
+        }
+
         if (default_expression)
         {
             res->default_expression = default_expression->clone();
@@ -51,6 +58,12 @@ protected:
             type->formatImpl(settings, state, frame);
         }
 
+        if (codec)
+        {
+            settings.ostr << ' ';
+            codec->formatImpl(settings, state, frame);
+        }
+
         if (default_expression)
         {
             settings.ostr << ' ' << (settings.hilite ? hilite_keyword : "") << default_specifier << (settings.hilite ? hilite_none : "") << ' ';
diff --git a/dbms/src/Parsers/ASTLiteral.cpp b/dbms/src/Parsers/ASTLiteral.cpp
index 44c7662ba..18a16d0f6 100644
--- a/dbms/src/Parsers/ASTLiteral.cpp
+++ b/dbms/src/Parsers/ASTLiteral.cpp
@@ -12,7 +12,7 @@ void ASTLiteral::appendColumnNameImpl(WriteBuffer & ostr) const
     /// Special case for very large arrays. Instead of listing all elements, will use hash of them.
     /// (Otherwise column name will be too long, that will lead to significant slowdown of expression analysis.)
     if (value.getType() == Field::Types::Array
-        && value.get<const Array &>().size() > 100)        /// 100 - just arbitrary value.
+        && value.get<const Array &>().size() > 100)        /// 100 - just arbitary value.
     {
         SipHash hash;
         applyVisitor(FieldVisitorHash(hash), value);
diff --git a/dbms/src/Parsers/ParserCreateQuery.h b/dbms/src/Parsers/ParserCreateQuery.h
index 75ce5b805..411cc906e 100644
--- a/dbms/src/Parsers/ParserCreateQuery.h
+++ b/dbms/src/Parsers/ParserCreateQuery.h
@@ -98,6 +98,8 @@ class IParserColumnDeclaration : public IParserBase
 protected:
     const char * getName() const { return "column declaration"; }
     bool parseImpl(Pos & pos, ASTPtr & node, Expected & expected);
+    bool isDeclareColumnType(Pos & pos, Expected & expected);
+    bool isDeclareColumnCodec(Pos & pos, Expected & expected);
 };
 
 using ParserColumnDeclaration = IParserColumnDeclaration<ParserIdentifier>;
@@ -106,33 +108,26 @@ using ParserCompoundColumnDeclaration = IParserColumnDeclaration<ParserCompoundI
 template <typename NameParser>
 bool IParserColumnDeclaration<NameParser>::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)
 {
+    ASTPtr column_name;
+    ASTPtr column_type;
+    ASTPtr column_codec;
     NameParser name_parser;
-    ParserIdentifierWithOptionalParameters type_parser;
+    ParserKeyword s_alias{"ALIAS"};
     ParserKeyword s_default{"DEFAULT"};
     ParserKeyword s_materialized{"MATERIALIZED"};
-    ParserKeyword s_alias{"ALIAS"};
+    ParserIdentifierWithParameters codec_parser;
     ParserTernaryOperatorExpression expr_parser;
+    ParserIdentifierWithOptionalParameters type_parser;
 
-    /// mandatory column name
-    ASTPtr name;
-    if (!name_parser.parse(pos, name, expected))
+    if (!name_parser.parse(pos, column_name, expected))
         return false;
 
-    /** column name should be followed by type name if it
-      *    is not immediately followed by {DEFAULT, MATERIALIZED, ALIAS}
-      */
-    ASTPtr type;
-    const auto fallback_pos = pos;
-    if (!s_default.check(pos, expected) &&
-        !s_materialized.check(pos, expected) &&
-        !s_alias.check(pos, expected))
-    {
-        type_parser.parse(pos, type, expected);
-    }
-    else
-        pos = fallback_pos;
+    if (isDeclareColumnType(pos, expected))
+        type_parser.parse(pos, column_type, expected);
+
+    if (isDeclareColumnCodec(pos, expected))
+        codec_parser.parse(pos, column_codec, expected);
 
-    /// parse {DEFAULT, MATERIALIZED, ALIAS}
     String default_specifier;
     ASTPtr default_expression;
     Pos pos_before_specifier = pos;
@@ -146,16 +141,21 @@ bool IParserColumnDeclaration<NameParser>::parseImpl(Pos & pos, ASTPtr & node, E
         if (!expr_parser.parse(pos, default_expression, expected))
             return false;
     }
-    else if (!type)
-        return false; /// reject sole column name without type
 
     const auto column_declaration = std::make_shared<ASTColumnDeclaration>();
-    node = column_declaration;
-    column_declaration->name = typeid_cast<ASTIdentifier &>(*name).name;
-    if (type)
+
+    column_declaration->name = typeid_cast<ASTIdentifier *>(column_name.get())->name;
+
+    if (column_type)
     {
-        column_declaration->type = type;
-        column_declaration->children.push_back(std::move(type));
+        column_declaration->type = column_type;
+        column_declaration->children.push_back(std::move(column_type));
+    }
+
+    if (column_codec)
+    {
+        column_declaration->codec = column_codec;
+        column_declaration->children.push_back(std::move(column_codec));
     }
 
     if (default_expression)
@@ -165,9 +165,26 @@ bool IParserColumnDeclaration<NameParser>::parseImpl(Pos & pos, ASTPtr & node, E
         column_declaration->children.push_back(std::move(default_expression));
     }
 
+    node = column_declaration;
     return true;
 }
 
+template<typename NameParser>
+bool IParserColumnDeclaration<NameParser>::isDeclareColumnType(Pos & pos, Expected & expected)
+{
+    auto check_pos = pos;
+    return !ParserKeyword{"CODEC"}.check(check_pos, expected) &&
+           !ParserKeyword{"ALIAS"}.check(check_pos, expected) &&
+           !ParserKeyword{"DEFAULT"}.check(check_pos, expected) &&
+           !ParserKeyword{"MATERIALIZED"}.check(check_pos, expected);
+}
+template<typename NameParser>
+bool IParserColumnDeclaration<NameParser>::isDeclareColumnCodec(Pos & pos, Expected & expected)
+{
+    auto check_pos = pos;
+    return ParserKeyword{"CODEC"}.check(check_pos, expected);
+}
+
 class ParserColumnDeclarationList : public IParserBase
 {
 protected:
diff --git a/dbms/src/Storages/ColumnCodec.h b/dbms/src/Storages/ColumnCodec.h
new file mode 100644
index 000000000..63a604c81
--- /dev/null
+++ b/dbms/src/Storages/ColumnCodec.h
@@ -0,0 +1,11 @@
+#pragma once
+
+#include <string>
+#include <unordered_map>
+#include <Parsers/IAST.h>
+#include <Compression/ICompressionCodec.h>
+
+namespace DB
+{
+    using ColumnCodecs = std::unordered_map<std::string, CompressionCodecPtr>;
+}
diff --git a/dbms/src/Storages/ColumnsDescription.cpp b/dbms/src/Storages/ColumnsDescription.cpp
index cb67d01a4..28f30c69d 100644
--- a/dbms/src/Storages/ColumnsDescription.cpp
+++ b/dbms/src/Storages/ColumnsDescription.cpp
@@ -15,6 +15,7 @@
 #include <ext/map.h>
 
 #include <boost/range/join.hpp>
+#include <Compression/CompressionFactory.h>
 
 
 namespace DB
@@ -102,6 +103,15 @@ String ColumnsDescription::toString() const
     return buf.str();
 }
 
+CompressionCodecPtr ColumnsDescription::getCodec(const String & column_name) const
+{
+    const auto codec = codecs.find(column_name);
+
+    if (codec == codecs.end())
+        return CompressionCodecFactory::instance().getDefaultCodec();
+
+    return codec->second;
+}
 
 ColumnsDescription ColumnsDescription::parse(const String & str)
 {
diff --git a/dbms/src/Storages/ColumnsDescription.h b/dbms/src/Storages/ColumnsDescription.h
index 288d2712b..25358863e 100644
--- a/dbms/src/Storages/ColumnsDescription.h
+++ b/dbms/src/Storages/ColumnsDescription.h
@@ -4,6 +4,7 @@
 #include <Core/Names.h>
 #include <Storages/ColumnDefault.h>
 #include <Core/Block.h>
+#include <Storages/ColumnCodec.h>
 
 
 namespace DB
@@ -15,6 +16,7 @@ struct ColumnsDescription
     NamesAndTypesList materialized;
     NamesAndTypesList aliases;
     ColumnDefaults defaults;
+    ColumnCodecs codecs;
 
     ColumnsDescription() = default;
 
@@ -53,9 +55,10 @@ struct ColumnsDescription
 
     bool hasPhysical(const String & column_name) const;
 
-
     String toString() const;
 
+    CompressionCodecPtr getCodec(const String & column_name) const;
+
     static ColumnsDescription parse(const String & str);
 };
 
diff --git a/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.cpp b/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.cpp
index 303fce24c..82edb7baf 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.cpp
@@ -20,7 +20,7 @@ namespace ErrorCodes
 
 
 MergeTreeBaseBlockInputStream::MergeTreeBaseBlockInputStream(
-    const MergeTreeData & storage,
+    MergeTreeData & storage,
     const PrewhereInfoPtr & prewhere_info,
     UInt64 max_block_size_rows,
     UInt64 preferred_block_size_bytes,
diff --git a/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.h b/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.h
index 510408622..e2763b08e 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeBaseBlockInputStream.h
@@ -18,7 +18,7 @@ class MergeTreeBaseBlockInputStream : public IProfilingBlockInputStream
 {
 public:
     MergeTreeBaseBlockInputStream(
-        const MergeTreeData & storage,
+        MergeTreeData & storage,
         const PrewhereInfoPtr & prewhere_info,
         UInt64 max_block_size_rows,
         UInt64 preferred_block_size_bytes,
@@ -47,7 +47,7 @@ protected:
     void injectVirtualColumns(Block & block) const;
 
 protected:
-    const MergeTreeData & storage;
+    MergeTreeData & storage;
 
     PrewhereInfoPtr prewhere_info;
 
diff --git a/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.cpp b/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.cpp
index 16492b832..01548d097 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.cpp
@@ -16,7 +16,7 @@ namespace ErrorCodes
 
 
 MergeTreeBlockInputStream::MergeTreeBlockInputStream(
-    const MergeTreeData & storage_,
+    MergeTreeData & storage_,
     const MergeTreeData::DataPartPtr & owned_data_part_,
     size_t max_block_size_rows_,
     size_t preferred_block_size_bytes_,
diff --git a/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.h b/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.h
index 7411a7ff0..df63eaea5 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeBlockInputStream.h
@@ -17,7 +17,7 @@ class MergeTreeBlockInputStream : public MergeTreeBaseBlockInputStream
 {
 public:
     MergeTreeBlockInputStream(
-        const MergeTreeData & storage,
+        MergeTreeData & storage,
         const MergeTreeData::DataPartPtr & owned_data_part,
         size_t max_block_size_rows,
         size_t preferred_block_size_bytes,
diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.cpp b/dbms/src/Storages/MergeTree/MergeTreeData.cpp
index a31d12d93..0a55bfa97 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeData.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeData.cpp
@@ -1223,9 +1223,7 @@ MergeTreeData::AlterDataPartTransactionPtr MergeTreeData::alterDataPart(
           *  temporary column name ('converting_column_name') created in 'createConvertExpression' method
           *  will have old name of shared offsets for arrays.
           */
-        IMergedBlockOutputStream::WrittenOffsetColumns unused_written_offsets;
-        MergedColumnOnlyOutputStream out(
-            *this, in.getHeader(), full_path + part->name + '/', true /* sync */, compression_settings, true /* skip_offsets */, unused_written_offsets);
+        MergedColumnOnlyOutputStream out(*this, in.getHeader(), full_path + part->name + '/', true /* sync */, compression_settings, true /* skip_offsets */);
 
         in.readPrefix();
         out.writePrefix();
diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.h b/dbms/src/Storages/MergeTree/MergeTreeData.h
index 5ad413f21..55b09f43b 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeData.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeData.h
@@ -481,7 +481,7 @@ public:
         bool skip_sanity_checks);
 
     /// Should be called if part data is suspected to be corrupted.
-    void reportBrokenPart(const String & name) const
+    void reportBrokenPart(const String & name)
     {
         broken_part_callback(name);
     }
diff --git a/dbms/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/dbms/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
index 2244fb28a..7c923a249 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
@@ -22,6 +22,8 @@
 #include <Interpreters/MutationsInterpreter.h>
 #include <IO/CompressedWriteBuffer.h>
 #include <IO/CompressedReadBufferFromFile.h>
+#include <DataTypes/NestedUtils.h>
+#include <DataTypes/DataTypeArray.h>
 #include <Common/SimpleIncrement.h>
 #include <Common/interpolate.h>
 #include <Common/typeid_cast.h>
@@ -748,6 +750,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
         merge_entry->progress.store(column_sizes.keyColumnsProgress(sum_input_rows_exact, sum_input_rows_exact), std::memory_order_relaxed);
 
         BlockInputStreams column_part_streams(parts.size());
+        NameSet offset_columns_written;
 
         auto it_name_and_type = gathering_columns.cbegin();
 
@@ -764,20 +767,22 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
                 + "). It is a bug.", ErrorCodes::LOGICAL_ERROR);
 
         CompressedReadBufferFromFile rows_sources_read_buf(rows_sources_file_path, 0, 0);
-        IMergedBlockOutputStream::WrittenOffsetColumns written_offset_columns;
 
         for (size_t column_num = 0, gathering_column_names_size = gathering_column_names.size();
             column_num < gathering_column_names_size;
             ++column_num, ++it_name_and_type)
         {
             const String & column_name = it_name_and_type->name;
-            Names column_names{column_name};
+            const DataTypePtr & column_type = it_name_and_type->type;
+            const String offset_column_name = Nested::extractTableName(column_name);
+            Names column_name_{column_name};
             Float64 progress_before = merge_entry->progress.load(std::memory_order_relaxed);
+            bool offset_written = offset_columns_written.count(offset_column_name);
 
             for (size_t part_num = 0; part_num < parts.size(); ++part_num)
             {
                 auto column_part_stream = std::make_shared<MergeTreeBlockInputStream>(
-                    data, parts[part_num], DEFAULT_MERGE_BLOCK_SIZE, 0, 0, column_names, MarkRanges{MarkRange(0, parts[part_num]->marks_count)},
+                    data, parts[part_num], DEFAULT_MERGE_BLOCK_SIZE, 0, 0, column_name_, MarkRanges{MarkRange(0, parts[part_num]->marks_count)},
                     false, nullptr, true, min_bytes_when_use_direct_io, DBMS_DEFAULT_BUFFER_SIZE, false, Names{}, 0, true);
 
                 column_part_stream->setProgressCallback(MergeProgressCallbackVerticalStep(
@@ -788,8 +793,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
 
             rows_sources_read_buf.seek(0, 0);
             ColumnGathererStream column_gathered_stream(column_name, column_part_streams, rows_sources_read_buf);
-            MergedColumnOnlyOutputStream column_to(
-                data, column_gathered_stream.getHeader(), new_part_tmp_path, false, compression_settings, false, written_offset_columns);
+            MergedColumnOnlyOutputStream column_to(data, column_gathered_stream.getHeader(), new_part_tmp_path, false, compression_settings, offset_written);
             size_t column_elems_written = 0;
 
             column_to.writePrefix();
@@ -807,6 +811,9 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
                                 ", but " + toString(rows_written) + " rows of PK columns", ErrorCodes::LOGICAL_ERROR);
             }
 
+            if (typeid_cast<const DataTypeArray *>(column_type.get()))
+                offset_columns_written.emplace(offset_column_name);
+
             /// NOTE: 'progress' is modified by single thread, but it may be concurrently read from MergeListElement::getInfo() (StorageSystemMerges).
 
             merge_entry->columns_written = merging_column_names.size() + column_num;
@@ -964,9 +971,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor
             createHardLink(dir_it.path().toString(), destination.toString());
         }
 
-        IMergedBlockOutputStream::WrittenOffsetColumns unused_written_offsets;
-        MergedColumnOnlyOutputStream out(
-            data, in_header, new_part_tmp_path, /* sync = */ false, compression_settings, /* skip_offsets = */ false, unused_written_offsets);
+        MergedColumnOnlyOutputStream out(data, in_header, new_part_tmp_path, /* sync = */ false, compression_settings, /* skip_offsets = */ false);
 
         in->readPrefix();
         out.writePrefix();
diff --git a/dbms/src/Storages/MergeTree/MergeTreeDataPart.h b/dbms/src/Storages/MergeTree/MergeTreeDataPart.h
index d714cf1b0..03b6756bd 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeDataPart.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeDataPart.h
@@ -27,7 +27,7 @@ struct MergeTreeDataPart
     using Checksums = MergeTreeDataPartChecksums;
     using Checksum = MergeTreeDataPartChecksums::Checksum;
 
-    MergeTreeDataPart(const MergeTreeData & storage_, const String & name_, const MergeTreePartInfo & info_)
+    MergeTreeDataPart(MergeTreeData & storage_, const String & name_, const MergeTreePartInfo & info_)
         : storage(storage_), name(name_), info(info_)
     {
     }
@@ -77,7 +77,7 @@ struct MergeTreeDataPart
 
     bool isEmpty() const { return rows_count == 0; }
 
-    const MergeTreeData & storage;
+    MergeTreeData & storage;
 
     String name;
     MergeTreePartInfo info;
diff --git a/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
index 20042b528..438a660af 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
@@ -67,7 +67,7 @@ namespace ErrorCodes
 }
 
 
-MergeTreeDataSelectExecutor::MergeTreeDataSelectExecutor(const MergeTreeData & data_)
+MergeTreeDataSelectExecutor::MergeTreeDataSelectExecutor(MergeTreeData & data_)
     : data(data_), log(&Logger::get(data.getLogName() + " (SelectExecutor)"))
 {
 }
diff --git a/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
index 589fc6e85..f854480b3 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
@@ -17,7 +17,7 @@ class KeyCondition;
 class MergeTreeDataSelectExecutor
 {
 public:
-    MergeTreeDataSelectExecutor(const MergeTreeData & data_);
+    MergeTreeDataSelectExecutor(MergeTreeData & data_);
 
     /** When reading, selects a set of parts that covers the desired range of the index.
       * max_block_number_to_read - if not zero, do not read all the parts whose right border is greater than this threshold.
@@ -40,7 +40,7 @@ public:
         Int64 max_block_number_to_read) const;
 
 private:
-    const MergeTreeData & data;
+    MergeTreeData & data;
 
     Logger * log;
 
diff --git a/dbms/src/Storages/MergeTree/MergeTreeReadPool.cpp b/dbms/src/Storages/MergeTree/MergeTreeReadPool.cpp
index 33bdb8d2a..e48ac8591 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeReadPool.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeReadPool.cpp
@@ -16,7 +16,7 @@ namespace DB
 
 MergeTreeReadPool::MergeTreeReadPool(
     const size_t threads, const size_t sum_marks, const size_t min_marks_for_concurrent_read,
-    RangesInDataParts parts, const MergeTreeData & data, const PrewhereInfoPtr & prewhere_info,
+    RangesInDataParts parts, MergeTreeData & data, const PrewhereInfoPtr & prewhere_info,
     const bool check_columns, const Names & column_names,
     const BackoffSettings & backoff_settings, size_t preferred_block_size_bytes,
     const bool do_not_steal_tasks)
diff --git a/dbms/src/Storages/MergeTree/MergeTreeReadPool.h b/dbms/src/Storages/MergeTree/MergeTreeReadPool.h
index c0d10d01b..8b19c6b2a 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeReadPool.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeReadPool.h
@@ -67,7 +67,7 @@ private:
 public:
     MergeTreeReadPool(
         const size_t threads, const size_t sum_marks, const size_t min_marks_for_concurrent_read,
-        RangesInDataParts parts, const MergeTreeData & data, const PrewhereInfoPtr & prewhere_info,
+        RangesInDataParts parts, MergeTreeData & data, const PrewhereInfoPtr & prewhere_info,
         const bool check_columns, const Names & column_names,
         const BackoffSettings & backoff_settings, size_t preferred_block_size_bytes,
         const bool do_not_steal_tasks = false);
@@ -91,7 +91,7 @@ private:
         RangesInDataParts & parts, const size_t min_marks_for_concurrent_read);
 
     std::vector<std::shared_lock<std::shared_mutex>> per_part_columns_lock;
-    const MergeTreeData & data;
+    MergeTreeData & data;
     Names column_names;
     Names ordered_names;
     bool do_not_steal_tasks;
diff --git a/dbms/src/Storages/MergeTree/MergeTreeReader.cpp b/dbms/src/Storages/MergeTree/MergeTreeReader.cpp
index c9818c4ce..da65d38de 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeReader.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeReader.cpp
@@ -36,7 +36,7 @@ MergeTreeReader::~MergeTreeReader() = default;
 MergeTreeReader::MergeTreeReader(const String & path,
     const MergeTreeData::DataPartPtr & data_part, const NamesAndTypesList & columns,
     UncompressedCache * uncompressed_cache, MarkCache * mark_cache, bool save_marks_in_cache,
-    const MergeTreeData & storage, const MarkRanges & all_mark_ranges,
+    MergeTreeData & storage, const MarkRanges & all_mark_ranges,
     size_t aio_threshold, size_t max_read_buffer_size, const ValueSizeMap & avg_value_size_hints,
     const ReadBufferFromFileBase::ProfileCallback & profile_callback,
     clockid_t clock_type)
diff --git a/dbms/src/Storages/MergeTree/MergeTreeReader.h b/dbms/src/Storages/MergeTree/MergeTreeReader.h
index 8b165e607..32a71fe76 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeReader.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeReader.h
@@ -30,7 +30,7 @@ public:
         UncompressedCache * uncompressed_cache,
         MarkCache * mark_cache,
         bool save_marks_in_cache,
-        const MergeTreeData & storage, const MarkRanges & all_mark_ranges,
+        MergeTreeData & storage, const MarkRanges & all_mark_ranges,
         size_t aio_threshold, size_t max_read_buffer_size,
         const ValueSizeMap & avg_value_size_hints = ValueSizeMap{},
         const ReadBufferFromFileBase::ProfileCallback & profile_callback = ReadBufferFromFileBase::ProfileCallback{},
@@ -111,7 +111,7 @@ private:
     /// If save_marks_in_cache is false, then, if marks are not in cache, we will load them but won't save in the cache, to avoid evicting other data.
     bool save_marks_in_cache;
 
-    const MergeTreeData & storage;
+    MergeTreeData & storage;
     MarkRanges all_mark_ranges;
     size_t aio_threshold;
     size_t max_read_buffer_size;
diff --git a/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.cpp b/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.cpp
index b3e697b9b..e9f1fa26c 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.cpp
@@ -14,7 +14,7 @@ MergeTreeThreadBlockInputStream::MergeTreeThreadBlockInputStream(
     const size_t max_block_size_rows,
     size_t preferred_block_size_bytes,
     size_t preferred_max_column_in_block_size_bytes,
-    const MergeTreeData & storage,
+    MergeTreeData & storage,
     const bool use_uncompressed_cache,
     const PrewhereInfoPtr & prewhere_info,
     const Settings & settings,
diff --git a/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.h b/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.h
index be47ba3ca..06f76ccd2 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeThreadBlockInputStream.h
@@ -21,7 +21,7 @@ public:
         const size_t max_block_size,
         size_t preferred_block_size_bytes,
         size_t preferred_max_column_in_block_size_bytes,
-        const MergeTreeData & storage,
+        MergeTreeData & storage,
         const bool use_uncompressed_cache,
         const PrewhereInfoPtr & prewhere_info,
         const Settings & settings,
diff --git a/dbms/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/dbms/src/Storages/MergeTree/MergedBlockOutputStream.cpp
index c12ddc513..71de26e11 100644
--- a/dbms/src/Storages/MergeTree/MergedBlockOutputStream.cpp
+++ b/dbms/src/Storages/MergeTree/MergedBlockOutputStream.cpp
@@ -19,7 +19,6 @@ constexpr auto MARKS_FILE_EXTENSION = ".mrk";
 
 }
 
-
 /// Implementation of IMergedBlockOutputStream.
 
 IMergedBlockOutputStream::IMergedBlockOutputStream(
@@ -41,6 +40,7 @@ void IMergedBlockOutputStream::addStreams(
     const String & path,
     const String & name,
     const IDataType & type,
+    const CompressionCodecPtr & codec,
     size_t estimated_size,
     bool skip_offsets)
 {
@@ -59,8 +59,8 @@ void IMergedBlockOutputStream::addStreams(
             stream_name,
             path + stream_name, DATA_FILE_EXTENSION,
             path + stream_name, MARKS_FILE_EXTENSION,
+            codec,
             max_compress_block_size,
-            compression_settings,
             estimated_size,
             aio_threshold);
     };
@@ -71,7 +71,7 @@ void IMergedBlockOutputStream::addStreams(
 
 
 IDataType::OutputStreamGetter IMergedBlockOutputStream::createStreamGetter(
-        const String & name, WrittenOffsetColumns & offset_columns, bool skip_offsets)
+        const String & name, OffsetColumns & offset_columns, bool skip_offsets)
 {
     return [&, skip_offsets] (const IDataType::SubstreamPath & substream_path) -> WriteBuffer *
     {
@@ -94,7 +94,7 @@ void IMergedBlockOutputStream::writeData(
     const String & name,
     const IDataType & type,
     const IColumn & column,
-    WrittenOffsetColumns & offset_columns,
+    OffsetColumns & offset_columns,
     bool skip_offsets,
     IDataType::SerializeBinaryBulkStatePtr & serialization_state)
 {
@@ -183,15 +183,15 @@ IMergedBlockOutputStream::ColumnStream::ColumnStream(
     const std::string & data_file_extension_,
     const std::string & marks_path,
     const std::string & marks_file_extension_,
+    const CompressionCodecPtr & compression_codec,
     size_t max_compress_block_size,
-    CompressionSettings compression_settings,
     size_t estimated_size,
     size_t aio_threshold) :
     escaped_column_name(escaped_column_name_),
     data_file_extension{data_file_extension_},
     marks_file_extension{marks_file_extension_},
     plain_file(createWriteBufferFromFileBase(data_path + data_file_extension, estimated_size, aio_threshold, max_compress_block_size)),
-    plain_hashing(*plain_file), compressed_buf(plain_hashing, compression_settings), compressed(compressed_buf),
+    plain_hashing(*plain_file), compressed_buf(compression_codec->liftCompressed(plain_hashing)), compressed(*compressed_buf.get()),
     marks_file(marks_path + marks_file_extension, 4096, O_TRUNC | O_CREAT | O_WRONLY), marks(marks_file)
 {
 }
@@ -239,7 +239,10 @@ MergedBlockOutputStream::MergedBlockOutputStream(
 {
     init();
     for (const auto & it : columns_list)
-        addStreams(part_path, it.name, *it.type, 0, false);
+    {
+        const auto columns = storage.getColumns();
+        addStreams(part_path, it.name, *it.type, columns.getCodec(it.name), 0, false);
+    }
 }
 
 MergedBlockOutputStream::MergedBlockOutputStream(
@@ -265,7 +268,9 @@ MergedBlockOutputStream::MergedBlockOutputStream(
             if (it2 != merged_column_to_size_.end())
                 estimated_size = it2->second;
         }
-        addStreams(part_path, it.name, *it.type, estimated_size, false);
+
+        const auto columns = storage.getColumns();
+        addStreams(part_path, it.name, *it.type, columns.getCodec(it.name), estimated_size, false);
     }
 }
 
@@ -305,7 +310,7 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(
         IDataType::SerializeBinaryBulkSettings serialize_settings;
         serialize_settings.low_cardinality_max_dictionary_size = settings.low_cardinality_max_dictionary_size;
         serialize_settings.low_cardinality_use_single_dictionary_for_part = settings.low_cardinality_use_single_dictionary_for_part != 0;
-        WrittenOffsetColumns offset_columns;
+        OffsetColumns offset_columns;
         auto it = columns_list.begin();
         for (size_t i = 0; i < columns_list.size(); ++i, ++it)
         {
@@ -396,7 +401,7 @@ void MergedBlockOutputStream::writeImpl(const Block & block, const IColumn::Perm
     size_t rows = block.rows();
 
     /// The set of written offset columns so that you do not write shared offsets of nested structures columns several times
-    WrittenOffsetColumns offset_columns;
+    OffsetColumns offset_columns;
 
     auto sort_columns = storage.getPrimarySortColumns();
 
@@ -428,7 +433,7 @@ void MergedBlockOutputStream::writeImpl(const Block & block, const IColumn::Perm
     if (serialization_states.empty())
     {
         serialization_states.reserve(columns_list.size());
-        WrittenOffsetColumns tmp_offset_columns;
+        OffsetColumns tmp_offset_columns;
         IDataType::SerializeBinaryBulkSettings settings;
 
         for (const auto & col : columns_list)
@@ -502,15 +507,12 @@ void MergedBlockOutputStream::writeImpl(const Block & block, const IColumn::Perm
 /// Implementation of MergedColumnOnlyOutputStream.
 
 MergedColumnOnlyOutputStream::MergedColumnOnlyOutputStream(
-    MergeTreeData & storage_, const Block & header_, String part_path_, bool sync_,
-    CompressionSettings compression_settings, bool skip_offsets_,
-    WrittenOffsetColumns & already_written_offset_columns)
+    MergeTreeData & storage_, const Block & header_, String part_path_, bool sync_, CompressionSettings compression_settings, bool skip_offsets_)
     : IMergedBlockOutputStream(
         storage_, storage_.context.getSettings().min_compress_block_size,
         storage_.context.getSettings().max_compress_block_size, compression_settings,
         storage_.context.getSettings().min_bytes_to_use_direct_io),
-    header(header_), part_path(part_path_), sync(sync_), skip_offsets(skip_offsets_),
-    already_written_offset_columns(already_written_offset_columns)
+    header(header_), part_path(part_path_), sync(sync_), skip_offsets(skip_offsets_)
 {
 }
 
@@ -521,14 +523,15 @@ void MergedColumnOnlyOutputStream::write(const Block & block)
         column_streams.clear();
         serialization_states.clear();
         serialization_states.reserve(block.columns());
-        WrittenOffsetColumns tmp_offset_columns;
+        OffsetColumns tmp_offset_columns;
         IDataType::SerializeBinaryBulkSettings settings;
 
         for (size_t i = 0; i < block.columns(); ++i)
         {
             const auto & col = block.safeGetByPosition(i);
 
-            addStreams(part_path, col.name, *col.type, 0, skip_offsets);
+            const auto columns = storage.getColumns();
+            addStreams(part_path, col.name, *col.type, columns.getCodec(col.name), 0, skip_offsets);
             serialization_states.emplace_back(nullptr);
             settings.getter = createStreamGetter(col.name, tmp_offset_columns, false);
             col.type->serializeBinaryBulkStatePrefix(settings, serialization_states.back());
@@ -539,7 +542,7 @@ void MergedColumnOnlyOutputStream::write(const Block & block)
 
     size_t rows = block.rows();
 
-    WrittenOffsetColumns offset_columns = already_written_offset_columns;
+    OffsetColumns offset_columns;
     for (size_t i = 0; i < block.columns(); ++i)
     {
         const ColumnWithTypeAndName & column = block.safeGetByPosition(i);
@@ -562,11 +565,11 @@ MergeTreeData::DataPart::Checksums MergedColumnOnlyOutputStream::writeSuffixAndG
     IDataType::SerializeBinaryBulkSettings serialize_settings;
     serialize_settings.low_cardinality_max_dictionary_size = settings.low_cardinality_max_dictionary_size;
     serialize_settings.low_cardinality_use_single_dictionary_for_part = settings.low_cardinality_use_single_dictionary_for_part != 0;
-
-    for (size_t i = 0, size = header.columns(); i < size; ++i)
+    OffsetColumns offset_columns;
+    for (size_t i = 0; i < header.columns(); ++i)
     {
-        auto & column = header.getByPosition(i);
-        serialize_settings.getter = createStreamGetter(column.name, already_written_offset_columns, skip_offsets);
+        auto & column = header.safeGetByPosition(i);
+        serialize_settings.getter = createStreamGetter(column.name, offset_columns, skip_offsets);
         column.type->serializeBinaryBulkStateSuffix(serialize_settings, serialization_states[i]);
     }
 
diff --git a/dbms/src/Storages/MergeTree/MergedBlockOutputStream.h b/dbms/src/Storages/MergeTree/MergedBlockOutputStream.h
index a3f6a025c..a93a64b48 100644
--- a/dbms/src/Storages/MergeTree/MergedBlockOutputStream.h
+++ b/dbms/src/Storages/MergeTree/MergedBlockOutputStream.h
@@ -23,9 +23,8 @@ public:
         CompressionSettings compression_settings_,
         size_t aio_threshold_);
 
-    using WrittenOffsetColumns = std::set<std::string>;
-
 protected:
+    using OffsetColumns = std::set<std::string>;
     using SerializationState = IDataType::SerializeBinaryBulkStatePtr;
     using SerializationStates = std::vector<SerializationState>;
 
@@ -37,8 +36,8 @@ protected:
             const std::string & data_file_extension_,
             const std::string & marks_path,
             const std::string & marks_file_extension_,
+            const CompressionCodecPtr & compression_codec,
             size_t max_compress_block_size,
-            CompressionSettings compression_settings,
             size_t estimated_size,
             size_t aio_threshold);
 
@@ -49,7 +48,7 @@ protected:
         /// compressed -> compressed_buf -> plain_hashing -> plain_file
         std::unique_ptr<WriteBufferFromFileBase> plain_file;
         HashingWriteBuffer plain_hashing;
-        CompressedWriteBuffer compressed_buf;
+        WriteBufferPtr compressed_buf;
         HashingWriteBuffer compressed;
 
         /// marks -> marks_file
@@ -65,13 +64,14 @@ protected:
 
     using ColumnStreams = std::map<String, std::unique_ptr<ColumnStream>>;
 
-    void addStreams(const String & path, const String & name, const IDataType & type, size_t estimated_size, bool skip_offsets);
+    void addStreams(const String & path, const String & name, const IDataType & type,
+                    const CompressionCodecPtr & codec, size_t estimated_size, bool skip_offsets);
 
 
-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenOffsetColumns & offset_columns, bool skip_offsets);
+    IDataType::OutputStreamGetter createStreamGetter(const String & name, OffsetColumns & offset_columns, bool skip_offsets);
 
     /// Write data of one column.
-    void writeData(const String & name, const IDataType & type, const IColumn & column, WrittenOffsetColumns & offset_columns,
+    void writeData(const String & name, const IDataType & type, const IColumn & column, OffsetColumns & offset_columns,
                    bool skip_offsets, IDataType::SerializeBinaryBulkStatePtr & serialization_state);
 
     MergeTreeData & storage;
@@ -151,17 +151,13 @@ private:
 };
 
 
-/// Writes only those columns that are in `header`
+/// Writes only those columns that are in `block`
 class MergedColumnOnlyOutputStream final : public IMergedBlockOutputStream
 {
 public:
     /// skip_offsets: used when ALTERing columns if we know that array offsets are not altered.
-    /// Pass empty 'already_written_offset_columns' first time then and pass the same object to subsequent instances of MergedColumnOnlyOutputStream
-    ///  if you want to serialize elements of Nested data structure in different instances of MergedColumnOnlyOutputStream.
     MergedColumnOnlyOutputStream(
-        MergeTreeData & storage_, const Block & header_, String part_path_, bool sync_,
-        CompressionSettings compression_settings, bool skip_offsets_,
-        WrittenOffsetColumns & already_written_offset_columns);
+        MergeTreeData & storage_, const Block & header_, String part_path_, bool sync_, CompressionSettings compression_settings, bool skip_offsets_);
 
     Block getHeader() const override { return header; }
     void write(const Block & block) override;
@@ -176,9 +172,6 @@ private:
     bool initialized = false;
     bool sync;
     bool skip_offsets;
-
-    /// To correctly write Nested elements column-by-column.
-    WrittenOffsetColumns & already_written_offset_columns;
 };
 
 }
diff --git a/dbms/src/Storages/StorageReplicatedMergeTree.cpp b/dbms/src/Storages/StorageReplicatedMergeTree.cpp
index bee9f627d..ddb5d2059 100644
--- a/dbms/src/Storages/StorageReplicatedMergeTree.cpp
+++ b/dbms/src/Storages/StorageReplicatedMergeTree.cpp
@@ -15,7 +15,6 @@
 #include <Databases/IDatabase.h>
 
 #include <Parsers/formatAST.h>
-#include <Parsers/ASTDropQuery.h>
 #include <Parsers/ASTOptimizeQuery.h>
 #include <Parsers/ASTLiteral.h>
 
@@ -3827,11 +3826,6 @@ void StorageReplicatedMergeTree::sendRequestToLeaderReplica(const ASTPtr & query
         optimize->database = leader_address.database;
         optimize->table = leader_address.table;
     }
-    else if (auto * drop = typeid_cast<ASTDropQuery *>(new_query.get()); drop->kind == ASTDropQuery::Kind::Truncate)
-    {
-        drop->database = leader_address.database;
-        drop->table    = leader_address.table;
-    }
     else
         throw Exception("Can't proxy this query. Unsupported query type", ErrorCodes::NOT_IMPLEMENTED);
 
diff --git a/dbms/src/Storages/System/StorageSystemNumbers.h b/dbms/src/Storages/System/StorageSystemNumbers.h
index 926e336e1..a23137fa9 100644
--- a/dbms/src/Storages/System/StorageSystemNumbers.h
+++ b/dbms/src/Storages/System/StorageSystemNumbers.h
@@ -17,7 +17,7 @@ class Context;
   * You could also specify a limit (how many numbers to give).
   * If multithreaded is specified, numbers will be generated in several streams
   *  (and result could be out of order). If both multithreaded and limit are specified,
-  *  the table could give you not exactly 1..limit range, but some arbitrary 'limit' numbers.
+  *  the table could give you not exactly 1..limit range, but some arbitary 'limit' numbers.
   */
 class StorageSystemNumbers : public ext::shared_ptr_helper<StorageSystemNumbers>, public IStorage
 {
diff --git a/dbms/tests/instructions/syntax.txt b/dbms/tests/instructions/syntax.txt
deleted file mode 100644
index 8ec0df48d..000000000
--- a/dbms/tests/instructions/syntax.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-# Quick syntax check (2 minutes on 16-core server)
-
-mkdir build && cd build
-CC=clang-8 CXX=clang++-8 cmake -D ENABLE_EMBEDDED_COMPILER=0 -D CMAKE_BUILD_TYPE=Debug ..
-ninja re2_st
-time jq --raw-output '.[] | .command' compile_commands.json | grep -P -- ' -o [^ ]+\.o' | grep -v -P -- '-c .+/contrib/' | grep -vP '\.s$' | sed -r -e 's/ -o [^ ]+\.o/ -fsyntax-only/' | sort -R | xargs -I{} -P$(nproc) sh -c '{}'
diff --git a/dbms/tests/integration/README.md b/dbms/tests/integration/README.md
index 822e37e0a..868085031 100644
--- a/dbms/tests/integration/README.md
+++ b/dbms/tests/integration/README.md
@@ -14,9 +14,9 @@ Don't use Docker from your system repository.
 
 * [pip](https://pypi.python.org/pypi/pip). To install: `sudo apt-get install python-pip`
 * [py.test](https://docs.pytest.org/) testing framework. To install: `sudo -H pip install pytest`
-* [docker-compose](https://docs.docker.com/compose/) and additional python libraries. To install: `sudo -H pip install docker-compose docker dicttoxml kazoo PyMySQL psycopg2`
+* [docker-compose](https://docs.docker.com/compose/) and additional python libraries. To install: `sudo -H pip install docker-compose docker dicttoxml kazoo PyMySQL`
 
-(highly not recommended) If you really want to use OS packages on modern debian/ubuntu instead of "pip": `sudo apt install -y docker docker-compose python-pytest python-dicttoxml python-docker python-pymysql python-kazoo python-psycopg2`
+(highly not recommended) If you really want to use OS packages on modern debian/ubuntu instead of "pip": `sudo apt install -y docker docker-compose python-pytest python-dicttoxml python-docker python-pymysql python-kazoo`
 
 If you want to run the tests under a non-privileged user, you must add this user to `docker` group: `sudo usermod -aG docker $USER` and re-login.
 (You must close all your sessions (for example, restart your computer))
diff --git a/dbms/tests/integration/helpers/cluster.py b/dbms/tests/integration/helpers/cluster.py
index e124b15e0..6c4fdcbc7 100644
--- a/dbms/tests/integration/helpers/cluster.py
+++ b/dbms/tests/integration/helpers/cluster.py
@@ -13,7 +13,6 @@ import pymysql
 import xml.dom.minidom
 from kazoo.client import KazooClient
 from kazoo.exceptions import KazooException
-import psycopg2
 
 import docker
 from docker.errors import ContainerError
@@ -80,7 +79,6 @@ class ClickHouseCluster:
         self.instances = {}
         self.with_zookeeper = False
         self.with_mysql = False
-        self.with_postgres = False
         self.with_kafka = False
         self.with_odbc_drivers = False
 
@@ -94,7 +92,7 @@ class ClickHouseCluster:
             cmd += " client"
         return cmd
 
-    def add_instance(self, name, config_dir=None, main_configs=[], user_configs=[], macros={}, with_zookeeper=False, with_mysql=False, with_kafka=False, clickhouse_path_dir=None, with_odbc_drivers=False, with_postgres=False, hostname=None, env_variables={}, image="ubuntu:14.04"):
+    def add_instance(self, name, config_dir=None, main_configs=[], user_configs=[], macros={}, with_zookeeper=False, with_mysql=False, with_kafka=False, clickhouse_path_dir=None, with_odbc_drivers=False, hostname=None, env_variables={}, image="ubuntu:14.04"):
         """Add an instance to the cluster.
 
         name - the name of the instance directory and the value of the 'instance' macro in ClickHouse.
@@ -129,12 +127,6 @@ class ClickHouseCluster:
             self.base_mysql_cmd = ['docker-compose', '--project-directory', self.base_dir, '--project-name',
                                        self.project_name, '--file', p.join(HELPERS_DIR, 'docker_compose_mysql.yml')]
 
-        if with_postgres and not self.with_postgres:
-            self.with_postgres = True
-            self.base_cmd.extend(['--file', p.join(HELPERS_DIR, 'docker_compose_postgres.yml')])
-            self.base_postgres_cmd = ['docker-compose', '--project-directory', self.base_dir, '--project-name',
-                                       self.project_name, '--file', p.join(HELPERS_DIR, 'docker_compose_postgres.yml')]
-
         if with_odbc_drivers and not self.with_odbc_drivers:
             self.with_odbc_drivers = True
             if not self.with_mysql:
@@ -142,12 +134,6 @@ class ClickHouseCluster:
                 self.base_cmd.extend(['--file', p.join(HELPERS_DIR, 'docker_compose_mysql.yml')])
                 self.base_mysql_cmd = ['docker-compose', '--project-directory', self.base_dir, '--project-name',
                                        self.project_name, '--file', p.join(HELPERS_DIR, 'docker_compose_mysql.yml')]
-            if not self.with_postgres:
-                self.with_postgres = True
-                self.base_cmd.extend(['--file', p.join(HELPERS_DIR, 'docker_compose_postgres.yml')])
-                self.base_postgres_cmd = ['docker-compose', '--project-directory', self.base_dir, '--project-name',
-                                       self.project_name, '--file', p.join(HELPERS_DIR, 'docker_compose_postgres.yml')]
-
 
         if with_kafka and not self.with_kafka:
             self.with_kafka = True
@@ -182,21 +168,6 @@ class ClickHouseCluster:
 
         raise Exception("Cannot wait MySQL container")
 
-    def wait_postgres_to_start(self, timeout=60):
-        start = time.time()
-        while time.time() - start < timeout:
-            try:
-                conn_string = "host='localhost' user='postgres' password='mysecretpassword'"
-                conn = psycopg2.connect(conn_string)
-                conn.close()
-                print "Postgres Started"
-                return
-            except Exception as ex:
-                print "Can't connect to Postgres " + str(ex)
-                time.sleep(0.5)
-
-        raise Exception("Cannot wait Postgres container")
-
     def wait_zookeeper_to_start(self, timeout=60):
         start = time.time()
         while time.time() - start < timeout:
@@ -233,24 +204,20 @@ class ClickHouseCluster:
         self.docker_client = docker.from_env(version=self.docker_api_version)
 
         if self.with_zookeeper and self.base_zookeeper_cmd:
-            subprocess_check_call(self.base_zookeeper_cmd + ['up', '-d', '--force-recreate'])
+            subprocess_check_call(self.base_zookeeper_cmd + ['up', '-d', '--force-recreate', '--remove-orphans'])
             for command in self.pre_zookeeper_commands:
                 self.run_kazoo_commands_with_retries(command, repeats=5)
             self.wait_zookeeper_to_start(120)
 
         if self.with_mysql and self.base_mysql_cmd:
-            subprocess_check_call(self.base_mysql_cmd + ['up', '-d', '--force-recreate'])
+            subprocess_check_call(self.base_mysql_cmd + ['up', '-d', '--force-recreate', '--remove-orphans'])
             self.wait_mysql_to_start(120)
 
-        if self.with_postgres and self.base_postgres_cmd:
-            subprocess_check_call(self.base_postgres_cmd + ['up', '-d', '--force-recreate'])
-            self.wait_postgres_to_start(120)
-
         if self.with_kafka and self.base_kafka_cmd:
-            subprocess_check_call(self.base_kafka_cmd + ['up', '-d', '--force-recreate'])
+            subprocess_check_call(self.base_kafka_cmd + ['up', '-d', '--force-recreate', '--remove-orphans'])
             self.kafka_docker_id = self.get_instance_docker_id('kafka1')
 
-        subprocess_check_call(self.base_cmd + ['up', '-d', '--force-recreate'])
+        subprocess_check_call(self.base_cmd + ['up', '-d', '--force-recreate', '--remove-orphans'])
 
         start_deadline = time.time() + 20.0 # seconds
         for instance in self.instances.itervalues():
@@ -314,7 +281,7 @@ services:
             - {logs_dir}:/var/log/clickhouse-server/
             {odbc_ini_path}
         entrypoint:
-            -  clickhouse
+            -  /usr/bin/clickhouse
             -  server
             -  --config-file=/etc/clickhouse-server/config.xml
             -  --log-file=/var/log/clickhouse-server/clickhouse-server.log
@@ -477,18 +444,8 @@ class ClickHouseInstance:
                 },
                 "PostgreSQL": {
                     "DSN": "postgresql_odbc",
-                    "Database": "postgres",
-                    "UserName": "postgres",
-                    "Password": "mysecretpassword",
-                    "Port": "5432",
-                    "Servername": "postgres1",
-                    "Protocol": "9.3",
-                    "ReadOnly": "No",
-                    "RowVersioning": "No",
-                    "ShowSystemTables": "No",
                     "Driver": "/usr/lib/x86_64-linux-gnu/odbc/psqlodbca.so",
                     "Setup": "/usr/lib/x86_64-linux-gnu/odbc/libodbcpsqlS.so",
-                    "ConnSettings": "",
                 }
             }
         else:
diff --git a/dbms/tests/integration/helpers/docker_compose_postgres.yml b/dbms/tests/integration/helpers/docker_compose_postgres.yml
deleted file mode 100644
index 74049d9f0..000000000
--- a/dbms/tests/integration/helpers/docker_compose_postgres.yml
+++ /dev/null
@@ -1,9 +0,0 @@
-version: '2'
-services:
-    postgres1:
-        image: postgres
-        restart: always
-        environment:
-            POSTGRES_PASSWORD: mysecretpassword
-        ports:
-          - 5432:5432
diff --git a/dbms/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml b/dbms/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml
deleted file mode 100644
index 1c293f667..000000000
--- a/dbms/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml
+++ /dev/null
@@ -1,38 +0,0 @@
-<dictionaries>
-    <dictionary>
-        <name>postgres_odbc_hashed</name>
-        <source>
-            <odbc>
-                <table>clickhouse.test_table</table>
-                <connection_string>DSN=postgresql_odbc;</connection_string>
-                <db>postgres</db>
-            </odbc>
-        </source>
-        <lifetime>
-            <min>5</min>
-            <max>5</max>
-        </lifetime>
-        <layout>
-            <hashed />
-        </layout>
-
-        <structure>
-            <id>
-                <name>column1</name>
-            </id>
-
-            <attribute>
-                <name>column1</name>
-                <type>Int64</type>
-                <null_value>1</null_value>
-            </attribute>
-
-            <attribute>
-                <name>column2</name>
-                <type>String</type>
-                <null_value>''</null_value>
-            </attribute>
-
-        </structure>
-    </dictionary>
-</dictionaries>
diff --git a/dbms/tests/integration/test_odbc_interaction/test.py b/dbms/tests/integration/test_odbc_interaction/test.py
index 7b82d4a42..ec9a3ec32 100644
--- a/dbms/tests/integration/test_odbc_interaction/test.py
+++ b/dbms/tests/integration/test_odbc_interaction/test.py
@@ -3,14 +3,12 @@ import pytest
 
 import os
 import pymysql.cursors
-import psycopg2
-from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
 from helpers.cluster import ClickHouseCluster
 
 SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
 
 cluster = ClickHouseCluster(__file__, base_configs_dir=os.path.join(SCRIPT_DIR, 'configs'))
-node1 = cluster.add_instance('node1', with_odbc_drivers=True, with_mysql=True, image='alesapin/ubuntu_with_odbc:14.04', main_configs=['configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml', 'configs/dictionaries/sqlite3_odbc_cached_dictionary.xml', 'configs/dictionaries/postgres_odbc_hashed_dictionary.xml'])
+node1 = cluster.add_instance('node1', with_odbc_drivers=True, with_mysql=True, image='alesapin/ubuntu_with_odbc:14.04', main_configs=['configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml', 'configs/dictionaries/sqlite3_odbc_cached_dictionary.xml'])
 
 create_table_sql_template =   """
     CREATE TABLE `clickhouse`.`{}` (
@@ -33,49 +31,24 @@ def create_mysql_table(conn, table_name):
     with conn.cursor() as cursor:
         cursor.execute(create_table_sql_template.format(table_name))
 
-def get_postgres_conn():
-    conn_string = "host='localhost' user='postgres' password='mysecretpassword'"
-    conn = psycopg2.connect(conn_string)
-    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
-    conn.autocommit = True
-    return conn
-
-def create_postgres_db(conn, name):
-    cursor = conn.cursor()
-    cursor.execute("CREATE SCHEMA {}".format(name))
-
 @pytest.fixture(scope="module")
 def started_cluster():
     try:
         cluster.start()
-        sqlite_db = node1.odbc_drivers["SQLite3"]["Database"]
+        sqlite_db =  node1.odbc_drivers["SQLite3"]["Database"]
 
-        print "sqlite data received"
         node1.exec_in_container(["bash", "-c", "echo 'CREATE TABLE t1(x INTEGER PRIMARY KEY ASC, y, z);' | sqlite3 {}".format(sqlite_db)], privileged=True, user='root')
         node1.exec_in_container(["bash", "-c", "echo 'CREATE TABLE t2(X INTEGER PRIMARY KEY ASC, Y, Z);' | sqlite3 {}".format(sqlite_db)], privileged=True, user='root')
         node1.exec_in_container(["bash", "-c", "echo 'CREATE TABLE t3(X INTEGER PRIMARY KEY ASC, Y, Z);' | sqlite3 {}".format(sqlite_db)], privileged=True, user='root')
         node1.exec_in_container(["bash", "-c", "echo 'CREATE TABLE t4(X INTEGER PRIMARY KEY ASC, Y, Z);' | sqlite3 {}".format(sqlite_db)], privileged=True, user='root')
-        print "sqlite tables created"
-        mysql_conn = get_mysql_conn()
-        print "mysql connection received"
+        conn = get_mysql_conn()
         ## create mysql db and table
-        create_mysql_db(mysql_conn, 'clickhouse')
-        print "mysql database created"
-
-        postgres_conn = get_postgres_conn()
-        print "postgres connection received"
-
-        create_postgres_db(postgres_conn, 'clickhouse')
-        print "postgres db created"
-
-        cursor = postgres_conn.cursor()
-        cursor.execute("create table if not exists clickhouse.test_table (column1 int primary key, column2 varchar(40) not null)")
+        create_mysql_db(conn, 'clickhouse')
 
         yield cluster
 
     except Exception as ex:
         print(ex)
-        raise ex
     finally:
         cluster.shutdown()
 
@@ -168,11 +141,3 @@ def test_sqlite_odbc_cached_dictionary(started_cluster):
     time.sleep(5)
 
     assert node1.query("select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(1))") == "12\n"
-
-def test_postgres_odbc_hached_dictionary_with_schema(started_cluster):
-    conn = get_postgres_conn()
-    cursor = conn.cursor()
-    cursor.execute("insert into clickhouse.test_table values(1, 'hello'),(2, 'world')")
-    time.sleep(5)
-    assert node1.query("select dictGetString('postgres_odbc_hashed', 'column2', toUInt64(1))") == "hello\n"
-    assert node1.query("select dictGetString('postgres_odbc_hashed', 'column2', toUInt64(2))") == "world\n"
diff --git a/dbms/tests/queries/0_stateless/00642_cast.reference b/dbms/tests/queries/0_stateless/00642_cast.reference
index 296f1351c..fcfb73254 100644
--- a/dbms/tests/queries/0_stateless/00642_cast.reference
+++ b/dbms/tests/queries/0_stateless/00642_cast.reference
@@ -6,7 +6,6 @@ hello
 hello
 hello
 hello
-1970-01-01 00:00:01
 CREATE TABLE test.cast ( x UInt8,  e Enum8('hello' = 1, 'world' = 2) DEFAULT CAST(x, 'Enum8(\'hello\' = 1, \'world\' = 2)')) ENGINE = MergeTree ORDER BY e SETTINGS index_granularity = 8192
 x	UInt8		
 e	Enum8(\'hello\' = 1, \'world\' = 2)	DEFAULT	CAST(x, \'Enum8(\\\'hello\\\' = 1, \\\'world\\\' = 2)\')
diff --git a/dbms/tests/queries/0_stateless/00642_cast.sql b/dbms/tests/queries/0_stateless/00642_cast.sql
index 4f0c1e7f6..0e50d4c7e 100644
--- a/dbms/tests/queries/0_stateless/00642_cast.sql
+++ b/dbms/tests/queries/0_stateless/00642_cast.sql
@@ -15,8 +15,6 @@ SELECT cast(1 AS Enum8(
 SELECT CAST(1, 'Enum8(\'hello\' = 1,\n\t\'world\' = 2)');
 SELECT cast(1, 'Enum8(\'hello\' = 1,\n\t\'world\' = 2)');
 
-SELECT toTimeZone(CAST(1 AS TIMESTAMP), 'UTC');
-
 DROP TABLE IF EXISTS test.cast;
 CREATE TABLE test.cast
 (
diff --git a/dbms/tests/queries/0_stateless/00698_validate_array_sizes_for_nested_kshvakov.sql b/dbms/tests/queries/0_stateless/00698_validate_array_sizes_for_nested_kshvakov.sql
deleted file mode 100644
index 0d15c7f99..000000000
--- a/dbms/tests/queries/0_stateless/00698_validate_array_sizes_for_nested_kshvakov.sql
+++ /dev/null
@@ -1,17 +0,0 @@
-SET send_logs_level = 'none';
-
-DROP TABLE IF EXISTS test.Issue_2231_Invalid_Nested_Columns_Size;
-CREATE TABLE test.Issue_2231_Invalid_Nested_Columns_Size (
-    Date Date,
-    NestedColumn Nested(
-        ID    Int32,
-        Count Int64
-    )
-) Engine = MergeTree 
-    PARTITION BY tuple()
-    ORDER BY Date;
-
-INSERT INTO test.Issue_2231_Invalid_Nested_Columns_Size VALUES (today(), [2,2], [1]), (today(), [2,2], [1, 1]); -- { serverError 190 }
-
-SELECT * FROM test.Issue_2231_Invalid_Nested_Columns_Size;
-DROP TABLE test.Issue_2231_Invalid_Nested_Columns_Size;
diff --git a/dbms/tests/queries/0_stateless/00729_prewhere_array_join.reference b/dbms/tests/queries/0_stateless/00729_prewhere_array_join.reference
deleted file mode 100644
index 60f4d20e0..000000000
--- a/dbms/tests/queries/0_stateless/00729_prewhere_array_join.reference
+++ /dev/null
@@ -1,8 +0,0 @@
-background	1
-foreground	1
-heading	1
-image	1
-background	1
-foreground	1
-heading	1
-image	1
diff --git a/dbms/tests/queries/0_stateless/00729_prewhere_array_join.sql b/dbms/tests/queries/0_stateless/00729_prewhere_array_join.sql
deleted file mode 100644
index 8c0c86b19..000000000
--- a/dbms/tests/queries/0_stateless/00729_prewhere_array_join.sql
+++ /dev/null
@@ -1,33 +0,0 @@
-SET send_logs_level = 'none';
-USE test;
-
-drop table if exists t1;
-create table t1 (id UInt64, val Array(String),nid UInt64, eDate Date)ENGINE = MergeTree(eDate, (id, eDate), 8192);
-
-insert into t1 (id,val,nid,eDate) values (1,['background','foreground','heading','image'],1,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (1,['background','foreground','heading','image'],1,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (2,['background','foreground','heading','image'],1,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (2,[],2,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (3,[],4,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (3,[],5,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (3,[],6,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (3,[],7,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (3,[],8,'2018-09-27');
-
-select arrayJoin(val) as nameGroup6 from t1 prewhere notEmpty(toString(nameGroup6)) group by nameGroup6 order by nameGroup6; -- { serverError 182 }
-select arrayJoin(val) as nameGroup6, countDistinct(nid) as rowids from t1 where notEmpty(toString(nameGroup6)) group by nameGroup6 order by nameGroup6;
-select arrayJoin(val) as nameGroup6, countDistinct(nid) as rowids from t1 prewhere notEmpty(toString(nameGroup6)) group by nameGroup6 order by nameGroup6; -- { serverError 182 }
-
-drop table t1;
-create table t1 (id UInt64, val Array(String),nid UInt64, eDate Date) ENGINE = MergeTree(eDate, (id, eDate), 8192);
-
-insert into t1 (id,val,nid,eDate) values (1,['background','foreground','heading','image'],1,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (1,['background','foreground','heading','image'],1,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (2,['background','foreground','heading','image'],1,'2018-09-27');
-insert into t1 (id,val,nid,eDate) values (2,[],2,'2018-09-27');
-
-select arrayJoin(val) as nameGroup6 from t1 prewhere notEmpty(toString(nameGroup6)) group by nameGroup6 order by nameGroup6; -- { serverError 182 }
-select arrayJoin(val) as nameGroup6, countDistinct(nid) as rowids from t1 where notEmpty(toString(nameGroup6)) group by nameGroup6 order by nameGroup6;
-select arrayJoin(val) as nameGroup6, countDistinct(nid) as rowids from t1 prewhere notEmpty(toString(nameGroup6)) group by nameGroup6 order by nameGroup6; -- { serverError 182 }
-
-drop table t1;
diff --git a/dbms/tests/queries/0_stateless/00732_decimal_summing_merge_tree.reference b/dbms/tests/queries/0_stateless/00732_decimal_summing_merge_tree.reference
deleted file mode 100644
index 1644f5099..000000000
--- a/dbms/tests/queries/0_stateless/00732_decimal_summing_merge_tree.reference
+++ /dev/null
@@ -1,2 +0,0 @@
-2001-01-01	2.0000	0.00000000	-2.0000000000
-2001-01-01	0.0000	1.00000000	0.0000000000
diff --git a/dbms/tests/queries/0_stateless/00732_decimal_summing_merge_tree.sql b/dbms/tests/queries/0_stateless/00732_decimal_summing_merge_tree.sql
deleted file mode 100644
index f56028343..000000000
--- a/dbms/tests/queries/0_stateless/00732_decimal_summing_merge_tree.sql
+++ /dev/null
@@ -1,27 +0,0 @@
-CREATE DATABASE IF NOT EXISTS test;
-DROP TABLE IF EXISTS test.decimal_sum;
-CREATE TABLE test.decimal_sum
-(
-    date Date,
-    sum32 Decimal32(4),
-    sum64 Decimal64(8),
-    sum128 Decimal128(10)
-) Engine = SummingMergeTree(date, (date), 8192);
-
-INSERT INTO test.decimal_sum VALUES ('2001-01-01', 1, 1, -1);
-INSERT INTO test.decimal_sum VALUES ('2001-01-01', 1, -1, -1);
-
-OPTIMIZE TABLE test.decimal_sum;
-SELECT * FROM test.decimal_sum;
-
-INSERT INTO test.decimal_sum VALUES ('2001-01-01', -2, 1, 2);
-
-OPTIMIZE TABLE test.decimal_sum;
-SELECT * FROM test.decimal_sum;
-
-INSERT INTO test.decimal_sum VALUES ('2001-01-01', 0, -1, 0);
-
-OPTIMIZE TABLE test.decimal_sum;
-SELECT * FROM test.decimal_sum;
-
-drop table test.decimal_sum;
diff --git a/dbms/tests/queries/0_stateless/00733_if_datetime.reference b/dbms/tests/queries/0_stateless/00733_if_datetime.reference
deleted file mode 100644
index de6934f87..000000000
--- a/dbms/tests/queries/0_stateless/00733_if_datetime.reference
+++ /dev/null
@@ -1,16 +0,0 @@
-2001-02-03 04:05:06
-2000-01-01 00:00:00
-2001-02-03 04:05:06
-2000-01-01 00:00:00
-2001-02-03 04:05:06
-2000-01-01 00:00:00
-2001-02-03 04:05:06
-2000-01-01 00:00:00
-2001-02-03
-2000-01-01
-2001-02-03
-2000-01-01
-2001-02-03
-2000-01-01
-2001-02-03
-2000-01-01
diff --git a/dbms/tests/queries/0_stateless/00733_if_datetime.sql b/dbms/tests/queries/0_stateless/00733_if_datetime.sql
deleted file mode 100644
index 23ebc3d8a..000000000
--- a/dbms/tests/queries/0_stateless/00733_if_datetime.sql
+++ /dev/null
@@ -1,9 +0,0 @@
-SELECT number % 2 ? toDateTime('2000-01-01 00:00:00') : toDateTime('2001-02-03 04:05:06') FROM numbers(2);
-SELECT number % 2 ? toDateTime('2000-01-01 00:00:00') : materialize(toDateTime('2001-02-03 04:05:06')) FROM numbers(2);
-SELECT number % 2 ? materialize(toDateTime('2000-01-01 00:00:00')) : toDateTime('2001-02-03 04:05:06') FROM numbers(2);
-SELECT number % 2 ? materialize(toDateTime('2000-01-01 00:00:00')) : materialize(toDateTime('2001-02-03 04:05:06')) FROM numbers(2);
-
-SELECT number % 2 ? toDate('2000-01-01') : toDate('2001-02-03') FROM numbers(2);
-SELECT number % 2 ? toDate('2000-01-01') : materialize(toDate('2001-02-03')) FROM numbers(2);
-SELECT number % 2 ? materialize(toDate('2000-01-01')) : toDate('2001-02-03') FROM numbers(2);
-SELECT number % 2 ? materialize(toDate('2000-01-01')) : materialize(toDate('2001-02-03')) FROM numbers(2);
diff --git a/dbms/tests/queries/0_stateless/00734_timeslot.reference b/dbms/tests/queries/0_stateless/00734_timeslot.reference
deleted file mode 100644
index 5acfa2b53..000000000
--- a/dbms/tests/queries/0_stateless/00734_timeslot.reference
+++ /dev/null
@@ -1,2 +0,0 @@
-2000-01-02 03:00:00
-['2000-01-02 03:00:00','2000-01-02 03:30:00','2000-01-02 04:00:00','2000-01-02 04:30:00','2000-01-02 05:00:00','2000-01-02 05:30:00']
diff --git a/dbms/tests/queries/0_stateless/00734_timeslot.sql b/dbms/tests/queries/0_stateless/00734_timeslot.sql
deleted file mode 100644
index 7362074cc..000000000
--- a/dbms/tests/queries/0_stateless/00734_timeslot.sql
+++ /dev/null
@@ -1,2 +0,0 @@
-SELECT timeSlot(toDateTime('2000-01-02 03:04:05', 'UTC'));
-SELECT timeSlots(toDateTime('2000-01-02 03:04:05', 'UTC'), toUInt32(10000));
diff --git a/dbms/tests/queries/0_stateless/00735_conditional.reference b/dbms/tests/queries/0_stateless/00735_conditional.reference
deleted file mode 100644
index bdea753dc..000000000
--- a/dbms/tests/queries/0_stateless/00735_conditional.reference
+++ /dev/null
@@ -1,148 +0,0 @@
-value vs value
-0	1	1	Int8	Int8	Int8
-0	1	1	Int8	Int16	Int16
-0	1	1	Int8	Int32	Int32
-0	1	1	Int8	Int64	Int64
-0	1	1	Int8	UInt8	Int16
-0	1	1	Int8	UInt16	Int32
-0	1	1	Int8	UInt32	Int64
-0	1	1	Int8	Float32	Float32
-0	1	1	Int8	Float64	Float64
-0	1	1	Int16	Int8	Int16
-0	1	1	Int16	Int16	Int16
-0	1	1	Int16	Int32	Int32
-0	1	1	Int16	Int64	Int64
-0	1	1	Int16	UInt8	Int16
-0	1	1	Int16	UInt16	Int32
-0	1	1	Int16	UInt32	Int64
-0	1	1	Int16	Float32	Float32
-0	1	1	Int16	Float64	Float64
-0	1	1	Int32	Int8	Int32
-0	1	1	Int32	Int16	Int32
-0	1	1	Int32	Int32	Int32
-0	1	1	Int32	Int64	Int64
-0	1	1	Int32	UInt8	Int32
-0	1	1	Int32	UInt16	Int32
-0	1	1	Int32	UInt32	Int64
-0	1	1	Int32	Float32	Float64
-0	1	1	Int32	Float64	Float64
-0	1	1	Int64	Int8	Int64
-0	1	1	Int64	Int16	Int64
-0	1	1	Int64	Int32	Int64
-0	1	1	Int64	Int64	Int64
-0	1	1	Int64	UInt8	Int64
-0	1	1	Int64	UInt16	Int64
-0	1	1	Int64	UInt32	Int64
-0	1	1	UInt8	Int8	Int16
-0	1	1	UInt8	Int16	Int16
-0	1	1	UInt8	Int32	Int32
-0	1	1	UInt8	Int64	Int64
-0	1	1	UInt8	UInt8	UInt8
-0	1	1	UInt8	UInt16	UInt16
-0	1	1	UInt8	UInt32	UInt32
-0	1	1	UInt8	UInt64	UInt64
-0	1	1	UInt8	Float32	Float32
-0	1	1	UInt8	Float64	Float64
-0	1	1	UInt16	Int8	Int32
-0	1	1	UInt16	Int16	Int32
-0	1	1	UInt16	Int32	Int32
-0	1	1	UInt16	Int64	Int64
-0	1	1	UInt16	UInt8	UInt16
-0	1	1	UInt16	UInt16	UInt16
-0	1	1	UInt16	UInt32	UInt32
-0	1	1	UInt16	UInt64	UInt64
-0	1	1	UInt16	Float32	Float32
-0	1	1	UInt16	Float64	Float64
-0	1	1	UInt32	Int8	Int64
-0	1	1	UInt32	Int16	Int64
-0	1	1	UInt32	Int32	Int64
-0	1	1	UInt32	Int64	Int64
-0	1	1	UInt32	UInt8	UInt32
-0	1	1	UInt32	UInt16	UInt32
-0	1	1	UInt32	UInt32	UInt32
-0	1	1	UInt32	UInt64	UInt64
-0	1	1	UInt32	Float32	Float64
-0	1	1	UInt32	Float64	Float64
-0	1	1	UInt64	UInt8	UInt64
-0	1	1	UInt64	UInt16	UInt64
-0	1	1	UInt64	UInt32	UInt64
-0	1	1	UInt64	UInt64	UInt64
-0000-00-00	1970-01-02	1970-01-02	Date	Date	Date
-0000-00-00	1970-01-01 03:00:01	1970-01-01 03:00:01	Date	DateTime	DateTime
-0000-00-00 00:00:00	1970-01-02	1970-01-01 03:00:01	DateTime	Date	DateTime
-0000-00-00 00:00:00	1970-01-01 03:00:01	1970-01-01 03:00:01	DateTime	DateTime	DateTime
-00000000-0000-0000-0000-000000000000	00000000-0000-0001-0000-000000000000	00000000-0000-0001-0000-000000000000	UUID	UUID	UUID
-column vs value
-0	1	1	Int8	Int8	Int8
-0	1	1	Int8	Int16	Int16
-0	1	1	Int8	Int32	Int32
-0	1	1	Int8	Int64	Int64
-0	1	1	Int8	UInt8	Int16
-0	1	1	Int8	UInt16	Int32
-0	1	1	Int8	UInt32	Int64
-0	1	1	Int8	Float32	Float32
-0	1	1	Int8	Float64	Float64
-0	1	1	Int16	Int8	Int16
-0	1	1	Int16	Int16	Int16
-0	1	1	Int16	Int32	Int32
-0	1	1	Int16	Int64	Int64
-0	1	1	Int16	UInt8	Int16
-0	1	1	Int16	UInt16	Int32
-0	1	1	Int16	UInt32	Int64
-0	1	1	Int16	Float32	Float32
-0	1	1	Int16	Float64	Float64
-0	1	1	Int32	Int8	Int32
-0	1	1	Int32	Int16	Int32
-0	1	1	Int32	Int32	Int32
-0	1	1	Int32	Int64	Int64
-0	1	1	Int32	UInt8	Int32
-0	1	1	Int32	UInt16	Int32
-0	1	1	Int32	UInt32	Int64
-0	1	1	Int32	Float32	Float64
-0	1	1	Int32	Float64	Float64
-0	1	1	Int64	Int8	Int64
-0	1	1	Int64	Int16	Int64
-0	1	1	Int64	Int32	Int64
-0	1	1	Int64	Int64	Int64
-0	1	1	Int64	UInt8	Int64
-0	1	1	Int64	UInt16	Int64
-0	1	1	Int64	UInt32	Int64
-0	1	1	UInt8	Int8	Int16
-0	1	1	UInt8	Int16	Int16
-0	1	1	UInt8	Int32	Int32
-0	1	1	UInt8	Int64	Int64
-0	1	1	UInt8	UInt8	UInt8
-0	1	1	UInt8	UInt16	UInt16
-0	1	1	UInt8	UInt32	UInt32
-0	1	1	UInt8	UInt64	UInt64
-0	1	1	UInt8	Float32	Float32
-0	1	1	UInt8	Float64	Float64
-0	1	1	UInt16	Int8	Int32
-0	1	1	UInt16	Int16	Int32
-0	1	1	UInt16	Int32	Int32
-0	1	1	UInt16	Int64	Int64
-0	1	1	UInt16	UInt8	UInt16
-0	1	1	UInt16	UInt16	UInt16
-0	1	1	UInt16	UInt32	UInt32
-0	1	1	UInt16	UInt64	UInt64
-0	1	1	UInt16	Float32	Float32
-0	1	1	UInt16	Float64	Float64
-0	1	1	UInt32	Int8	Int64
-0	1	1	UInt32	Int16	Int64
-0	1	1	UInt32	Int32	Int64
-0	1	1	UInt32	Int64	Int64
-0	1	1	UInt32	UInt8	UInt32
-0	1	1	UInt32	UInt16	UInt32
-0	1	1	UInt32	UInt32	UInt32
-0	1	1	UInt32	UInt64	UInt64
-0	1	1	UInt32	Float32	Float64
-0	1	1	UInt32	Float64	Float64
-0	1	1	UInt64	UInt8	UInt64
-0	1	1	UInt64	UInt16	UInt64
-0	1	1	UInt64	UInt32	UInt64
-0	1	1	UInt64	UInt64	UInt64
-0000-00-00	1970-01-02	1970-01-02	Date	Date	Date
-0000-00-00	1970-01-01 03:00:01	1970-01-01 03:00:01	Date	DateTime	DateTime
-0000-00-00 00:00:00	1970-01-02	1970-01-01 03:00:01	DateTime	Date	DateTime
-0000-00-00 00:00:00	1970-01-01 03:00:01	1970-01-01 03:00:01	DateTime	DateTime	DateTime
-00000000-0000-0000-0000-000000000000	00000000-0000-0001-0000-000000000000	00000000-0000-0001-0000-000000000000	UUID	UUID	UUID
diff --git a/dbms/tests/queries/0_stateless/00735_conditional.sql b/dbms/tests/queries/0_stateless/00735_conditional.sql
deleted file mode 100644
index aeb90dca7..000000000
--- a/dbms/tests/queries/0_stateless/00735_conditional.sql
+++ /dev/null
@@ -1,379 +0,0 @@
-SET send_logs_level = 'none';
-
-SELECT 'value vs value';
-
-SELECT toInt8(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt8(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt8(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt8(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt8(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt8(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt8(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt8(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toInt16(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt16(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt16(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt16(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt16(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt16(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt16(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt16(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toInt32(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt32(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt32(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt32(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt32(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt32(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt32(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt32(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toInt64(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toInt64(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt64(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt64(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt64(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt64(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt64(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toInt64(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt64(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toInt64(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toUInt8(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt8(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt8(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt8(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt8(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt8(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt8(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toUInt16(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt16(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt16(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt16(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt16(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt16(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt16(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toUInt32(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt32(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt32(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt32(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt32(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt32(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt32(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toUInt64(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt64(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt64(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt64(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUInt64(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUInt64(0) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt64(0) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT toUInt64(0) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT toDate(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toDate(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toDate(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toDecimal32(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toDecimal64(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDate(0) AS x, toDecimal128(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-
-SELECT toDateTime(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toDateTime(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toDateTime(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toDecimal32(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toDecimal64(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toDateTime(0) AS x, toDecimal128(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-
-SELECT toUUID(0) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT toUUID(0) AS x, toDecimal32(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toDecimal64(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT toUUID(0) AS x, toDecimal128(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-
-SELECT 'column vs value';
-
-SELECT materialize(toInt8(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt8(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt8(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt8(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt8(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt8(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt8(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt8(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toInt16(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt16(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt16(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt16(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt16(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt16(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt16(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt16(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toInt32(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt32(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt32(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt32(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt32(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt32(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt32(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt32(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toInt64(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toInt64(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt64(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt64(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt64(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt64(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt64(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toInt64(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt64(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toInt64(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toUInt8(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt8(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt8(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt8(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt8(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt8(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt8(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toUInt16(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt16(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt16(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt16(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt16(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt16(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt16(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toUInt32(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt32(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt32(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt32(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt32(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt32(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt32(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toUInt64(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt64(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt64(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt64(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUInt64(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUInt64(0)) AS x, toDecimal32(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt64(0)) AS x, toDecimal64(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-SELECT materialize(toUInt64(0)) AS x, toDecimal128(1, 0) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 48 }
-
-SELECT materialize(toDate(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toDate(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toDate(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toDecimal32(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toDecimal64(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDate(0)) AS x, toDecimal128(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-
-SELECT materialize(toDateTime(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toDateTime(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toDateTime(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toDecimal32(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toDecimal64(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toDateTime(0)) AS x, toDecimal128(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-
-SELECT materialize(toUUID(0)) AS x, toInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toUInt8(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toUInt16(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toUInt32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toUInt64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toFloat32(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toFloat64(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toDate(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toDateTime(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toUUID(1) AS y, ((x > y) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z);
-SELECT materialize(toUUID(0)) AS x, toDecimal32(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toDecimal64(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
-SELECT materialize(toUUID(0)) AS x, toDecimal128(1, 0) AS y, ((x = 0) ? x : y) AS z, toTypeName(x), toTypeName(y), toTypeName(z); -- { serverError 386 }
diff --git a/dbms/tests/queries/0_stateless/00735_or_expr_optimize_bug.reference b/dbms/tests/queries/0_stateless/00735_or_expr_optimize_bug.reference
deleted file mode 100644
index ec635144f..000000000
--- a/dbms/tests/queries/0_stateless/00735_or_expr_optimize_bug.reference
+++ /dev/null
@@ -1 +0,0 @@
-9
diff --git a/dbms/tests/queries/0_stateless/00735_or_expr_optimize_bug.sql b/dbms/tests/queries/0_stateless/00735_or_expr_optimize_bug.sql
deleted file mode 100644
index 422f69878..000000000
--- a/dbms/tests/queries/0_stateless/00735_or_expr_optimize_bug.sql
+++ /dev/null
@@ -1,7 +0,0 @@
-CREATE DATABASE IF NOT EXISTS test;
-DROP TABLE IF EXISTS test.or_expr_bug;
-CREATE TABLE test.or_expr_bug (a UInt64, b UInt64) ENGINE = Memory;
-
-INSERT INTO test.or_expr_bug VALUES(1,21),(1,22),(1,23),(2,21),(2,22),(2,23),(3,21),(3,22),(3,23);
-
-SELECT count(*) FROM test.or_expr_bug WHERE (a=1 OR a=2 OR a=3) AND (b=21 OR b=22 OR b=23);
diff --git a/dbms/tests/queries/0_stateless/00736_disjunction_optimisation.reference b/dbms/tests/queries/0_stateless/00736_disjunction_optimisation.reference
deleted file mode 100644
index afd698b42..000000000
--- a/dbms/tests/queries/0_stateless/00736_disjunction_optimisation.reference
+++ /dev/null
@@ -1,90 +0,0 @@
-1	21
-1	22
-1	23
-2	21
-2	22
-2	23
-3	21
-3	22
-3	23
-1	21
-1	22
-1	23
-2	21
-2	22
-2	23
-3	21
-3	22
-3	23
-1	21
-1	22
-1	23
-2	21
-2	22
-2	23
-3	21
-3	22
-3	23
-1	1	21	1	1	1
-1	1	22	0	1	1
-1	1	23	0	0	1
-2	1	21	1	1	1
-2	1	22	0	1	1
-2	1	23	0	0	1
-3	1	21	1	1	1
-3	1	22	0	1	1
-3	1	23	0	0	1
-21	1
-22	1
-23	1
-21	1
-22	1
-23	1
-21	1
-22	1
-23	1
-1	21
-1	22
-1	23
-2	21
-2	22
-2	23
-3	21
-3	22
-3	23
-1	21
-1	22
-1	23
-2	21
-2	22
-2	23
-3	21
-3	22
-3	23
-1	21
-1	22
-1	23
-2	21
-2	22
-2	23
-3	21
-3	22
-3	23
-1	1	21	1	1	1
-1	1	22	0	1	1
-1	1	23	0	0	1
-2	1	21	1	1	1
-2	1	22	0	1	1
-2	1	23	0	0	1
-3	1	21	1	1	1
-3	1	22	0	1	1
-3	1	23	0	0	1
-21	1
-22	1
-23	1
-21	1
-22	1
-23	1
-21	1
-22	1
-23	1
diff --git a/dbms/tests/queries/0_stateless/00736_disjunction_optimisation.sql b/dbms/tests/queries/0_stateless/00736_disjunction_optimisation.sql
deleted file mode 100644
index caf2af7a0..000000000
--- a/dbms/tests/queries/0_stateless/00736_disjunction_optimisation.sql
+++ /dev/null
@@ -1,21 +0,0 @@
-DROP TABLE IF EXISTS test.bug;
-CREATE TABLE IF NOT EXISTS test.bug(k UInt64, s UInt64) ENGINE = Memory;
-insert into test.bug values(1,21),(1,22),(1,23),(2,21),(2,22),(2,23),(3,21),(3,22),(3,23);
-
-set optimize_min_equality_disjunction_chain_length = 2;
-
-select * from test.bug;
-select * from test.bug where (k =1 or k=2 or k =3) and (s=21 or s=22 or s=23);
-select * from (select * from test.bug where k=1 or k=2 or k=3) where (s=21 or s=22 or s=23);
-select k, (k=1 or k=2 or k=3), s, (s=21), (s=21 or s=22), (s=21 or s=22 or s=23) from test.bug;
-select s, (s=21 or s=22 or s=23) from test.bug;
-
-set optimize_min_equality_disjunction_chain_length = 3;
-
-select * from test.bug;
-select * from test.bug where (k =1 or k=2 or k =3) and (s=21 or s=22 or s=23);
-select * from (select * from test.bug where k=1 or k=2 or k=3) where (s=21 or s=22 or s=23);
-select k, (k=1 or k=2 or k=3), s, (s=21), (s=21 or s=22), (s=21 or s=22 or s=23) from test.bug;
-select s, (s=21 or s=22 or s=23) from test.bug;
-
-DROP TABLE test.bug;
diff --git a/dbms/tests/queries/0_stateless/00737_decimal_group_by.reference b/dbms/tests/queries/0_stateless/00737_decimal_group_by.reference
deleted file mode 100644
index 2f838f4bc..000000000
--- a/dbms/tests/queries/0_stateless/00737_decimal_group_by.reference
+++ /dev/null
@@ -1,11 +0,0 @@
-1.10
-2.1000
-3.100000000000
-1.20
-2.2000
-3.200000000000
-1.30
-2.3000
-3.300000000000
-1	1.000000000000000000	10.000000000000000000
-1	1.000000000000000000	10.000000000000000000
diff --git a/dbms/tests/queries/0_stateless/00737_decimal_group_by.sql b/dbms/tests/queries/0_stateless/00737_decimal_group_by.sql
deleted file mode 100644
index 0904bb1d6..000000000
--- a/dbms/tests/queries/0_stateless/00737_decimal_group_by.sql
+++ /dev/null
@@ -1,26 +0,0 @@
-select toDecimal32(1.1, 2) as x group by x;
-select toDecimal64(2.1, 4) as x group by x;
-select toDecimal128(3.1, 12) as x group by x;
-
-select materialize(toDecimal32(1.2, 2)) as x group by x;
-select materialize(toDecimal64(2.2, 4)) as x group by x;
-select materialize(toDecimal128(3.2, 12)) as x group by x;
-
-select x from (select toDecimal32(1.3, 2) x) group by x;
-select x from (select toDecimal64(2.3, 4) x) group by x;
-select x from (select toDecimal128(3.3, 12) x) group by x;
-
-DROP TABLE IF EXISTS test.decimal;
-CREATE TABLE IF NOT EXISTS test.decimal
-(
-    A UInt64,
-    B Decimal128(18),
-    C Decimal128(18)
-) Engine = Memory;
-
-INSERT INTO test.decimal VALUES (1,1,1), (1,1,2), (1,1,3), (1,1,4);
-
-SELECT A, toString(B) AS B_str, toString(SUM(C)) AS c_str FROM test.decimal GROUP BY A, B_str;
-SELECT A, B_str, toString(cc) FROM (SELECT A, toString(B) AS B_str, SUM(C) AS cc FROM test.decimal GROUP BY A, B_str);
-
-DROP TABLE test.decimal;
diff --git a/dbms/tests/queries/0_stateless/00738_nested_merge_multidimensional_array.reference b/dbms/tests/queries/0_stateless/00738_nested_merge_multidimensional_array.reference
deleted file mode 100644
index f2a3a5abf..000000000
--- a/dbms/tests/queries/0_stateless/00738_nested_merge_multidimensional_array.reference
+++ /dev/null
@@ -1,2 +0,0 @@
-2	2	1
-2	2	1
diff --git a/dbms/tests/queries/0_stateless/00738_nested_merge_multidimensional_array.sql b/dbms/tests/queries/0_stateless/00738_nested_merge_multidimensional_array.sql
deleted file mode 100644
index 41b13e32a..000000000
--- a/dbms/tests/queries/0_stateless/00738_nested_merge_multidimensional_array.sql
+++ /dev/null
@@ -1,14 +0,0 @@
-DROP TABLE IF EXISTS test.sites;
-CREATE TABLE test.sites (Domain UInt8, `Users.UserID` Array(UInt64), `Users.Dates` Array(Array(Date))) ENGINE = MergeTree ORDER BY Domain SETTINGS vertical_merge_algorithm_min_rows_to_activate = 0, vertical_merge_algorithm_min_columns_to_activate = 0;
-
-SYSTEM STOP MERGES;
-
-INSERT INTO test.sites VALUES (1,[1],[[]]);
-INSERT INTO test.sites VALUES (2,[1],[['2018-06-22']]);
-
-SELECT count(), countArray(Users.Dates), countArrayArray(Users.Dates) FROM test.sites;
-SYSTEM START MERGES;
-OPTIMIZE TABLE test.sites FINAL;
-SELECT count(), countArray(Users.Dates), countArrayArray(Users.Dates) FROM test.sites;
-
-DROP TABLE test.sites;
diff --git a/dbms/tests/queries/0_stateless/00739_array_element_nullable_string_mattrobenolt.reference b/dbms/tests/queries/0_stateless/00739_array_element_nullable_string_mattrobenolt.reference
deleted file mode 100644
index 945f0c2de..000000000
--- a/dbms/tests/queries/0_stateless/00739_array_element_nullable_string_mattrobenolt.reference
+++ /dev/null
@@ -1,4 +0,0 @@
-1	foo
-1	foo
-1	\N
-\N
diff --git a/dbms/tests/queries/0_stateless/00739_array_element_nullable_string_mattrobenolt.sql b/dbms/tests/queries/0_stateless/00739_array_element_nullable_string_mattrobenolt.sql
deleted file mode 100644
index a5b33c55d..000000000
--- a/dbms/tests/queries/0_stateless/00739_array_element_nullable_string_mattrobenolt.sql
+++ /dev/null
@@ -1,12 +0,0 @@
-create temporary table wups (a Array(Nullable(String)));
-select count(), a[1] from wups group by a[1];
-insert into wups (a) values(['foo']);
-select count(), a[1] from wups group by a[1];
-insert into wups (a) values([]);
-select count(), a[1] from wups group by a[1] order by a[1];
-
-drop temporary table wups;
-
-create temporary table wups (a Array(Nullable(String)));
-insert into wups (a) values([]);
-select a[1] from wups;
diff --git a/dbms/tests/queries/0_stateless/00740_optimize_predicate_expression.reference b/dbms/tests/queries/0_stateless/00740_optimize_predicate_expression.reference
deleted file mode 100644
index 6db331af7..000000000
--- a/dbms/tests/queries/0_stateless/00740_optimize_predicate_expression.reference
+++ /dev/null
@@ -1 +0,0 @@
-nan
diff --git a/dbms/tests/queries/0_stateless/00740_optimize_predicate_expression.sql b/dbms/tests/queries/0_stateless/00740_optimize_predicate_expression.sql
deleted file mode 100644
index 57a7aa81a..000000000
--- a/dbms/tests/queries/0_stateless/00740_optimize_predicate_expression.sql
+++ /dev/null
@@ -1,35 +0,0 @@
-DROP TABLE IF EXISTS test.perf;
-CREATE TABLE test.perf (site String, user_id UInt64, z Float64) ENGINE = Log;
-
-SELECT * FROM (SELECT perf_1.z AS z_1 FROM test.perf AS perf_1);
-
-SELECT sum(mul)/sqrt(sum(sqr_dif_1) * sum(sqr_dif_2)) AS z_r
-FROM(
-SELECT 
-        (SELECT avg(z_1) AS z_1_avg, 
-                avg(z_2) AS z_2_avg
-        FROM ( 
-            SELECT perf_1.site, perf_1.z AS z_1
-            FROM test.perf AS perf_1
-            WHERE user_id = 000
-        ) ALL INNER JOIN (
-            SELECT perf_2.site, perf_2.z AS z_2
-            FROM test.perf AS perf_2
-            WHERE user_id = 999
-        ) USING site) as avg_values,
-       z_1 - avg_values.1 AS dif_1, 
-       z_2 - avg_values.2 AS dif_2, 
-       dif_1 * dif_2 AS mul, 
-       dif_1*dif_1 AS sqr_dif_1, 
-       dif_2*dif_2 AS sqr_dif_2
-FROM (
-            SELECT perf_1.site, perf_1.z AS z_1
-            FROM test.perf AS perf_1
-            WHERE user_id = 000
-) ALL INNER JOIN (
-            SELECT perf_2.site, perf_2.z AS z_2
-            FROM test.perf AS perf_2
-            WHERE user_id = 999
-) USING site);
-
-DROP TABLE test.perf;
diff --git a/dbms/tests/queries/0_stateless/00741_client_comment_multiline.reference b/dbms/tests/queries/0_stateless/00741_client_comment_multiline.reference
deleted file mode 100644
index e69de29bb..000000000
diff --git a/dbms/tests/queries/0_stateless/00741_client_comment_multiline.sql b/dbms/tests/queries/0_stateless/00741_client_comment_multiline.sql
deleted file mode 100644
index 5ca79f1c2..000000000
--- a/dbms/tests/queries/0_stateless/00741_client_comment_multiline.sql
+++ /dev/null
@@ -1 +0,0 @@
-CREATE DATABASE IF NOT EXISTS test; -- foo
diff --git a/dbms/tests/queries/0_stateless/00742_require_join_strictness.reference b/dbms/tests/queries/0_stateless/00742_require_join_strictness.reference
deleted file mode 100644
index e69de29bb..000000000
diff --git a/dbms/tests/queries/0_stateless/00742_require_join_strictness.sql b/dbms/tests/queries/0_stateless/00742_require_join_strictness.sql
deleted file mode 100644
index 63d81b653..000000000
--- a/dbms/tests/queries/0_stateless/00742_require_join_strictness.sql
+++ /dev/null
@@ -1,3 +0,0 @@
-SET send_logs_level = 'none';
-SET join_default_strictness = '';
-SELECT * FROM system.one INNER JOIN (SELECT number AS k FROM system.numbers) ON dummy = k; -- { serverError 417 }
diff --git a/dbms/tests/queries/0_stateless/00743_limit_by_not_found_column.reference b/dbms/tests/queries/0_stateless/00743_limit_by_not_found_column.reference
deleted file mode 100644
index de72e35bf..000000000
--- a/dbms/tests/queries/0_stateless/00743_limit_by_not_found_column.reference
+++ /dev/null
@@ -1,2 +0,0 @@
-0
-http://reddit.com/r/cpp/comments/xyz
diff --git a/dbms/tests/queries/0_stateless/00743_limit_by_not_found_column.sql b/dbms/tests/queries/0_stateless/00743_limit_by_not_found_column.sql
deleted file mode 100644
index 2401e1713..000000000
--- a/dbms/tests/queries/0_stateless/00743_limit_by_not_found_column.sql
+++ /dev/null
@@ -1,65 +0,0 @@
-USE test;
-DROP TABLE IF EXISTS installation_stats;
-CREATE TABLE installation_stats (message String, info String, message_type String) ENGINE = Log;
-
-SELECT count(*) AS total
-FROM
-(
-    SELECT
-        message,
-        info,
-        count() AS cnt
-    FROM installation_stats
-    WHERE message_type LIKE 'fail'
-    GROUP BY
-        message,
-        info
-    ORDER BY cnt DESC
-    LIMIT 5 BY message
-);
-
-DROP TABLE installation_stats;
-
-CREATE TEMPORARY TABLE Accounts (AccountID UInt64, Currency String);
-
-SELECT AccountID
-FROM 
-(
-    SELECT 
-        AccountID, 
-        Currency
-    FROM Accounts 
-    LIMIT 2 BY Currency
-);
-
-CREATE TEMPORARY TABLE commententry1 (created_date Date, link_id String, subreddit String);
-INSERT INTO commententry1 VALUES ('2016-01-01', 'xyz', 'cpp');
-
-SELECT concat('http://reddit.com/r/', subreddit, '/comments/', replaceRegexpOne(link_id, 't[0-9]_', ''))
-FROM
-(
-    SELECT
-        y,
-        subreddit,
-        link_id,
-        cnt
-    FROM
-    (
-        SELECT
-            created_date AS y,
-            link_id,
-            subreddit,
-            count(*) AS cnt
-        FROM commententry1
-        WHERE toYear(created_date) = 2016
-        GROUP BY
-            y,
-            link_id,
-            subreddit
-        ORDER BY y ASC
-    )
-    ORDER BY
-        y ASC,
-        cnt DESC
-    LIMIT 1 BY y
-);
diff --git a/dbms/tests/queries/0_stateless/00744_join_not_found_column.reference b/dbms/tests/queries/0_stateless/00744_join_not_found_column.reference
deleted file mode 100644
index cd121fd3f..000000000
--- a/dbms/tests/queries/0_stateless/00744_join_not_found_column.reference
+++ /dev/null
@@ -1,2 +0,0 @@
-1
-1	1
diff --git a/dbms/tests/queries/0_stateless/00744_join_not_found_column.sql b/dbms/tests/queries/0_stateless/00744_join_not_found_column.sql
deleted file mode 100644
index c942b0819..000000000
--- a/dbms/tests/queries/0_stateless/00744_join_not_found_column.sql
+++ /dev/null
@@ -1,36 +0,0 @@
-CREATE TEMPORARY TABLE test
-(
-    x Int32
-);
-
-INSERT INTO test VALUES (1);
-
-SELECT x
-FROM
-(
-    SELECT
-        x,
-        1
-    FROM test
-    ALL INNER JOIN
-    (
-        SELECT
-            count(),
-            1
-        FROM test
-    ) USING (1)
-    LIMIT 10
-);
-
-SELECT
-    x,
-    1
-FROM test
-ALL INNER JOIN
-(
-    SELECT
-        count(),
-        1
-    FROM test
-) USING (1)
-LIMIT 10;
diff --git a/dbms/tests/queries/bugs/database_in_view.sql b/dbms/tests/queries/bugs/database_in_view.sql
deleted file mode 100644
index 0b7149e4e..000000000
--- a/dbms/tests/queries/bugs/database_in_view.sql
+++ /dev/null
@@ -1,18 +0,0 @@
-DROP TABLE IF EXISTS test.whoami;
-DROP TABLE IF EXISTS test.tellme;
-DROP TABLE IF EXISTS test.tellme_nested;
-
-use test;
-create view whoami as select 1 as n;
-create view tellme as select * from whoami;
-create view tellme_nested as select * from (select * from whoami);
-select * from tellme;
-select * from tellme_nested;
-
-use default;
-select * from test.tellme;
-select * from test.tellme_nested;
-
-DROP TABLE test.whoami;
-DROP TABLE test.tellme;
-DROP TABLE test.tellme_nested;
diff --git a/dbms/tests/queries/bugs/prewhere_alias_array.sql b/dbms/tests/queries/bugs/prewhere_alias_array.sql
deleted file mode 100644
index 3281c6ac0..000000000
--- a/dbms/tests/queries/bugs/prewhere_alias_array.sql
+++ /dev/null
@@ -1,4 +0,0 @@
-DROP TABLE IF EXISTS test.prewhere;
-CREATE TABLE test.prewhere (x Array(UInt64), y ALIAS x, s String) ENGINE = MergeTree ORDER BY tuple()
-SELECT count() FROM test.prewhere PREWHERE (length(s) >= 1) = 0 WHERE NOT ignore(y)
-DROP TABLE test.prewhere;
diff --git a/debian/changelog b/debian/changelog
index 8c8faa45d..6fb0d845d 100644
--- a/debian/changelog
+++ b/debian/changelog
@@ -1,5 +1,5 @@
-clickhouse (18.14.9) unstable; urgency=low
+clickhouse (18.14.6) unstable; urgency=low
 
   * Modified source code
 
- --  <root@yandex-team.ru>  Tue, 16 Oct 2018 15:58:16 +0300
+ --  <root@yandex-team.ru>  Thu, 11 Oct 2018 01:25:18 +0300
diff --git a/docker/client/Dockerfile b/docker/client/Dockerfile
index e1780db7c..33c41b7e4 100644
--- a/docker/client/Dockerfile
+++ b/docker/client/Dockerfile
@@ -1,7 +1,7 @@
 FROM ubuntu:18.04
 
 ARG repository="deb http://repo.yandex.ru/clickhouse/deb/stable/ main/"
-ARG version=18.14.9
+ARG version=18.14.6
 
 RUN apt-get update \
     && apt-get install --yes --no-install-recommends \
diff --git a/docker/server/Dockerfile b/docker/server/Dockerfile
index fc86c17ad..9563107ad 100644
--- a/docker/server/Dockerfile
+++ b/docker/server/Dockerfile
@@ -1,7 +1,7 @@
 FROM ubuntu:18.04
 
 ARG repository="deb http://repo.yandex.ru/clickhouse/deb/stable/ main/"
-ARG version=18.14.9
+ARG version=18.14.6
 ARG gosu_ver=1.10
 
 RUN apt-get update \
@@ -39,4 +39,4 @@ ENV CLICKHOUSE_CONFIG /etc/clickhouse-server/config.xml
 
 ENTRYPOINT ["/entrypoint.sh"]
 
-CMD /usr/bin/clickhouse-server --config=${CLICKHOUSE_CONFIG}
+CMD ["/usr/bin/clickhouse-server", "--config=${CLICKHOUSE_CONFIG}"]
diff --git a/docker/test/Dockerfile b/docker/test/Dockerfile
index 32f0143d6..5fba7d947 100644
--- a/docker/test/Dockerfile
+++ b/docker/test/Dockerfile
@@ -1,7 +1,7 @@
 FROM ubuntu:18.04
 
 ARG repository="deb http://repo.yandex.ru/clickhouse/deb/stable/ main/"
-ARG version=18.14.9
+ARG version=18.14.6
 
 RUN apt-get update && \
     apt-get install -y apt-transport-https dirmngr && \
diff --git a/docs/README.md b/docs/README.md
index fbc371d0d..d733c1a23 100644
--- a/docs/README.md
+++ b/docs/README.md
@@ -2,14 +2,12 @@
 
 Basically ClickHouse uses "documentation as code" approach, so you can edit Markdown files in this folder from GitHub web interface or fork ClickHouse repository, edit, commit, push and open pull request.
 
-At the moment documentation is bilingual in English and Russian, so it's better to try keeping languages in sync if you can, but it's not strictly required as there are people watching over this. If you add new article, you should also add it to `toc_{en,ru,zh,fa}.yaml` files with pages index.
+At the moment documentation is bilingual in English and Russian, so it's better to try keeping languages in sync if you can, but it's not strictly required as there are people watching over this. If you add new article, you should also add it to `toc_{en,ru}.yaml` file with pages index.
 
 Master branch is then asynchronously published to ClickHouse official website:
 
 * In English: https://clickhouse.yandex/docs/en/
 * In Russian: https://clickhouse.yandex/docs/ru/
-* In Chinese: https://clickhouse.yandex/docs/zh/
-* In Farsi: https://clickhouse.yandex/docs/fa/
 
 Infrastructure to build Markdown to documentation website resides in [tools](tools) folder, it has it's own [README.md](tools/README.md) with more details.
 
@@ -31,14 +29,6 @@ ClickHouse can be directly used by all sorts of either analysts and engineers, s
 * People tend to get temporary stuck with some specific words or phrases, usually auxiliary, for a shord period of time. So they get repeated over and over in small part of content, which looks weird when reading. It is easy to fix this by reading your text again before publishing, also you can use this opportunity to fix mistypes and lost punctuation.
 * Try to avoid naming the reader in text, it is not strictly prohibited though.
 
-# How to start translation to new language
-
-1. Create new docs subfolder named with [ISO-639-1 language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
-2. Add Markdown files with some translation, mirroring the folder structure of other languages
-3. Commit and open pull request with new content
-
-Some additional configuration has to be done to actually make new language live on official website, but it's not automated/documented yet, so we'll do it on our own after pull request with content is merged.
-
 # Quick cheatsheet on used Markdown dialect
 
 * Headers on separate line starting with `# `, `## ` or `### `.
@@ -48,7 +38,7 @@ Some additional configuration has to be done to actually make new language live
 * Inline piece of code is <code>&#96;in backticks&#96;</code>.
 * Multiline code block are <code>&#96;&#96;&#96;in triple backtick quotes &#96;&#96;&#96;</code>.
 * Brightly highlighted block of text starts with  `!!! info "Header"`, on next line 4 spaces and content. Instead of `info` can be `warning`.
-* Hide block to be opened by click: `<details markdown="1"> <summary>Header</summary> hidden content</details>`.
+* Hide block to be opened by click: `<details> <summary>Header</summary> hidden content</details>`.
 * Colored text: `<span style="color: red;">text</span>`.
 * Additional anchor to be linked to: `<a name="my_anchor"></a>`, for headers fully in English they are created automatically like `"FoO Bar" -> "foo-bar"`.
 * Table:
diff --git a/docs/en/data_types/array.md b/docs/en/data_types/array.md
index f8d909cfa..8bb8d2200 100644
--- a/docs/en/data_types/array.md
+++ b/docs/en/data_types/array.md
@@ -83,5 +83,3 @@ Code: 386. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception
 0 rows in set. Elapsed: 0.246 sec.
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/array/) <!--hide-->
diff --git a/docs/en/data_types/boolean.md b/docs/en/data_types/boolean.md
index 52d2c8e32..14cde1591 100644
--- a/docs/en/data_types/boolean.md
+++ b/docs/en/data_types/boolean.md
@@ -2,5 +2,3 @@
 
 There isn't a separate type for boolean values. They use the UInt8 type, restricted to the values 0 or 1.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/boolean/) <!--hide-->
diff --git a/docs/en/data_types/date.md b/docs/en/data_types/date.md
index b5bac572a..cb179c0d8 100644
--- a/docs/en/data_types/date.md
+++ b/docs/en/data_types/date.md
@@ -5,5 +5,3 @@ The minimum value is output as 0000-00-00.
 
 The date is stored without the time zone.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/date/) <!--hide-->
diff --git a/docs/en/data_types/datetime.md b/docs/en/data_types/datetime.md
index c87eacbc6..584a00485 100644
--- a/docs/en/data_types/datetime.md
+++ b/docs/en/data_types/datetime.md
@@ -13,5 +13,3 @@ By default, the client switches to the timezone of the server when it connects.
 
 So when working with a textual date (for example, when saving text dumps), keep in mind that there may be ambiguity during changes for daylight savings time, and there may be problems matching data if the time zone changed.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/datetime/) <!--hide-->
diff --git a/docs/en/data_types/decimal.md b/docs/en/data_types/decimal.md
index 101b59ada..fe321ee99 100644
--- a/docs/en/data_types/decimal.md
+++ b/docs/en/data_types/decimal.md
@@ -95,5 +95,3 @@ SELECT toDecimal32(1, 8) < 100
 ```
 DB::Exception: Can't compare.
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/decimal/) <!--hide-->
diff --git a/docs/en/data_types/enum.md b/docs/en/data_types/enum.md
index 7faeecf5b..a3f44ca99 100644
--- a/docs/en/data_types/enum.md
+++ b/docs/en/data_types/enum.md
@@ -113,5 +113,3 @@ The Enum type can be changed without cost using ALTER, if only the set of values
 
 Using ALTER, it is possible to change an Enum8 to an Enum16 or vice versa, just like changing an Int8 to Int16.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/enum/) <!--hide-->
diff --git a/docs/en/data_types/fixedstring.md b/docs/en/data_types/fixedstring.md
index 5e158ccd2..d83b00870 100644
--- a/docs/en/data_types/fixedstring.md
+++ b/docs/en/data_types/fixedstring.md
@@ -8,5 +8,3 @@ Note that this behavior differs from MySQL behavior for the CHAR type (where str
 
 Fewer functions can work with the FixedString(N) type than with String, so it is less convenient to use.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/fixedstring/) <!--hide-->
diff --git a/docs/en/data_types/float.md b/docs/en/data_types/float.md
index c106640e8..d05758b0e 100644
--- a/docs/en/data_types/float.md
+++ b/docs/en/data_types/float.md
@@ -13,7 +13,7 @@ We recommend that you store data in integer form whenever possible. For example,
 
 - Computations with floating-point numbers might produce a rounding error.
 
-``` sql
+```sql
 SELECT 1 - 0.9
 ```
 
@@ -33,7 +33,7 @@ In contrast to standard SQL, ClickHouse supports the following categories of flo
 
 - `Inf` – Infinity.
 
-``` sql
+```sql
 SELECT 0.5 / 0
 ```
 
@@ -45,7 +45,7 @@ SELECT 0.5 / 0
 
 - `-Inf` – Negative infinity.
 
-``` sql
+```sql
 SELECT -0.5 / 0
 ```
 
@@ -69,5 +69,3 @@ SELECT 0 / 0
 
   See the rules for `NaN` sorting in the section [ORDER BY clause](../query_language/select.md#query_language-queries-order_by).
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/float/) <!--hide-->
diff --git a/docs/en/data_types/index.md b/docs/en/data_types/index.md
index 9acd30e11..ecab86c25 100644
--- a/docs/en/data_types/index.md
+++ b/docs/en/data_types/index.md
@@ -6,5 +6,3 @@ ClickHouse can store various types of data in table cells.
 
 This section describes the supported data types and special considerations when using and/or implementing them, if any.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/) <!--hide-->
diff --git a/docs/en/data_types/int_uint.md b/docs/en/data_types/int_uint.md
index 75a0d7bbd..cd940f287 100644
--- a/docs/en/data_types/int_uint.md
+++ b/docs/en/data_types/int_uint.md
@@ -18,5 +18,3 @@ Fixed-length integers, with or without a sign.
 - UInt32 - [0 : 4294967295]
 - UInt64 - [0 : 18446744073709551615]
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/int_uint/) <!--hide-->
diff --git a/docs/en/data_types/nested_data_structures/aggregatefunction.md b/docs/en/data_types/nested_data_structures/aggregatefunction.md
index 1db3bb9e0..33e7b4f31 100644
--- a/docs/en/data_types/nested_data_structures/aggregatefunction.md
+++ b/docs/en/data_types/nested_data_structures/aggregatefunction.md
@@ -2,5 +2,3 @@
 
 The intermediate state of an aggregate function. To get it, use aggregate functions with the '-State' suffix. For more information, see "AggregatingMergeTree".
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/nested_data_structures/aggregatefunction/) <!--hide-->
diff --git a/docs/en/data_types/nested_data_structures/index.md b/docs/en/data_types/nested_data_structures/index.md
index 1c4d2c902..53acd5b55 100644
--- a/docs/en/data_types/nested_data_structures/index.md
+++ b/docs/en/data_types/nested_data_structures/index.md
@@ -1,4 +1,2 @@
 # Nested Data Structures
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/nested_data_structures/) <!--hide-->
diff --git a/docs/en/data_types/nested_data_structures/nested.md b/docs/en/data_types/nested_data_structures/nested.md
index 1d90048d6..8a4bd4297 100644
--- a/docs/en/data_types/nested_data_structures/nested.md
+++ b/docs/en/data_types/nested_data_structures/nested.md
@@ -4,7 +4,7 @@ A nested data structure is like a nested table. The parameters of a nested data
 
 Example:
 
-``` sql
+```sql
 CREATE TABLE test.visits
 (
     CounterID UInt32,
@@ -35,7 +35,7 @@ In most cases, when working with a nested data structure, its individual columns
 
 Example:
 
-``` sql
+```sql
 SELECT
     Goals.ID,
     Goals.EventTime
@@ -44,7 +44,7 @@ WHERE CounterID = 101500 AND length(Goals.ID) < 5
 LIMIT 10
 ```
 
-```
+```text
 ┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐
 │ [1073752,591325,591325]        │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27']                       │
 │ [1073752]                      │ ['2014-03-17 00:28:25']                                                                   │
@@ -63,7 +63,7 @@ It is easiest to think of a nested data structure as a set of multiple column ar
 
 The only place where a SELECT query can specify the name of an entire nested data structure instead of individual columns is the ARRAY JOIN clause. For more information, see "ARRAY JOIN clause". Example:
 
-``` sql
+```sql
 SELECT
     Goal.ID,
     Goal.EventTime
@@ -73,7 +73,7 @@ WHERE CounterID = 101500 AND length(Goals.ID) < 5
 LIMIT 10
 ```
 
-```
+```text
 ┌─Goal.ID─┬──────Goal.EventTime─┐
 │ 1073752 │ 2014-03-17 16:38:10 │
 │  591325 │ 2014-03-17 16:38:48 │
@@ -96,5 +96,3 @@ For a DESCRIBE query, the columns in a nested data structure are listed separate
 
 The ALTER query is very limited for elements in a nested data structure.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/nested_data_structures/nested/) <!--hide-->
diff --git a/docs/en/data_types/nullable.md b/docs/en/data_types/nullable.md
index 87d0ac0c3..5cddb4c51 100644
--- a/docs/en/data_types/nullable.md
+++ b/docs/en/data_types/nullable.md
@@ -53,5 +53,3 @@ FROM t_null
 
 2 rows in set. Elapsed: 0.144 sec.
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/nullable/) <!--hide-->
diff --git a/docs/en/data_types/special_data_types/expression.md b/docs/en/data_types/special_data_types/expression.md
index 0bc7f635a..0fec4b715 100644
--- a/docs/en/data_types/special_data_types/expression.md
+++ b/docs/en/data_types/special_data_types/expression.md
@@ -2,5 +2,3 @@
 
 Used for representing lambda expressions in high-order functions.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/special_data_types/expression/) <!--hide-->
diff --git a/docs/en/data_types/special_data_types/index.md b/docs/en/data_types/special_data_types/index.md
index c66857cf5..2c4b62dea 100644
--- a/docs/en/data_types/special_data_types/index.md
+++ b/docs/en/data_types/special_data_types/index.md
@@ -2,5 +2,3 @@
 
 Special data type values can't be saved to a table or output in results, but are used as the intermediate result of running a query.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/special_data_types/) <!--hide-->
diff --git a/docs/en/data_types/special_data_types/nothing.md b/docs/en/data_types/special_data_types/nothing.md
index 5b916b055..525142fd3 100644
--- a/docs/en/data_types/special_data_types/nothing.md
+++ b/docs/en/data_types/special_data_types/nothing.md
@@ -20,5 +20,3 @@ SELECT toTypeName([])
 1 rows in set. Elapsed: 0.062 sec.
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/special_data_types/nothing/) <!--hide-->
diff --git a/docs/en/data_types/special_data_types/set.md b/docs/en/data_types/special_data_types/set.md
index d2d61e7ac..346fe51e2 100644
--- a/docs/en/data_types/special_data_types/set.md
+++ b/docs/en/data_types/special_data_types/set.md
@@ -2,5 +2,3 @@
 
 Used for the right half of an IN expression.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/special_data_types/set/) <!--hide-->
diff --git a/docs/en/data_types/string.md b/docs/en/data_types/string.md
index 5e2fe8e18..5386be28c 100644
--- a/docs/en/data_types/string.md
+++ b/docs/en/data_types/string.md
@@ -12,5 +12,3 @@ If you need to store texts, we recommend using UTF-8 encoding. At the very least
 Similarly, certain functions for working with strings have separate variations that work under the assumption that the string contains a set of bytes representing a UTF-8 encoded text.
 For example, the 'length' function calculates the string length in bytes, while the 'lengthUTF8' function calculates the string length in Unicode code points, assuming that the value is UTF-8 encoded.
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/string/) <!--hide-->
diff --git a/docs/en/data_types/tuple.md b/docs/en/data_types/tuple.md
index 592e5537b..98e548fe8 100644
--- a/docs/en/data_types/tuple.md
+++ b/docs/en/data_types/tuple.md
@@ -52,5 +52,3 @@ SELECT
 1 rows in set. Elapsed: 0.002 sec.
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/data_types/tuple/) <!--hide-->
diff --git a/docs/en/development/architecture.md b/docs/en/development/architecture.md
index e20dc2eeb..0f4435382 100644
--- a/docs/en/development/architecture.md
+++ b/docs/en/development/architecture.md
@@ -193,5 +193,3 @@ In addition, each replica stores its state in ZooKeeper as the set of parts and
 
 > The ClickHouse cluster consists of independent shards, and each shard consists of replicas. The cluster is not elastic, so after adding a new shard, data is not rebalanced between shards automatically. Instead, the cluster load will be uneven. This implementation gives you more control, and it is fine for relatively small clusters such as tens of nodes. But for clusters with hundreds of nodes that we are using in production, this approach becomes a significant drawback. We should implement a table engine that will span its data across the cluster with dynamically replicated regions that could be split and balanced between clusters automatically.
 
-
-[Original article](https://clickhouse.yandex/docs/en/development/architecture/) <!--hide-->
diff --git a/docs/en/development/build.md b/docs/en/development/build.md
index bb434ae5b..d2470cf2f 100644
--- a/docs/en/development/build.md
+++ b/docs/en/development/build.md
@@ -95,5 +95,3 @@ cd ..
 To create an executable, run `ninja clickhouse`.
 This will create the `dbms/programs/clickhouse` executable, which can be used with `client` or `server` arguments.
 
-
-[Original article](https://clickhouse.yandex/docs/en/development/build/) <!--hide-->
diff --git a/docs/en/development/build_osx.md b/docs/en/development/build_osx.md
index 7de659e7c..4e2aa2a6f 100644
--- a/docs/en/development/build_osx.md
+++ b/docs/en/development/build_osx.md
@@ -79,5 +79,3 @@ Reboot.
 
 To check if it's working, you can use `ulimit -n` command.
 
-
-[Original article](https://clickhouse.yandex/docs/en/development/build_osx/) <!--hide-->
diff --git a/docs/en/development/index.md b/docs/en/development/index.md
index d9095f383..a7b046fd2 100644
--- a/docs/en/development/index.md
+++ b/docs/en/development/index.md
@@ -1,4 +1,2 @@
 # ClickHouse Development
 
-
-[Original article](https://clickhouse.yandex/docs/en/development/) <!--hide-->
diff --git a/docs/en/development/style.md b/docs/en/development/style.md
index edfda73a7..d80f535e1 100644
--- a/docs/en/development/style.md
+++ b/docs/en/development/style.md
@@ -834,5 +834,3 @@ function(
       const & RangesInDataParts ranges,
       size_t limit)
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/development/style/) <!--hide-->
diff --git a/docs/en/development/tests.md b/docs/en/development/tests.md
index 5455a234a..ee2402aed 100644
--- a/docs/en/development/tests.md
+++ b/docs/en/development/tests.md
@@ -249,5 +249,3 @@ In Travis CI due to limit on time and computational power we can afford only sub
 In Jenkins we run functional tests for each commit and for each pull request from trusted users; the same under ASan; we also run quorum tests, dictionary tests, Metrica B2B tests. We use Jenkins to prepare and publish releases. Worth to note that we are not happy with Jenkins at all.
 
 One of our goals is to provide reliable testing infrastructure that will be available to community.
-
-[Original article](https://clickhouse.yandex/docs/en/development/tests/) <!--hide-->
diff --git a/docs/en/faq/general.md b/docs/en/faq/general.md
index f5b9d29b5..2b19d6f35 100644
--- a/docs/en/faq/general.md
+++ b/docs/en/faq/general.md
@@ -11,5 +11,3 @@ Distributed sorting is one of the main causes of reduced performance when runnin
 
 Most MapReduce implementations allow you to execute arbitrary code on a cluster. But a declarative query language is better suited to OLAP in order to run experiments quickly. For example, Hadoop has Hive and Pig. Also consider Cloudera Impala or Shark (outdated) for Spark, as well as Spark SQL, Presto, and Apache Drill. Performance when running such tasks is highly sub-optimal compared to specialized systems, but relatively high latency makes it unrealistic to use these systems as the backend for a web interface.
 
-
-[Original article](https://clickhouse.yandex/docs/en/faq/general/) <!--hide-->
diff --git a/docs/en/getting_started/example_datasets/amplab_benchmark.md b/docs/en/getting_started/example_datasets/amplab_benchmark.md
index e5d9812a9..49265d5da 100644
--- a/docs/en/getting_started/example_datasets/amplab_benchmark.md
+++ b/docs/en/getting_started/example_datasets/amplab_benchmark.md
@@ -21,7 +21,7 @@ cd ..
 
 Run the following ClickHouse queries:
 
-``` sql
+```sql
 CREATE TABLE rankings_tiny
 (
     pageURL String,
@@ -96,7 +96,7 @@ for i in 5nodes/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i |
 
 Queries for obtaining data samples:
 
-``` sql
+```sql
 SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank > 1000
 
 SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8)
@@ -119,5 +119,3 @@ ORDER BY totalRevenue DESC
 LIMIT 1
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/example_datasets/amplab_benchmark/) <!--hide-->
diff --git a/docs/en/getting_started/example_datasets/criteo.md b/docs/en/getting_started/example_datasets/criteo.md
index 268e8346d..8544c0da9 100644
--- a/docs/en/getting_started/example_datasets/criteo.md
+++ b/docs/en/getting_started/example_datasets/criteo.md
@@ -4,7 +4,7 @@ Download the data from <http://labs.criteo.com/downloads/download-terabyte-click
 
 Create a table to import the log to:
 
-``` sql
+```sql
 CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log
 ```
 
@@ -16,7 +16,7 @@ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/
 
 Create a table for the converted data:
 
-``` sql
+```sql
 CREATE TABLE criteo
 (
     date Date,
@@ -65,11 +65,9 @@ CREATE TABLE criteo
 
 Transform data from the raw log and put it in the second table:
 
-``` sql
+```sql
 INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log;
 
 DROP TABLE criteo_log;
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/example_datasets/criteo/) <!--hide-->
diff --git a/docs/en/getting_started/example_datasets/nyc_taxi.md b/docs/en/getting_started/example_datasets/nyc_taxi.md
index 0f5f1cb3c..19cf5e242 100644
--- a/docs/en/getting_started/example_datasets/nyc_taxi.md
+++ b/docs/en/getting_started/example_datasets/nyc_taxi.md
@@ -24,7 +24,7 @@ It takes about 20-30 minutes to process each month's worth of data in PostgreSQL
 
 You can check the number of downloaded rows as follows:
 
-```
+```text
 time psql nyc-taxi-data -c "SELECT count(*) FROM trips;"
 ## Count
  1298979494
@@ -39,7 +39,7 @@ The data in PostgreSQL uses 370 GB of space.
 
 Exporting the data from PostgreSQL:
 
-``` sql
+```sql
 COPY
 (
     SELECT trips.id,
@@ -114,7 +114,7 @@ This takes about 5 hours. The resulting TSV file is 590612904969 bytes.
 
 Create a temporary table in ClickHouse:
 
-``` sql
+```sql
 CREATE TABLE trips
 (
 trip_id                 UInt32,
@@ -173,7 +173,7 @@ dropoff_puma            Nullable(String)
 
 It is needed for converting fields to more correct data types and, if possible, to eliminate NULLs.
 
-```
+```text
 time clickhouse-client --query="INSERT INTO trips FORMAT TabSeparated" < trips.tsv
 
 real    75m56.214s
@@ -191,7 +191,7 @@ To start, we'll create a table on a single server. Later we will make the table
 
 Create and populate a summary table:
 
-```
+```text
 CREATE TABLE trips_mergetree
 ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
 AS SELECT
@@ -258,7 +258,7 @@ To load it faster, you can create the table with the `Log` engine instead of `Me
 
 The table uses 126 GB of disk space.
 
-```
+```text
 :) SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active
 
 SELECT formatReadableSize(sum(bytes))
@@ -276,7 +276,7 @@ Among other things, you can run the OPTIMIZE query on MergeTree. But it's not re
 
 Q1:
 
-``` sql
+```sql
 SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
 ```
 
@@ -284,7 +284,7 @@ SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
 
 Q2:
 
-``` sql
+```sql
 SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count
 ```
 
@@ -292,7 +292,7 @@ SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenge
 
 Q3:
 
-``` sql
+```sql
 SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year
 ```
 
@@ -300,7 +300,7 @@ SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetr
 
 Q4:
 
-``` sql
+```sql
 SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*)
 FROM trips_mergetree
 GROUP BY passenger_count, year, distance
@@ -319,19 +319,19 @@ Creating a table on three servers:
 
 On each server:
 
-```
+```text
 CREATE TABLE default.trips_mergetree_third ( trip_id UInt32,  vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14),  pickup_date Date,  pickup_datetime DateTime,  dropoff_date Date,  dropoff_datetime DateTime,  store_and_fwd_flag UInt8,  rate_code_id UInt8,  pickup_longitude Float64,  pickup_latitude Float64,  dropoff_longitude Float64,  dropoff_latitude Float64,  passenger_count UInt8,  trip_distance Float64,  fare_amount Float32,  extra Float32,  mta_tax Float32,  tip_amount Float32,  tolls_amount Float32,  ehail_fee Float32,  improvement_surcharge Float32,  total_amount Float32,  payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),  trip_type UInt8,  pickup FixedString(25),  dropoff FixedString(25),  cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),  pickup_nyct2010_gid UInt8,  pickup_ctlabel Float32,  pickup_borocode UInt8,  pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  pickup_ct2010 FixedString(6),  pickup_boroct2010 FixedString(7),  pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  pickup_ntacode FixedString(4),  pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  pickup_puma UInt16,  dropoff_nyct2010_gid UInt8,  dropoff_ctlabel Float32,  dropoff_borocode UInt8,  dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  dropoff_ct2010 FixedString(6),  dropoff_boroct2010 FixedString(7),  dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  dropoff_ntacode FixedString(4),  dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
 ```
 
 On the source server:
 
-``` sql
+```sql
 CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())
 ```
 
 The following query redistributes data:
 
-``` sql
+```sql
 INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree
 ```
 
@@ -364,5 +364,3 @@ We ran queries using a client located in a Yandex datacenter in Finland on a clu
 | 3 | 0.212 | 0.438 | 0.733 | 1.241 |
 | 140 | 0.028 | 0.043 | 0.051 | 0.072 |
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/example_datasets/nyc_taxi/) <!--hide-->
diff --git a/docs/en/getting_started/example_datasets/ontime.md b/docs/en/getting_started/example_datasets/ontime.md
index f319325d6..bceab083d 100644
--- a/docs/en/getting_started/example_datasets/ontime.md
+++ b/docs/en/getting_started/example_datasets/ontime.md
@@ -18,7 +18,7 @@ done
 
 Creating a table:
 
-``` sql
+```sql
 CREATE TABLE `ontime` (
   `Year` UInt16,
   `Quarter` UInt8,
@@ -142,37 +142,37 @@ Queries:
 
 Q0.
 
-``` sql
+```sql
 select avg(c1) from (select Year, Month, count(*) as c1 from ontime group by Year, Month);
 ```
 
 Q1. The number of flights per day from the year 2000 to 2008
 
-``` sql
+```sql
 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE Year >= 2000 AND Year <= 2008 GROUP BY DayOfWeek ORDER BY c DESC;
 ```
 
 Q2. The number of flights delayed by more than 10 minutes, grouped by the day of the week, for 2000-2008
 
-``` sql
+```sql
 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE DepDelay>10 AND Year >= 2000 AND Year <= 2008 GROUP BY DayOfWeek ORDER BY c DESC
 ```
 
 Q3. The number of delays by airport for 2000-2008
 
-``` sql
+```sql
 SELECT Origin, count(*) AS c FROM ontime WHERE DepDelay>10 AND Year >= 2000 AND Year <= 2008 GROUP BY Origin ORDER BY c DESC LIMIT 10
 ```
 
 Q4. The number of delays by carrier for 2007
 
-``` sql
+```sql
 SELECT Carrier, count(*) FROM ontime WHERE DepDelay>10  AND Year = 2007 GROUP BY Carrier ORDER BY count(*) DESC
 ```
 
 Q5. The percentage of delays by carrier for 2007
 
-``` sql
+```sql
 SELECT Carrier, c, c2, c*1000/c2 as c3
 FROM
 (
@@ -198,13 +198,13 @@ ORDER BY c3 DESC;
 
 Better version of the same query:
 
-``` sql
+```sql
 SELECT Carrier, avg(DepDelay > 10) * 1000 AS c3 FROM ontime WHERE Year = 2007 GROUP BY Carrier ORDER BY Carrier
 ```
 
 Q6. The previous request for a broader range of years, 2000-2008
 
-``` sql
+```sql
 SELECT Carrier, c, c2, c*1000/c2 as c3
 FROM
 (
@@ -230,13 +230,13 @@ ORDER BY c3 DESC;
 
 Better version of the same query:
 
-``` sql
+```sql
 SELECT Carrier, avg(DepDelay > 10) * 1000 AS c3 FROM ontime WHERE Year >= 2000 AND Year <= 2008 GROUP BY Carrier ORDER BY Carrier
 ```
 
 Q7. Percentage of flights delayed for more than 10 minutes, by year
 
-``` sql
+```sql
 SELECT Year, c1/c2
 FROM
 (
@@ -260,25 +260,25 @@ ORDER BY Year
 
 Better version of the same query:
 
-``` sql
+```sql
 SELECT Year, avg(DepDelay > 10) FROM ontime GROUP BY Year ORDER BY Year
 ```
 
 Q8. The most popular destinations by the number of directly connected cities for various year ranges
 
-``` sql
+```sql
 SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime WHERE Year >= 2000 and Year <= 2010 GROUP BY DestCityName ORDER BY u DESC LIMIT 10;
 ```
 
 Q9.
 
-``` sql
+```sql
 select Year, count(*) as c1 from ontime group by Year;
 ```
 
 Q10.
 
-``` sql
+```sql
 select
    min(Year), max(Year), Carrier, count(*) as cnt,
    sum(ArrDelayMinutes>30) as flights_delayed,
@@ -296,7 +296,7 @@ LIMIT 1000;
 
 Bonus:
 
-``` sql
+```sql
 SELECT avg(cnt) FROM (SELECT Year,Month,count(*) AS cnt FROM ontime WHERE DepDel15=1 GROUP BY Year,Month)
 
 select avg(c1) from (select Year,Month,count(*) as c1 from ontime group by Year,Month)
@@ -317,5 +317,3 @@ This performance test was created by Vadim Tkachenko. See:
 - <https://www.percona.com/blog/2016/01/07/apache-spark-with-air-ontime-performance-data/>
 - <http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html>
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/example_datasets/ontime/) <!--hide-->
diff --git a/docs/en/getting_started/example_datasets/star_schema.md b/docs/en/getting_started/example_datasets/star_schema.md
index 5606cf71d..6c1585bd6 100644
--- a/docs/en/getting_started/example_datasets/star_schema.md
+++ b/docs/en/getting_started/example_datasets/star_schema.md
@@ -21,7 +21,7 @@ Generating data:
 
 Creating tables in ClickHouse:
 
-``` sql
+```sql
 CREATE TABLE lineorder (
         LO_ORDERKEY             UInt32,
         LO_LINENUMBER           UInt8,
@@ -83,5 +83,3 @@ cat customer.tbl | sed 's/$/2000-01-01/' | clickhouse-client --query "INSERT INT
 cat lineorder.tbl | clickhouse-client --query "INSERT INTO lineorder FORMAT CSV"
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/example_datasets/star_schema/) <!--hide-->
diff --git a/docs/en/getting_started/example_datasets/wikistat.md b/docs/en/getting_started/example_datasets/wikistat.md
index f81d05253..fee0a56b5 100644
--- a/docs/en/getting_started/example_datasets/wikistat.md
+++ b/docs/en/getting_started/example_datasets/wikistat.md
@@ -4,7 +4,7 @@ See: <http://dumps.wikimedia.org/other/pagecounts-raw/>
 
 Creating a table:
 
-``` sql
+```sql
 CREATE TABLE wikistat
 (
     date Date,
@@ -25,5 +25,3 @@ cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pageco
 ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time="$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\.gz/\1-\2-\3 \4-00-00/')" | clickhouse-client --query="INSERT INTO wikistat FORMAT TabSeparated"; done
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/example_datasets/wikistat/) <!--hide-->
diff --git a/docs/en/getting_started/index.md b/docs/en/getting_started/index.md
index 53a8fa495..a4a18e5ab 100644
--- a/docs/en/getting_started/index.md
+++ b/docs/en/getting_started/index.md
@@ -24,7 +24,7 @@ For testing and development, the system can be installed on a single server or o
 
 In `/etc/apt/sources.list` (or in a separate `/etc/apt/sources.list.d/clickhouse.list` file), add the repository:
 
-```
+```text
 deb http://repo.yandex.ru/clickhouse/deb/stable/ main/
 ```
 
@@ -51,14 +51,14 @@ To compile, follow the instructions: build.md
 You can compile packages and install them.
 You can also use programs without installing packages.
 
-```
+```text
 Client: dbms/programs/clickhouse-client
 Server: dbms/programs/clickhouse-server
 ```
 
 For the server, create a catalog with data, such as:
 
-```
+```text
 /opt/clickhouse/data/default/
 /opt/clickhouse/metadata/default/
 ```
@@ -137,5 +137,3 @@ SELECT 1
 
 To continue experimenting, you can try to download from the test data sets.
 
-
-[Original article](https://clickhouse.yandex/docs/en/getting_started/) <!--hide-->
diff --git a/docs/en/index.md b/docs/en/index.md
index 450b14181..9794746e9 100644
--- a/docs/en/index.md
+++ b/docs/en/index.md
@@ -77,8 +77,9 @@ See the difference?
 
 For example, the query "count the number of records for each advertising platform" requires reading one "advertising platform ID" column, which takes up 1 byte uncompressed. If most of the traffic was not from advertising platforms, you can expect at least 10-fold compression of this column. When using a quick compression algorithm, data decompression is possible at a speed of at least several gigabytes of uncompressed data per second. In other words, this query can be processed at a speed of approximately several billion rows per second on a single server. This speed is actually achieved in practice.
 
-<details markdown="1"><summary>Example</summary>
-```
+<details><summary>Example</summary>
+<p>
+<pre>
 $ clickhouse-client
 ClickHouse client version 0.0.52053.
 Connecting to localhost:9000.
@@ -119,10 +120,9 @@ LIMIT 20
 
 20 rows in set. Elapsed: 0.153 sec. Processed 1.00 billion rows, 4.00 GB (6.53 billion rows/s., 26.10 GB/s.)
 
-:)
-```
+:)</pre>
 
-</details>
+</p></details>
 
 ### CPU
 
@@ -138,5 +138,3 @@ There are two ways to do this:
 This is not done in "normal" databases, because it doesn't make sense when running simple queries. However, there are exceptions. For example, MemSQL uses code generation to reduce latency when processing SQL queries. (For comparison, analytical DBMSs require optimization of throughput, not latency.)
 
 Note that for CPU efficiency, the query language must be declarative (SQL or MDX), or at least a vector (J, K). The query should only contain implicit loops, allowing for optimization.
-
-[Original article](https://clickhouse.yandex/docs/en/) <!--hide-->
diff --git a/docs/en/interfaces/cli.md b/docs/en/interfaces/cli.md
index c042a87ee..f3824e395 100644
--- a/docs/en/interfaces/cli.md
+++ b/docs/en/interfaces/cli.md
@@ -113,5 +113,3 @@ Example of a config file:
 </config>
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/cli/) <!--hide-->
diff --git a/docs/en/interfaces/formats.md b/docs/en/interfaces/formats.md
index d26b8f54a..2d08acf9c 100644
--- a/docs/en/interfaces/formats.md
+++ b/docs/en/interfaces/formats.md
@@ -32,131 +32,31 @@ The table below lists supported formats and how they can be used in `INSERT` and
 | [XML](#xml) | ✗ | ✔ |
 | [CapnProto](#capnproto) | ✔ | ✔ |
 
-<a name="tabseparated"></a>
-
-## TabSeparated
-
-In TabSeparated format, data is written by row. Each row contains values separated by tabs. Each value is follow by a tab, except the last value in the row, which is followed by a line feed. Strictly Unix line feeds are assumed everywhere. The last row also must contain a line feed at the end. Values are written in text format, without enclosing quotation marks, and with special characters escaped.
-
-This format is also available under the name `TSV`.
-
-The `TabSeparated` format is convenient for processing data using custom programs and scripts. It is used by default in the HTTP interface, and in the command-line client's batch mode. This format also allows transferring data between different DBMSs. For example, you can get a dump from MySQL and upload it to ClickHouse, or vice versa.
-
-The `TabSeparated` format supports outputting total values (when using WITH TOTALS) and extreme values (when 'extremes' is set to 1). In these cases, the total values and extremes are output after the main data. The main result, total values, and extremes are separated from each other by an empty line. Example:
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
-```
-
-```
-2014-03-17      1406958
-2014-03-18      1383658
-2014-03-19      1405797
-2014-03-20      1353623
-2014-03-21      1245779
-2014-03-22      1031592
-2014-03-23      1046491
-
-0000-00-00      8873898
-
-2014-03-17      1031592
-2014-03-23      1406958
-```
-
-### Data formatting
-
-Integer numbers are written in decimal form. Numbers can contain an extra "+" character at the beginning (ignored when parsing, and not recorded when formatting). Non-negative numbers can't contain the negative sign. When reading, it is allowed to parse an empty string as a zero, or (for signed types) a string consisting of just a minus sign as a zero. Numbers that do not fit into the corresponding data type may be parsed as a different number, without an error message.
-
-Floating-point numbers are written in decimal form. The dot is used as the decimal separator. Exponential entries are supported, as are 'inf', '+inf', '-inf', and 'nan'. An entry of floating-point numbers may begin or end with a decimal point.
-During formatting, accuracy may be lost on floating-point numbers.
-During parsing, it is not strictly required to read the nearest machine-representable number.
-
-Dates are written in YYYY-MM-DD format and parsed in the same format, but with any characters as separators.
-Dates with times are written in the format YYYY-MM-DD hh:mm:ss and parsed in the same format, but with any characters as separators.
-This all occurs in the system time zone at the time the client or server starts (depending on which one formats data). For dates with times, daylight saving time is not specified. So if a dump has times during daylight saving time, the dump does not unequivocally match the data, and parsing will select one of the two times.
-During a read operation, incorrect dates and dates with times can be parsed with natural overflow or as null dates and times, without an error message.
-
-As an exception, parsing dates with times is also supported in Unix timestamp format, if it consists of exactly 10 decimal digits. The result is not time zone-dependent. The formats YYYY-MM-DD hh:mm:ss and NNNNNNNNNN are differentiated automatically.
-
-Strings are output with backslash-escaped special characters. The following escape sequences are used for output: `\b`, `\f`, `\r`, `\n`, `\t`, `\0`, `\'`, `\\`. Parsing also supports the sequences `\a`, `\v`, and `\xHH`  (hex escape sequences) and any `\c` sequences, where `c` is any character (these sequences are converted to `c`). Thus, reading data supports formats where a line feed can be written as `\n`  or `\`, or as a line feed. For example, the string `Hello world` with a line feed between the words instead of a space can be parsed in any of the following variations:
-
-```
-Hello\nworld
-
-Hello\
-world
-```
-
-The second variant is supported because MySQL uses it when writing tab-separated dumps.
-
-The minimum set of characters that you need to escape when passing data in TabSeparated format: tab, line feed (LF) and backslash.
-
-Only a small set of symbols are escaped. You can easily stumble onto a string value that your terminal will ruin in output.
-
-Arrays are written as a list of comma-separated values in square brackets. Number items in the array are fomratted as normally, but dates, dates with times, and strings are written in single quotes with the same escaping rules as above.
-
-[NULL](../query_language/syntax.md#null-literal) is formatted as `\N`.
-
-<a name="tabseparatedraw"></a>
-
-## TabSeparatedRaw
-
-Differs from `TabSeparated` format in that the rows are written without escaping.
-This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table).
-
-This format is also available under the name `TSVRaw`.
-<a name="tabseparatedwithnames"></a>
-
-## TabSeparatedWithNames
-
-Differs from the `TabSeparated` format in that the column names are written in the first row.
-During parsing, the first row is completely ignored. You can't use column names to determine their position or to check their correctness.
-(Support for parsing the header row may be added in the future.)
-
-This format is also available under the name `TSVWithNames`.
-<a name="tabseparatedwithnamesandtypes"></a>
-
-## TabSeparatedWithNamesAndTypes
-
-Differs from the `TabSeparated` format in that the column names are written to the first row, while the column types are in the second row.
-During parsing, the first and second rows are completely ignored.
+<a name="format_capnproto"></a>
 
-This format is also available under the name `TSVWithNamesAndTypes`.
-<a name="tskv"></a>
+## CapnProto
 
-## TSKV
+Cap'n Proto is a binary message format similar to Protocol Buffers and Thrift, but not like JSON or MessagePack.
 
-Similar to TabSeparated, but outputs a value in name=value format. Names are escaped the same way as in TabSeparated format, and the = symbol is also escaped.
+Cap'n Proto messages are strictly typed and not self-describing, meaning they need an external schema description. The schema is applied on the fly and cached for each query.
 
-```
-SearchPhrase=   count()=8267016
-SearchPhrase=bathroom interior design    count()=2166
-SearchPhrase=yandex     count()=1655
-SearchPhrase=2014 spring fashion    count()=1549
-SearchPhrase=freeform photos       count()=1480
-SearchPhrase=angelina jolie    count()=1245
-SearchPhrase=omsk       count()=1112
-SearchPhrase=photos of dog breeds    count()=1091
-SearchPhrase=curtain designs        count()=1064
-SearchPhrase=baku       count()=1000
+```sql
+SELECT SearchPhrase, count() AS c FROM test.hits
+       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
 ```
 
-[NULL](../query_language/syntax.md#null-literal) is formatted as `\N`.
-
-``` sql
-SELECT * FROM t_null FORMAT TSKV
-```
+Where `schema.capnp` looks like this:
 
 ```
-x=1	y=\N
+struct Message {
+  SearchPhrase @0 :Text;
+  c @1 :Uint64;
+}
 ```
 
-When there is a large number of small columns, this format is ineffective, and there is generally no reason to use it. It is used in some departments of Yandex.
-
-Both data output and parsing are supported in this format. For parsing, any order is supported for the values of different columns. It is acceptable for some values to be omitted – they are treated as equal to their default values. In this case, zeros and blank rows are used as default values. Complex values that could be specified in the table are not supported as defaults.
-
-Parsing allows the presence of the additional field `tskv` without the equal sign or a value. This field is ignored.
+Schema files are in the file that is located in the directory specified in [ format_schema_path](../operations/server_settings/settings.md#server_settings-format_schema_path) in the server configuration.
 
+Deserialization is effective and usually doesn't increase the system load.
 <a name="csv"></a>
 
 ## CSV
@@ -186,7 +86,7 @@ Also prints the header row, similar to `TabSeparatedWithNames`.
 
 Outputs data in JSON format. Besides data tables, it also outputs column names and types, along with some additional information: the total number of output rows, and the number of rows that could have been output if there weren't a LIMIT. Example:
 
-``` sql
+```sql
 SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON
 ```
 
@@ -363,7 +263,7 @@ Each result block is output as a separate table. This is necessary so that block
 
 [NULL](../query_language/syntax.md#null-literal) is output as `ᴺᵁᴸᴸ`.
 
-``` sql
+```sql
 SELECT * FROM t_null
 ```
 
@@ -378,11 +278,11 @@ This format is only appropriate for outputting a query result, but not for parsi
 
 The Pretty format supports outputting total values (when using WITH TOTALS) and extremes (when 'extremes' is set to 1). In these cases, total values and extreme values are output after the main data, in separate tables. Example (shown for the PrettyCompact format):
 
-``` sql
+```sql
 SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact
 ```
 
-```
+```text
 ┌──EventDate─┬───────c─┐
 │ 2014-03-17 │ 1406958 │
 │ 2014-03-18 │ 1383658 │
@@ -459,6 +359,131 @@ Array is represented as a varint length (unsigned [LEB128](https://en.wikipedia.
 
 For [NULL](../query_language/syntax.md#null-literal) support, an additional byte containing 1 or 0 is added before each [Nullable](../data_types/nullable.md#data_type-nullable) value. If 1, then the value is `NULL` and this byte is interpreted as a separate value. If 0, the value after the byte is not `NULL`.
 
+<a name="tabseparated"></a>
+
+## TabSeparated
+
+In TabSeparated format, data is written by row. Each row contains values separated by tabs. Each value is follow by a tab, except the last value in the row, which is followed by a line feed. Strictly Unix line feeds are assumed everywhere. The last row also must contain a line feed at the end. Values are written in text format, without enclosing quotation marks, and with special characters escaped.
+
+This format is also available under the name `TSV`.
+
+The `TabSeparated` format is convenient for processing data using custom programs and scripts. It is used by default in the HTTP interface, and in the command-line client's batch mode. This format also allows transferring data between different DBMSs. For example, you can get a dump from MySQL and upload it to ClickHouse, or vice versa.
+
+The `TabSeparated` format supports outputting total values (when using WITH TOTALS) and extreme values (when 'extremes' is set to 1). In these cases, the total values and extremes are output after the main data. The main result, total values, and extremes are separated from each other by an empty line. Example:
+
+```sql
+SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
+```
+
+```text
+2014-03-17      1406958
+2014-03-18      1383658
+2014-03-19      1405797
+2014-03-20      1353623
+2014-03-21      1245779
+2014-03-22      1031592
+2014-03-23      1046491
+
+0000-00-00      8873898
+
+2014-03-17      1031592
+2014-03-23      1406958
+```
+
+## Data formatting
+
+Integer numbers are written in decimal form. Numbers can contain an extra "+" character at the beginning (ignored when parsing, and not recorded when formatting). Non-negative numbers can't contain the negative sign. When reading, it is allowed to parse an empty string as a zero, or (for signed types) a string consisting of just a minus sign as a zero. Numbers that do not fit into the corresponding data type may be parsed as a different number, without an error message.
+
+Floating-point numbers are written in decimal form. The dot is used as the decimal separator. Exponential entries are supported, as are 'inf', '+inf', '-inf', and 'nan'. An entry of floating-point numbers may begin or end with a decimal point.
+During formatting, accuracy may be lost on floating-point numbers.
+During parsing, it is not strictly required to read the nearest machine-representable number.
+
+Dates are written in YYYY-MM-DD format and parsed in the same format, but with any characters as separators.
+Dates with times are written in the format YYYY-MM-DD hh:mm:ss and parsed in the same format, but with any characters as separators.
+This all occurs in the system time zone at the time the client or server starts (depending on which one formats data). For dates with times, daylight saving time is not specified. So if a dump has times during daylight saving time, the dump does not unequivocally match the data, and parsing will select one of the two times.
+During a read operation, incorrect dates and dates with times can be parsed with natural overflow or as null dates and times, without an error message.
+
+As an exception, parsing dates with times is also supported in Unix timestamp format, if it consists of exactly 10 decimal digits. The result is not time zone-dependent. The formats YYYY-MM-DD hh:mm:ss and NNNNNNNNNN are differentiated automatically.
+
+Strings are output with backslash-escaped special characters. The following escape sequences are used for output: `\b`, `\f`, `\r`, `\n`, `\t`, `\0`, `\'`, `\\`. Parsing also supports the sequences `\a`, `\v`, and `\xHH`  (hex escape sequences) and any `\c` sequences, where `c` is any character (these sequences are converted to `c`). Thus, reading data supports formats where a line feed can be written as `\n`  or `\`, or as a line feed. For example, the string `Hello world` with a line feed between the words instead of a space can be parsed in any of the following variations:
+
+```text
+Hello\nworld
+
+Hello\
+world
+```
+
+The second variant is supported because MySQL uses it when writing tab-separated dumps.
+
+The minimum set of characters that you need to escape when passing data in TabSeparated format: tab, line feed (LF) and backslash.
+
+Only a small set of symbols are escaped. You can easily stumble onto a string value that your terminal will ruin in output.
+
+Arrays are written as a list of comma-separated values in square brackets. Number items in the array are fomratted as normally, but dates, dates with times, and strings are written in single quotes with the same escaping rules as above.
+
+[NULL](../query_language/syntax.md#null-literal) is formatted as `\N`.
+
+<a name="tabseparatedraw"></a>
+
+## TabSeparatedRaw
+
+Differs from `TabSeparated` format in that the rows are written without escaping.
+This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table).
+
+This format is also available under the name `TSVRaw`.
+<a name="tabseparatedwithnames"></a>
+
+## TabSeparatedWithNames
+
+Differs from the `TabSeparated` format in that the column names are written in the first row.
+During parsing, the first row is completely ignored. You can't use column names to determine their position or to check their correctness.
+(Support for parsing the header row may be added in the future.)
+
+This format is also available under the name `TSVWithNames`.
+<a name="tabseparatedwithnamesandtypes"></a>
+
+## TabSeparatedWithNamesAndTypes
+
+Differs from the `TabSeparated` format in that the column names are written to the first row, while the column types are in the second row.
+During parsing, the first and second rows are completely ignored.
+
+This format is also available under the name `TSVWithNamesAndTypes`.
+<a name="tskv"></a>
+
+## TSKV
+
+Similar to TabSeparated, but outputs a value in name=value format. Names are escaped the same way as in TabSeparated format, and the = symbol is also escaped.
+
+```text
+SearchPhrase=   count()=8267016
+SearchPhrase=bathroom interior design    count()=2166
+SearchPhrase=yandex     count()=1655
+SearchPhrase=2014 spring fashion    count()=1549
+SearchPhrase=freeform photos       count()=1480
+SearchPhrase=angelina jolie    count()=1245
+SearchPhrase=omsk       count()=1112
+SearchPhrase=photos of dog breeds    count()=1091
+SearchPhrase=curtain designs        count()=1064
+SearchPhrase=baku       count()=1000
+```
+
+[NULL](../query_language/syntax.md#null-literal) is formatted as `\N`.
+
+```sql
+SELECT * FROM t_null FORMAT TSKV
+```
+
+```
+x=1	y=\N
+```
+
+When there is a large number of small columns, this format is ineffective, and there is generally no reason to use it. It is used in some departments of Yandex.
+
+Both data output and parsing are supported in this format. For parsing, any order is supported for the values of different columns. It is acceptable for some values to be omitted – they are treated as equal to their default values. In this case, zeros and blank rows are used as default values. Complex values that could be specified in the table are not supported as defaults.
+
+Parsing allows the presence of the additional field `tskv` without the equal sign or a value. This field is ignored.
+
 ## Values
 
 Prints every row in brackets. Rows are separated by commas. There is no comma after the last row. The values inside the brackets are also comma-separated. Numbers are output in decimal format without quotes. Arrays are output in square brackets. Strings, dates, and dates with times are output in quotes. Escaping rules and parsing are similar to the [TabSeparated](#tabseparated) format. During formatting, extra spaces aren't inserted, but during parsing, they are allowed and skipped (except for spaces inside array values, which are not allowed). [NULL](../query_language/syntax.md#null-literal) is represented as `NULL`.
@@ -477,7 +502,7 @@ Prints each value on a separate line with the column name specified. This format
 
 Example:
 
-``` sql
+```sql
 SELECT * FROM t_null FORMAT Vertical
 ```
 
@@ -595,31 +620,3 @@ Just as for JSON, invalid UTF-8 sequences are changed to the replacement charact
 In string values, the characters `<` and `&` are escaped as `<` and `&`.
 
 Arrays are output as `<array><elem>Hello</elem><elem>World</elem>...</array>`,and tuples as `<tuple><elem>Hello</elem><elem>World</elem>...</tuple>`.
-
-<a name="format_capnproto"></a>
-
-## CapnProto
-
-Cap'n Proto is a binary message format similar to Protocol Buffers and Thrift, but not like JSON or MessagePack.
-
-Cap'n Proto messages are strictly typed and not self-describing, meaning they need an external schema description. The schema is applied on the fly and cached for each query.
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits
-       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
-```
-
-Where `schema.capnp` looks like this:
-
-```
-struct Message {
-  SearchPhrase @0 :Text;
-  c @1 :Uint64;
-}
-```
-
-Schema files are in the file that is located in the directory specified in [ format_schema_path](../operations/server_settings/settings.md#server_settings-format_schema_path) in the server configuration.
-
-Deserialization is effective and usually doesn't increase the system load.
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/formats/) <!--hide-->
diff --git a/docs/en/interfaces/http_interface.md b/docs/en/interfaces/http_interface.md
index e514f0c49..4b20f2a0b 100644
--- a/docs/en/interfaces/http_interface.md
+++ b/docs/en/interfaces/http_interface.md
@@ -218,5 +218,3 @@ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&buffer_size=3000000&wa
 
 Use buffering to avoid situations where a query processing error occurred after the response code and HTTP headers were sent to the client. In this situation, an error message is written at the end of the response body, and on the client side, the error can only be detected at the parsing stage.
 
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/http_interface/) <!--hide-->
diff --git a/docs/en/interfaces/index.md b/docs/en/interfaces/index.md
index 9f445d452..e43f44742 100644
--- a/docs/en/interfaces/index.md
+++ b/docs/en/interfaces/index.md
@@ -4,5 +4,3 @@
 
 To explore the system's capabilities, download data to tables, or make manual queries, use the clickhouse-client program.
 
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/) <!--hide-->
diff --git a/docs/en/interfaces/jdbc.md b/docs/en/interfaces/jdbc.md
index 8454881df..a8808770f 100644
--- a/docs/en/interfaces/jdbc.md
+++ b/docs/en/interfaces/jdbc.md
@@ -3,5 +3,3 @@
 - [Official driver](https://github.com/yandex/clickhouse-jdbc).
 - Third-party driver from [ClickHouse-Native-JDBC](https://github.com/housepower/ClickHouse-Native-JDBC).
 
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/jdbc/) <!--hide-->
diff --git a/docs/en/interfaces/tcp.md b/docs/en/interfaces/tcp.md
index 86ef118e8..c9813823b 100644
--- a/docs/en/interfaces/tcp.md
+++ b/docs/en/interfaces/tcp.md
@@ -2,5 +2,3 @@
 
 The native interface is used in the "clickhouse-client" command-line client for interaction between servers with distributed query processing, and also in C++ programs. We will only cover the command-line client.
 
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/tcp/) <!--hide-->
diff --git a/docs/en/interfaces/third-party_client_libraries.md b/docs/en/interfaces/third-party_client_libraries.md
index 3ae40e829..a46d9efc6 100644
--- a/docs/en/interfaces/third-party_client_libraries.md
+++ b/docs/en/interfaces/third-party_client_libraries.md
@@ -7,7 +7,6 @@ We have not tested the libraries listed below.
     - [sqlalchemy-clickhouse](https://github.com/cloudflare/sqlalchemy-clickhouse)
     - [clickhouse-driver](https://github.com/mymarilyn/clickhouse-driver)
     - [clickhouse-client](https://github.com/yurial/clickhouse-client)
-    - [aiochclient](https://github.com/maximdanilchenko/aiochclient)
 - PHP
     - [phpClickHouse](https://github.com/smi2/phpClickHouse)
     - [clickhouse-php-client](https://github.com/8bitov/clickhouse-php-client)
@@ -43,10 +42,6 @@ We have not tested the libraries listed below.
     - [clickhouse_ecto](https://github.com/appodeal/clickhouse_ecto)
 - Java
     - [clickhouse-client-java](https://github.com/VirtusAI/clickhouse-client-java)
-- Kotlin
-    - [AORM](https://github.com/TanVD/AORM)
 - Nim
     - [nim-clickhouse](https://github.com/leonardoce/nim-clickhouse)
 
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/third-party_client_libraries/) <!--hide-->
diff --git a/docs/en/interfaces/third-party_gui.md b/docs/en/interfaces/third-party_gui.md
index 280b0bee2..eee127f4f 100644
--- a/docs/en/interfaces/third-party_gui.md
+++ b/docs/en/interfaces/third-party_gui.md
@@ -36,14 +36,3 @@ The following features are planned for development:
 - Cluster management.
 - Monitoring replicated and Kafka tables.
 
-## DBeaver
-
-[DBeaver](https://dbeaver.io/) - universal desktop database client with ClickHouse support.
-
-Key features:
-
-- Query development with syntax highlight.
-- Table preview.
-- Autocompletion.
-
-[Original article](https://clickhouse.yandex/docs/en/interfaces/third-party_gui/) <!--hide-->
diff --git a/docs/en/introduction/distinctive_features.md b/docs/en/introduction/distinctive_features.md
index 3354a75c5..eff79191b 100644
--- a/docs/en/introduction/distinctive_features.md
+++ b/docs/en/introduction/distinctive_features.md
@@ -60,5 +60,3 @@ ClickHouse provides various ways to trade accuracy for performance:
 Uses asynchronous multimaster replication. After being written to any available replica, data is distributed to all the remaining replicas in the background. The system maintains identical data on different replicas. Recovery after most failures is performed automatically, and in complex cases — semi-automatically.
 
 For more information, see the section [Data replication](../operations/table_engines/replication.md#table_engines-replication).
-
-[Original article](https://clickhouse.yandex/docs/en/introduction/distinctive_features/) <!--hide-->
diff --git a/docs/en/introduction/features_considered_disadvantages.md b/docs/en/introduction/features_considered_disadvantages.md
index 5071c9b9b..55fecaf12 100644
--- a/docs/en/introduction/features_considered_disadvantages.md
+++ b/docs/en/introduction/features_considered_disadvantages.md
@@ -3,5 +3,3 @@
 1. No full-fledged transactions.
 2. Lack of ability to modify or delete already inserted data with high rate and low latency. There are batch deletes and updates available to clean up or modify data, for example to comply with [GDPR](https://gdpr-info.eu).
 3. The sparse index makes ClickHouse not really suitable for point queries retrieving single rows by their keys.
-
-[Original article](https://clickhouse.yandex/docs/en/introduction/features_considered_disadvantages/) <!--hide-->
diff --git a/docs/en/introduction/performance.md b/docs/en/introduction/performance.md
index 47d4a3b94..d9796d263 100644
--- a/docs/en/introduction/performance.md
+++ b/docs/en/introduction/performance.md
@@ -21,5 +21,3 @@ Under the same conditions, ClickHouse can handle several hundred queries per sec
 ## Performance When Inserting Data
 
 We recommend inserting data in packets of at least 1000 rows, or no more than a single request per second. When inserting to a MergeTree table from a tab-separated dump, the insertion speed will be from 50 to 200 MB/s. If the inserted rows are around 1 Kb in size, the speed will be from 50,000 to 200,000 rows per second. If the rows are small, the performance will be higher in rows per second (on Banner System data -`>` 500,000 rows per second; on Graphite data -`>` 1,000,000 rows per second). To improve performance, you can make multiple INSERT queries in parallel, and performance will increase linearly.
-
-[Original article](https://clickhouse.yandex/docs/en/introduction/performance/) <!--hide-->
diff --git a/docs/en/introduction/ya_metrika_task.md b/docs/en/introduction/ya_metrika_task.md
index 9acbdc098..db173e178 100644
--- a/docs/en/introduction/ya_metrika_task.md
+++ b/docs/en/introduction/ya_metrika_task.md
@@ -46,5 +46,3 @@ OLAPServer worked well for non-aggregated data, but it had many restrictions tha
 
 To remove the limitations of OLAPServer and solve the problem of working with non-aggregated data for all reports, we developed the ClickHouse DBMS.
 
-
-[Original article](https://clickhouse.yandex/docs/en/introduction/ya_metrika_task/) <!--hide-->
diff --git a/docs/en/operations/access_rights.md b/docs/en/operations/access_rights.md
index 451be2c73..3064da751 100644
--- a/docs/en/operations/access_rights.md
+++ b/docs/en/operations/access_rights.md
@@ -99,5 +99,3 @@ The user can get a list of all databases and tables in them by using `SHOW` quer
 
 Database access is not related to the [readonly](settings/query_complexity.md#query_complexity_readonly) setting. You can't grant full access to one database and `readonly` access to another one.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/access_rights/) <!--hide-->
diff --git a/docs/en/operations/configuration_files.md b/docs/en/operations/configuration_files.md
index a7cdfb124..d55cf7c20 100644
--- a/docs/en/operations/configuration_files.md
+++ b/docs/en/operations/configuration_files.md
@@ -40,5 +40,3 @@ $ cat /etc/clickhouse-server/users.d/alice.xml
 For each config file, the server also generates `file-preprocessed.xml` files when starting. These files contain all the completed substitutions and overrides, and they are intended for informational use. If ZooKeeper substitutions were used in the config files but ZooKeeper is not available on the server start, the server loads the configuration from the preprocessed file.
 
 The server tracks changes in config files, as well as files and ZooKeeper nodes that were used when performing substitutions and overrides, and reloads the settings for users and clusters on the fly. This means that you can modify the cluster, users, and their settings without restarting the server.
-
-[Original article](https://clickhouse.yandex/docs/en/operations/configuration_files/) <!--hide-->
diff --git a/docs/en/operations/index.md b/docs/en/operations/index.md
index 63cb19bb6..9073e2a2a 100644
--- a/docs/en/operations/index.md
+++ b/docs/en/operations/index.md
@@ -1,4 +1,2 @@
 # Operations 
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/) <!--hide-->
diff --git a/docs/en/operations/quotas.md b/docs/en/operations/quotas.md
index e8cde1308..e8b603ff1 100644
--- a/docs/en/operations/quotas.md
+++ b/docs/en/operations/quotas.md
@@ -104,5 +104,3 @@ For distributed query processing, the accumulated amounts are stored on the requ
 
 When the server is restarted, quotas are reset.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/quotas/) <!--hide-->
diff --git a/docs/en/operations/server_settings/index.md b/docs/en/operations/server_settings/index.md
index 88f11c48f..5631d131a 100644
--- a/docs/en/operations/server_settings/index.md
+++ b/docs/en/operations/server_settings/index.md
@@ -10,5 +10,3 @@ Other settings are described in the "[Settings](../settings/index.md#settings)"
 
 Before studying the settings, read the [Configuration files](../configuration_files.md#configuration_files) section and note the use of substitutions (the `incl` and `optional` attributes).
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/server_settings/) <!--hide-->
diff --git a/docs/en/operations/server_settings/settings.md b/docs/en/operations/server_settings/settings.md
index 8e10969ed..7f52f9bb8 100644
--- a/docs/en/operations/server_settings/settings.md
+++ b/docs/en/operations/server_settings/settings.md
@@ -717,5 +717,3 @@ For more information, see the section "[Replication](../../operations/table_engi
 <zookeeper incl="zookeeper-servers" optional="true" />
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/server_settings/settings/) <!--hide-->
diff --git a/docs/en/operations/settings/index.md b/docs/en/operations/settings/index.md
index 5676796fd..1c0334067 100644
--- a/docs/en/operations/settings/index.md
+++ b/docs/en/operations/settings/index.md
@@ -22,5 +22,3 @@ Similarly, you can use ClickHouse sessions in the HTTP protocol. To do this, you
 
 Settings that can only be made in the server config file are not covered in this section.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/settings/) <!--hide-->
diff --git a/docs/en/operations/settings/query_complexity.md b/docs/en/operations/settings/query_complexity.md
index 9e49dc58c..2132557d6 100644
--- a/docs/en/operations/settings/query_complexity.md
+++ b/docs/en/operations/settings/query_complexity.md
@@ -193,5 +193,3 @@ Maximum number of bytes (uncompressed data) that can be passed to a remote serve
 ## transfer_overflow_mode
 
 What to do when the amount of data exceeds one of the limits: 'throw' or 'break'. By default, throw.
-
-[Original article](https://clickhouse.yandex/docs/en/operations/settings/query_complexity/) <!--hide-->
diff --git a/docs/en/operations/settings/settings.md b/docs/en/operations/settings/settings.md
index 8afea8e5d..67673cab0 100644
--- a/docs/en/operations/settings/settings.md
+++ b/docs/en/operations/settings/settings.md
@@ -417,5 +417,3 @@ See also the following parameters:
 - [insert_quorum](#setting-insert_quorum)
 - [insert_quorum_timeout](#setting-insert_quorum_timeout)
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/settings/settings/) <!--hide-->
diff --git a/docs/en/operations/settings/settings_profiles.md b/docs/en/operations/settings/settings_profiles.md
index 338800fbe..3e3175bc9 100644
--- a/docs/en/operations/settings/settings_profiles.md
+++ b/docs/en/operations/settings/settings_profiles.md
@@ -9,7 +9,7 @@ Example:
 
 Install the `web` profile.
 
-``` sql
+```sql
 SET profile = 'web'
 ```
 
@@ -63,5 +63,3 @@ The example specifies two profiles: `default`  and `web`. The `default` profile
 
 Settings profiles can inherit from each other. To use inheritance, indicate the `profile`  setting before the other settings that are listed in the profile.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/settings/settings_profiles/) <!--hide-->
diff --git a/docs/en/operations/system_tables.md b/docs/en/operations/system_tables.md
index a67fec3f9..abd6e8193 100644
--- a/docs/en/operations/system_tables.md
+++ b/docs/en/operations/system_tables.md
@@ -18,7 +18,7 @@ Example: The number of SELECT queries currently running; the amount of memory in
 Contains information about clusters available in the config file and the servers in them.
 Columns:
 
-```
+```text
 cluster String      — The cluster name.
 shard_num UInt32 — The shard number in the cluster, starting from 1.
 shard_weight UInt32 — The relative weight of the shard when writing data.
@@ -34,7 +34,7 @@ user String — The name of the user for connecting to the server.
 Contains information about the columns in all tables.
 You can use this table to get information similar to `DESCRIBE TABLE`, but for multiple tables at once.
 
-```
+```text
 database String — The name of the database the table is in.
 table String – Table name.
 name String — Column name.
@@ -183,7 +183,7 @@ Formats:
 This system table is used for implementing the `SHOW PROCESSLIST` query.
 Columns:
 
-```
+```text
 user String              – Name of the user who made the request. For distributed query processing, this is the user who helped the requestor server send the query to this server, not the user who made the distributed request on the requestor server.
 
 address String           - The IP address the request was made from. The same for distributed processing.
@@ -210,14 +210,14 @@ This table can be used for monitoring. The table contains a row for every Replic
 
 Example:
 
-``` sql
+```sql
 SELECT *
 FROM system.replicas
 WHERE table = 'visits'
 FORMAT Vertical
 ```
 
-```
+```text
 Row 1:
 ──────
 database:           merge
@@ -243,7 +243,7 @@ active_replicas:    2
 
 Columns:
 
-```
+```text
 database:          Database name
 table:              Table name
 engine:            Table engine name
@@ -296,7 +296,7 @@ If you don't request the last 4 columns (log_max_index, log_pointer, total_repli
 
 For example, you can check that everything is working correctly like this:
 
-``` sql
+```sql
 SELECT
     database,
     table,
@@ -335,7 +335,7 @@ I.e. used for executing the query you are using to read from the system.settings
 
 Columns:
 
-```
+```text
 name String  — Setting name.
 value String  — Setting value.
 changed UInt8 — Whether the setting was explicitly defined in the config or explicitly changed.
@@ -343,13 +343,13 @@ changed UInt8 — Whether the setting was explicitly defined in the config or ex
 
 Example:
 
-``` sql
+```sql
 SELECT *
 FROM system.settings
 WHERE changed
 ```
 
-```
+```text
 ┌─name───────────────────┬─value───────┬─changed─┐
 │ max_threads            │ 8           │       1 │
 │ use_uncompressed_cache │ 0           │       1 │
@@ -393,14 +393,14 @@ Columns:
 
 Example:
 
-``` sql
+```sql
 SELECT *
 FROM system.zookeeper
 WHERE path = '/clickhouse/tables/01-08/visits/replicas'
 FORMAT Vertical
 ```
 
-```
+```text
 Row 1:
 ──────
 name:           example01-08-1.yandex.ru
@@ -435,5 +435,3 @@ numChildren:    7
 pzxid:          987021252247
 path:           /clickhouse/tables/01-08/visits/replicas
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/operations/system_tables/) <!--hide-->
diff --git a/docs/en/operations/table_engines/aggregatingmergetree.md b/docs/en/operations/table_engines/aggregatingmergetree.md
index 44bc03a8b..0f99da076 100644
--- a/docs/en/operations/table_engines/aggregatingmergetree.md
+++ b/docs/en/operations/table_engines/aggregatingmergetree.md
@@ -8,7 +8,7 @@ There is an `AggregateFunction` data type. It is a parametric data type. As para
 
 Examples:
 
-``` sql
+```sql
 CREATE TABLE t
 (
     column1 AggregateFunction(uniq, UInt64),
@@ -33,7 +33,7 @@ Example: `uniqMerge(UserIDState)`, where `UserIDState` has the `AggregateFunctio
 In other words, an aggregate function with the 'Merge' suffix takes a set of states, combines them, and returns the result.
 As an example, these two queries return the same result:
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM table
 
 SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP BY RegionID)
@@ -51,7 +51,7 @@ Example:
 
 Create an `AggregatingMergeTree` materialized view that watches the `test.visits` table:
 
-``` sql
+```sql
 CREATE MATERIALIZED VIEW test.basic
 ENGINE = AggregatingMergeTree(StartDate, (CounterID, StartDate), 8192)
 AS SELECT
@@ -65,13 +65,13 @@ GROUP BY CounterID, StartDate;
 
 Insert data in the `test.visits` table. Data will also be inserted in the view, where it will be aggregated:
 
-``` sql
+```sql
 INSERT INTO test.visits ...
 ```
 
 Perform `SELECT` from the view using `GROUP BY` in order to complete data aggregation:
 
-``` sql
+```sql
 SELECT
     StartDate,
     sumMerge(Visits) AS Visits,
@@ -85,5 +85,3 @@ You can create a materialized view like this and assign a normal view to it that
 
 Note that in most cases, using `AggregatingMergeTree` is not justified, since queries can be run efficiently enough on non-aggregated data.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/aggregatingmergetree/) <!--hide-->
diff --git a/docs/en/operations/table_engines/buffer.md b/docs/en/operations/table_engines/buffer.md
index 24a990bb2..2098c5f19 100644
--- a/docs/en/operations/table_engines/buffer.md
+++ b/docs/en/operations/table_engines/buffer.md
@@ -2,7 +2,7 @@
 
 Buffers the data to write in RAM, periodically flushing it to another table. During the read operation, data is read from the buffer and the other table simultaneously.
 
-```
+```text
 Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes)
 ```
 
@@ -16,7 +16,7 @@ The conditions for flushing the data are calculated separately for each of the '
 
 Example:
 
-``` sql
+```sql
 CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000)
 ```
 
@@ -52,5 +52,3 @@ A Buffer table is used when too many INSERTs are received from a large number of
 
 Note that it doesn't make sense to insert data one row at a time, even for Buffer tables. This will only produce a speed of a few thousand rows per second, while inserting larger blocks of data can produce over a million rows per second (see the section "Performance").
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/buffer/) <!--hide-->
diff --git a/docs/en/operations/table_engines/collapsingmergetree.md b/docs/en/operations/table_engines/collapsingmergetree.md
index 193984c92..c0d0f33d3 100644
--- a/docs/en/operations/table_engines/collapsingmergetree.md
+++ b/docs/en/operations/table_engines/collapsingmergetree.md
@@ -14,7 +14,7 @@ This is the main concept that allows Yandex.Metrica to work in real time.
 
 CollapsingMergeTree accepts an additional parameter - the name of an Int8-type column that contains the row's "sign". Example:
 
-``` sql
+```sql
 CollapsingMergeTree(EventDate, (CounterID, EventDate, intHash32(UniqID), VisitID), 8192, Sign)
 ```
 
@@ -36,5 +36,3 @@ There are several ways to get completely "collapsed" data from a `CollapsingMerg
 1. Write a query with GROUP BY and aggregate functions that accounts for the sign. For example, to calculate quantity, write 'sum(Sign)' instead of 'count()'. To calculate the sum of something, write 'sum(Sign * x)' instead of 'sum(x)', and so on, and also add 'HAVING sum(Sign) `>` 0'. Not all amounts can be calculated this way. For example, the aggregate functions 'min' and 'max' can't be rewritten.
 2. If you must extract data without aggregation (for example, to check whether rows are present whose newest values match certain conditions), you can use the FINAL modifier for the FROM clause. This approach is significantly less efficient.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/collapsingmergetree/) <!--hide-->
diff --git a/docs/en/operations/table_engines/custom_partitioning_key.md b/docs/en/operations/table_engines/custom_partitioning_key.md
index 55940db8c..bcfe8c8c4 100644
--- a/docs/en/operations/table_engines/custom_partitioning_key.md
+++ b/docs/en/operations/table_engines/custom_partitioning_key.md
@@ -12,7 +12,7 @@ ENGINE [=] Name(...) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTI
 
 For MergeTree tables, the partition expression is specified after `PARTITION BY`, the primary key after `ORDER BY`, the sampling key after `SAMPLE BY`, and `SETTINGS` can specify `index_granularity` (optional; the default value is 8192), as well as other settings from [MergeTreeSettings.h](https://github.com/yandex/ClickHouse/blob/master/dbms/src/Storages/MergeTree/MergeTreeSettings.h). The other engine parameters are specified in parentheses after the engine name, as previously. Example:
 
-``` sql
+```sql
 ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)
     PARTITION BY (toMonday(StartDate), EventType)
     ORDER BY (CounterID, StartDate, intHash32(UserID))
@@ -27,7 +27,7 @@ After this table is created, merge will only work for data parts that have the s
 
 To specify a partition in ALTER PARTITION commands, specify the value of the partition expression (or a tuple). Constants and constant expressions are supported. Example:
 
-``` sql
+```sql
 ALTER TABLE table DROP PARTITION (toMonday(today()), 1)
 ```
 
@@ -45,5 +45,3 @@ The partition ID is its string identifier (human-readable, if possible) that is
 
 For more examples, see the tests [`00502_custom_partitioning_local`](https://github.com/yandex/ClickHouse/blob/master/dbms/tests/queries/0_stateless/00502_custom_partitioning_local.sql) and [`00502_custom_partitioning_replicated_zookeeper`](https://github.com/yandex/ClickHouse/blob/master/dbms/tests/queries/0_stateless/00502_custom_partitioning_replicated_zookeeper.sql).
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/custom_partitioning_key/) <!--hide-->
diff --git a/docs/en/operations/table_engines/dictionary.md b/docs/en/operations/table_engines/dictionary.md
index eed7f7afa..6bf606ffa 100644
--- a/docs/en/operations/table_engines/dictionary.md
+++ b/docs/en/operations/table_engines/dictionary.md
@@ -39,7 +39,7 @@ As an example, consider a dictionary of `products` with the following configurat
 
 Query the dictionary data:
 
-``` sql
+```sql
 select name, type, key, attribute.names, attribute.types, bytes_allocated, element_count,source from system.dictionaries where name = 'products';                     
 
 SELECT
@@ -73,7 +73,7 @@ CREATE TABLE %table_name% (%fields%) engine = Dictionary(%dictionary_name%)`
 
 Usage example:
 
-``` sql
+```sql
 create table products (product_id UInt64, title String) Engine = Dictionary(products);
 
 CREATE TABLE products
@@ -92,7 +92,7 @@ Ok.
 
 Take a look at what's in the table.
 
-``` sql
+```sql
 select * from products limit 1;
 
 SELECT *
@@ -108,5 +108,3 @@ LIMIT 1
 1 rows in set. Elapsed: 0.006 sec.
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/dictionary/) <!--hide-->
diff --git a/docs/en/operations/table_engines/distributed.md b/docs/en/operations/table_engines/distributed.md
index 6bd60c159..0bb55bc26 100644
--- a/docs/en/operations/table_engines/distributed.md
+++ b/docs/en/operations/table_engines/distributed.md
@@ -7,7 +7,7 @@ Reading is automatically parallelized. During a read, the table indexes on remot
 The Distributed engine accepts parameters: the cluster name in the server's config file, the name of a remote database, the name of a remote table, and (optionally) a sharding key.
 Example:
 
-```
+```text
 Distributed(logs, default, hits[, sharding_key])
 ```
 
@@ -122,5 +122,3 @@ If the server ceased to exist or had a rough restart (for example, after a devic
 
 When the max_parallel_replicas option is enabled, query processing is parallelized across all replicas within a single shard. For more information, see the section "Settings, max_parallel_replicas".
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/distributed/) <!--hide-->
diff --git a/docs/en/operations/table_engines/external_data.md b/docs/en/operations/table_engines/external_data.md
index ea27c9f0e..efb1f3c4f 100644
--- a/docs/en/operations/table_engines/external_data.md
+++ b/docs/en/operations/table_engines/external_data.md
@@ -60,5 +60,3 @@ curl -F 'passwd=@passwd.tsv;' 'http://localhost:8123/?query=SELECT+shell,+count(
 
 For distributed query processing, the temporary tables are sent to all the remote servers.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/external_data/) <!--hide-->
diff --git a/docs/en/operations/table_engines/file.md b/docs/en/operations/table_engines/file.md
index ed49a6936..8e31e3460 100644
--- a/docs/en/operations/table_engines/file.md
+++ b/docs/en/operations/table_engines/file.md
@@ -31,7 +31,7 @@ You may manually create this subfolder and file in server filesystem and then [A
 
 **1.** Set up the `file_engine_table` table:
 
-``` sql
+```sql
 CREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(TabSeparated)
 ```
 
@@ -47,11 +47,11 @@ two	2
 
 **3.** Query the data:
 
-``` sql
+```sql
 SELECT * FROM file_engine_table
 ```
 
-```
+```text
 ┌─name─┬─value─┐
 │ one  │     1 │
 │ two  │     2 │
@@ -76,5 +76,3 @@ $ echo -e "1,2\n3,4" | clickhouse-local -q "CREATE TABLE table (a Int64, b Int64
   - `SELECT ... SAMPLE`
   - Indices
   - Replication
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/file/) <!--hide-->
diff --git a/docs/en/operations/table_engines/graphitemergetree.md b/docs/en/operations/table_engines/graphitemergetree.md
index bc557594d..d1aaeca6a 100644
--- a/docs/en/operations/table_engines/graphitemergetree.md
+++ b/docs/en/operations/table_engines/graphitemergetree.md
@@ -27,7 +27,7 @@ The Graphite data table must contain the following fields at minimum:
 
 Rollup pattern:
 
-```
+```text
 pattern
     regexp
     function
@@ -84,5 +84,3 @@ Example of settings:
 </graphite_rollup>
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/graphitemergetree/) <!--hide-->
diff --git a/docs/en/operations/table_engines/index.md b/docs/en/operations/table_engines/index.md
index 21909e3ce..eba6fffe3 100644
--- a/docs/en/operations/table_engines/index.md
+++ b/docs/en/operations/table_engines/index.md
@@ -14,5 +14,3 @@ The table engine (type of table) determines:
 When reading, the engine is only required to output the requested columns, but in some cases the engine can partially process data when responding to the request.
 
 For most serious tasks, you should use engines from the `MergeTree` family.
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/) <!--hide-->
diff --git a/docs/en/operations/table_engines/join.md b/docs/en/operations/table_engines/join.md
index 299071312..1aff89ca8 100644
--- a/docs/en/operations/table_engines/join.md
+++ b/docs/en/operations/table_engines/join.md
@@ -2,7 +2,7 @@
 
 A prepared data structure for JOIN that is always located in RAM.
 
-```
+```text
 Join(ANY|ALL, LEFT|INNER, k1[, k2, ...])
 ```
 
@@ -15,5 +15,3 @@ You can use INSERT to add data to the table, similar to the Set engine. For ANY,
 
 Storing data on the disk is the same as for the Set engine.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/join/) <!--hide-->
diff --git a/docs/en/operations/table_engines/kafka.md b/docs/en/operations/table_engines/kafka.md
index 3ee701464..eb79599e2 100644
--- a/docs/en/operations/table_engines/kafka.md
+++ b/docs/en/operations/table_engines/kafka.md
@@ -44,7 +44,7 @@ Optional parameters:
 
 Examples:
 
-``` sql
+```sql
   CREATE TABLE queue (
     timestamp UInt64,
     level String,
@@ -86,7 +86,7 @@ When the `MATERIALIZED VIEW` joins the engine, it starts collecting data in the
 
 Example:
 
-``` sql
+```sql
   CREATE TABLE queue (
     timestamp UInt64,
     level String,
@@ -136,5 +136,3 @@ Similar to GraphiteMergeTree, the Kafka engine supports extended configuration u
 ```
 
 For a list of possible configuration options, see the [librdkafka configuration reference](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md). Use the underscore (`_`) instead of a dot in the ClickHouse configuration. For example, `check.crcs=true` will be `<check_crcs>true</check_crcs>`.
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/kafka/) <!--hide-->
diff --git a/docs/en/operations/table_engines/log.md b/docs/en/operations/table_engines/log.md
index fffc5a11a..406f94bd0 100644
--- a/docs/en/operations/table_engines/log.md
+++ b/docs/en/operations/table_engines/log.md
@@ -4,5 +4,3 @@ Log differs from TinyLog in that a small file of "marks" resides with the column
 For concurrent data access, the read operations can be performed simultaneously, while write operations block reads and each other.
 The Log engine does not support indexes. Similarly, if writing to a table failed, the table is broken, and reading from it returns an error. The Log engine is appropriate for temporary data, write-once tables, and for testing or demonstration purposes.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/log/) <!--hide-->
diff --git a/docs/en/operations/table_engines/materializedview.md b/docs/en/operations/table_engines/materializedview.md
index c13a1ac87..e2eb857ac 100644
--- a/docs/en/operations/table_engines/materializedview.md
+++ b/docs/en/operations/table_engines/materializedview.md
@@ -2,5 +2,3 @@
 
 Used for implementing materialized views (for more information, see [CREATE TABLE](../../query_language/create.md#query_language-queries-create_table)). For storing data, it uses a different engine that was specified when creating the view. When reading from a table, it just uses this engine.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/materializedview/) <!--hide-->
diff --git a/docs/en/operations/table_engines/memory.md b/docs/en/operations/table_engines/memory.md
index d68ae9236..113e4aa9f 100644
--- a/docs/en/operations/table_engines/memory.md
+++ b/docs/en/operations/table_engines/memory.md
@@ -9,5 +9,3 @@ Normally, using this table engine is not justified. However, it can be used for
 
 The Memory engine is used by the system for temporary tables with external query data (see the section "External data for processing a query"), and for implementing GLOBAL IN (see the section "IN operators").
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/memory/) <!--hide-->
diff --git a/docs/en/operations/table_engines/merge.md b/docs/en/operations/table_engines/merge.md
index e8de53bc2..e04dace12 100644
--- a/docs/en/operations/table_engines/merge.md
+++ b/docs/en/operations/table_engines/merge.md
@@ -65,5 +65,3 @@ The `Merge` type table contains a virtual `_table` column of the `String` type.
 
 If the `WHERE/PREWHERE`  clause contains conditions for the `_table`  column that do not depend on other table columns (as one of the conjunction elements, or as an entire expression), these conditions are used as an index. The conditions are performed on a data set of table names to read data from, and the read operation will be performed from only those tables that the condition was triggered on.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/merge/) <!--hide-->
diff --git a/docs/en/operations/table_engines/mergetree.md b/docs/en/operations/table_engines/mergetree.md
index 35cb660f5..3863c63b4 100644
--- a/docs/en/operations/table_engines/mergetree.md
+++ b/docs/en/operations/table_engines/mergetree.md
@@ -156,7 +156,7 @@ ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDa
 
 In this case, in queries:
 
-``` sql
+```sql
 SELECT count() FROM table WHERE EventDate = toDate(now()) AND CounterID = 34
 SELECT count() FROM table WHERE EventDate = toDate(now()) AND (CounterID = 34 OR CounterID = 42)
 SELECT count() FROM table WHERE ((EventDate >= toDate('2014-01-01') AND EventDate <= toDate('2014-01-31')) OR EventDate = toDate('2014-05-01')) AND CounterID IN (101500, 731962, 160656) AND (CounterID = 101500 OR EventDate != toDate('2014-05-01'))
@@ -168,7 +168,7 @@ The queries above show that the index is used even for complex expressions. Read
 
 In the example below, the index can't be used.
 
-``` sql
+```sql
 SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'
 ```
 
@@ -182,5 +182,3 @@ For concurrent table access, we use multi-versioning. In other words, when a tab
 
 Reading from a table is automatically parallelized.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/mergetree/) <!--hide-->
diff --git a/docs/en/operations/table_engines/mysql.md b/docs/en/operations/table_engines/mysql.md
index b9e419b5a..06c0fd6b6 100644
--- a/docs/en/operations/table_engines/mysql.md
+++ b/docs/en/operations/table_engines/mysql.md
@@ -26,5 +26,3 @@ The rest of the conditions and the `LIMIT` sampling constraint are executed in C
 
 The `MySQL` engine does not support the [Nullable](../../data_types/nullable.md#data_type-nullable) data type, so when reading data from MySQL tables, `NULL` is converted to default values for the specified column type (usually 0 or an empty string).
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/mysql/) <!--hide-->
diff --git a/docs/en/operations/table_engines/null.md b/docs/en/operations/table_engines/null.md
index 58d3552d1..49850ec98 100644
--- a/docs/en/operations/table_engines/null.md
+++ b/docs/en/operations/table_engines/null.md
@@ -4,5 +4,3 @@ When writing to a Null table, data is ignored. When reading from a Null table, t
 
 However, you can create a materialized view on a Null table. So the data written to the table will end up in the view.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/null/) <!--hide-->
diff --git a/docs/en/operations/table_engines/replacingmergetree.md b/docs/en/operations/table_engines/replacingmergetree.md
index 482f49982..92f2ffb34 100644
--- a/docs/en/operations/table_engines/replacingmergetree.md
+++ b/docs/en/operations/table_engines/replacingmergetree.md
@@ -6,7 +6,7 @@ The last optional parameter for the table engine is the version column. When mer
 
 The version column must have a type from the `UInt` family, `Date`, or `DateTime`.
 
-``` sql
+```sql
 ReplacingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, ver)
 ```
 
@@ -16,5 +16,3 @@ Thus, `ReplacingMergeTree` is suitable for clearing out duplicate data  in the b
 
 *This engine is not used in Yandex.Metrica, but it has been applied in other Yandex projects.*
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/replacingmergetree/) <!--hide-->
diff --git a/docs/en/operations/table_engines/replication.md b/docs/en/operations/table_engines/replication.md
index 9574cbaac..705dabe8e 100644
--- a/docs/en/operations/table_engines/replication.md
+++ b/docs/en/operations/table_engines/replication.md
@@ -78,7 +78,7 @@ Two parameters are also added in the beginning of the parameters list – the pa
 
 Example:
 
-```
+```text
 ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/hits', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192)
 ```
 
@@ -180,5 +180,3 @@ After this, you can launch the server, create a `MergeTree` table, move the data
 ## Recovery When Metadata in The ZooKeeper Cluster is Lost or Damaged
 
 If the data in ZooKeeper was lost or damaged, you can save data by moving it to an unreplicated table as described above.
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/replication/) <!--hide-->
diff --git a/docs/en/operations/table_engines/set.md b/docs/en/operations/table_engines/set.md
index 75c1f3072..d207fe0d7 100644
--- a/docs/en/operations/table_engines/set.md
+++ b/docs/en/operations/table_engines/set.md
@@ -9,5 +9,3 @@ Data is always located in RAM. For INSERT, the blocks of inserted data are also
 
 For a rough server restart, the block of data on the disk might be lost or damaged. In the latter case, you may need to manually delete the file with damaged data.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/set/) <!--hide-->
diff --git a/docs/en/operations/table_engines/summingmergetree.md b/docs/en/operations/table_engines/summingmergetree.md
index 82df82180..c75d09aae 100644
--- a/docs/en/operations/table_engines/summingmergetree.md
+++ b/docs/en/operations/table_engines/summingmergetree.md
@@ -4,13 +4,13 @@
 
 This engine differs from `MergeTree` in that it totals data while merging.
 
-``` sql
+```sql
 SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192)
 ```
 
 The columns to total are implicit. When merging, all rows with the same primary key value (in the example, OrderId, EventDate, BannerID, ...) have their values totaled in numeric columns that are not part of the primary key.
 
-``` sql
+```sql
 SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, (Shows, Clicks, Cost, ...))
 ```
 
@@ -32,7 +32,7 @@ Then this nested table is interpreted as a mapping of key `=>` (values...), and
 
 Examples:
 
-```
+```text
 [(1, 100)] + [(2, 150)] -> [(1, 100), (2, 150)]
 [(1, 100)] + [(1, 150)] -> [(1, 250)]
 [(1, 100)] + [(1, 150), (2, 150)] -> [(1, 250), (2, 150)]
@@ -45,5 +45,3 @@ For nested data structures, you don't need to specify the columns as a list of c
 
 This table engine is not particularly useful. Remember that when saving just pre-aggregated data, you lose some of the system's advantages.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/summingmergetree/) <!--hide-->
diff --git a/docs/en/operations/table_engines/tinylog.md b/docs/en/operations/table_engines/tinylog.md
index 6ec1cb817..577596e2d 100644
--- a/docs/en/operations/table_engines/tinylog.md
+++ b/docs/en/operations/table_engines/tinylog.md
@@ -17,5 +17,3 @@ The situation when you have a large number of small tables guarantees poor produ
 
 In Yandex.Metrica, TinyLog tables are used for intermediary data that is processed in small batches.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/tinylog/) <!--hide-->
diff --git a/docs/en/operations/table_engines/url.md b/docs/en/operations/table_engines/url.md
index 54d742e40..9bd79ac10 100644
--- a/docs/en/operations/table_engines/url.md
+++ b/docs/en/operations/table_engines/url.md
@@ -23,7 +23,7 @@ respectively. For processing `POST` requests, the remote server must support
 
 **1.** Create a `url_engine_table` table on the server :
 
-``` sql
+```sql
 CREATE TABLE url_engine_table (word String, value UInt64)
 ENGINE=URL('http://127.0.0.1:12345/', CSV)
 ```
@@ -53,11 +53,11 @@ python3 server.py
 
 **3.** Request data:
 
-``` sql
+```sql
 SELECT * FROM url_engine_table
 ```
 
-```
+```text
 ┌─word──┬─value─┐
 │ Hello │     1 │
 │ World │     2 │
@@ -71,5 +71,3 @@ SELECT * FROM url_engine_table
     - `ALTER` and `SELECT...SAMPLE` operations.
     - Indexes.
     - Replication.
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/url/) <!--hide-->
diff --git a/docs/en/operations/table_engines/view.md b/docs/en/operations/table_engines/view.md
index c74eab262..91f0ce9fd 100644
--- a/docs/en/operations/table_engines/view.md
+++ b/docs/en/operations/table_engines/view.md
@@ -2,5 +2,3 @@
 
 Used for implementing views (for more information, see the `CREATE VIEW query`). It does not store data, but only stores the specified `SELECT` query. When reading from a table, it runs this query (and deletes all unnecessary columns from the query).
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/table_engines/view/) <!--hide-->
diff --git a/docs/en/operations/tips.md b/docs/en/operations/tips.md
index a34f79f86..fe2e96701 100644
--- a/docs/en/operations/tips.md
+++ b/docs/en/operations/tips.md
@@ -178,7 +178,7 @@ dynamicConfigFile=/etc/zookeeper-{{ cluster['name'] }}/conf/zoo.cfg.dynamic
 
 Java version:
 
-```
+```text
 Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
 Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
 ```
@@ -226,7 +226,7 @@ JAVA_OPTS="-Xms{{ cluster.get('xms','128M') }} \
 
 Salt init:
 
-```
+```text
 description "zookeeper-{{ cluster['name'] }} centralized coordination service"
 
 start on runlevel [2345]
@@ -255,5 +255,3 @@ script
 end script
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/tips/) <!--hide-->
diff --git a/docs/en/operations/utils/clickhouse-copier.md b/docs/en/operations/utils/clickhouse-copier.md
index 361834a68..310904472 100644
--- a/docs/en/operations/utils/clickhouse-copier.md
+++ b/docs/en/operations/utils/clickhouse-copier.md
@@ -159,5 +159,3 @@ Parameters:
 
 `clickhouse-copier` tracks the changes in `/task/path/description` and applies them on the fly. For instance, if you change the value of `max_workers`, the number of processes running tasks will also change.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/utils/clickhouse-copier/) <!--hide-->
diff --git a/docs/en/operations/utils/clickhouse-local.md b/docs/en/operations/utils/clickhouse-local.md
index 4b20473cb..bfa612569 100644
--- a/docs/en/operations/utils/clickhouse-local.md
+++ b/docs/en/operations/utils/clickhouse-local.md
@@ -71,5 +71,3 @@ Read 186 rows, 4.15 KiB in 0.035 sec., 5302 rows/sec., 118.34 KiB/sec.
 ├──────────┼──────────┤
 ...
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/operations/utils/clickhouse-local/) <!--hide-->
diff --git a/docs/en/operations/utils/index.md b/docs/en/operations/utils/index.md
index 6406b486c..24faf79fa 100644
--- a/docs/en/operations/utils/index.md
+++ b/docs/en/operations/utils/index.md
@@ -3,5 +3,3 @@
 * [clickhouse-local](clickhouse-local.md#utils-clickhouse-local) — Allows running SQL queries on data without stopping the ClickHouse server, similar to how `awk` does this.
 * [clickhouse-copier](clickhouse-copier.md#utils-clickhouse-copier) — Copies (and reshards) data from one cluster to another cluster.
 
-
-[Original article](https://clickhouse.yandex/docs/en/operations/utils/) <!--hide-->
diff --git a/docs/en/query_language/agg_functions/combinators.md b/docs/en/query_language/agg_functions/combinators.md
index cf4f73fc1..3b7e372b3 100644
--- a/docs/en/query_language/agg_functions/combinators.md
+++ b/docs/en/query_language/agg_functions/combinators.md
@@ -38,5 +38,3 @@ Merges the intermediate aggregation states in the same way as the -Merge combina
 
 Converts an aggregate function for tables into an aggregate function for arrays that aggregates the corresponding array items and returns an array of results. For example, `sumForEach` for the arrays `[1, 2]`, `[3, 4, 5]`and`[6, 7]`returns the result `[10, 13, 5]` after adding together the corresponding array items.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/agg_functions/combinators/) <!--hide-->
diff --git a/docs/en/query_language/agg_functions/index.md b/docs/en/query_language/agg_functions/index.md
index a3cb2132e..0f9cefc7b 100644
--- a/docs/en/query_language/agg_functions/index.md
+++ b/docs/en/query_language/agg_functions/index.md
@@ -61,5 +61,3 @@ FROM t_null_big
 
 `groupArray` does not include `NULL` in the resulting array.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/agg_functions/) <!--hide-->
diff --git a/docs/en/query_language/agg_functions/parametric_functions.md b/docs/en/query_language/agg_functions/parametric_functions.md
index 134b4a4b9..18f519035 100644
--- a/docs/en/query_language/agg_functions/parametric_functions.md
+++ b/docs/en/query_language/agg_functions/parametric_functions.md
@@ -23,7 +23,7 @@ Example: `sequenceMatch ('(?1).*(?2)')(EventTime, URL LIKE '%company%', URL LIKE
 
 This is a singular example. You could write it using other aggregate functions:
 
-```
+```text
 minIf(EventTime, URL LIKE '%company%') < maxIf(EventTime, URL LIKE '%cart%').
 ```
 
@@ -151,9 +151,7 @@ It works as fast as possible, except for cases when a large N value is used and
 
 Usage example:
 
-```
+```text
 Problem: Generate a report that shows only keywords that produced at least 5 unique users.
 Solution: Write in the GROUP BY query SearchPhrase HAVING uniqUpTo(4)(UserID) >= 5
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/agg_functions/parametric_functions/) <!--hide-->
diff --git a/docs/en/query_language/agg_functions/reference.md b/docs/en/query_language/agg_functions/reference.md
index fd0bb2139..dcf692569 100644
--- a/docs/en/query_language/agg_functions/reference.md
+++ b/docs/en/query_language/agg_functions/reference.md
@@ -35,7 +35,7 @@ anyHeavy(column)
 
 Take the [OnTime](../../getting_started/example_datasets/ontime.md#example_datasets-ontime) data set and select any frequently occurring value in the `AirlineID` column.
 
-``` sql
+```sql
 SELECT anyHeavy(AirlineID) AS res
 FROM ontime
 ```
@@ -101,7 +101,7 @@ Returns a tuple of two arrays: keys in sorted order, and values ​​summed for
 
 Example:
 
-``` sql
+```sql
 CREATE TABLE sum_map(
     date Date,
     timeslot DateTime,
@@ -122,7 +122,7 @@ FROM sum_map
 GROUP BY timeslot
 ```
 
-```
+```text
 ┌────────────timeslot─┬─sumMap(statusMap.status, statusMap.requests)─┐
 │ 2000-01-01 00:00:00 │ ([1,2,3,4,5],[10,10,20,10,10])               │
 │ 2000-01-01 00:01:00 │ ([4,5,6,7,8],[10,10,20,10,10])               │
@@ -325,7 +325,7 @@ We recommend using the `N < 10 ` value; performance is reduced with large `N` va
 
 Take the [OnTime](../../getting_started/example_datasets/ontime.md#example_datasets-ontime) data set and select the three most frequently occurring values in the `AirlineID` column.
 
-``` sql
+```sql
 SELECT topK(3)(AirlineID) AS res
 FROM ontime
 ```
@@ -350,5 +350,3 @@ Calculates the value of `Σ((x - x̅)(y - y̅)) / n`.
 
 Calculates the Pearson correlation coefficient: `Σ((x - x̅)(y - y̅)) / sqrt(Σ((x - x̅)^2) * Σ((y - y̅)^2))`.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/agg_functions/reference/) <!--hide-->
diff --git a/docs/en/query_language/alter.md b/docs/en/query_language/alter.md
index cdc432711..8a6b0eb08 100644
--- a/docs/en/query_language/alter.md
+++ b/docs/en/query_language/alter.md
@@ -8,7 +8,7 @@ The `ALTER` query is only supported for `*MergeTree` tables, as well as `Merge`a
 
 Changing the table structure.
 
-``` sql
+```sql
 ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...
 ```
 
@@ -17,7 +17,7 @@ Each action is an operation on a column.
 
 The following actions are supported:
 
-``` sql
+```sql
 ADD COLUMN name [type] [default_expr] [AFTER name_after]
 ```
 
@@ -27,14 +27,14 @@ Adding a column just changes the table structure, without performing any actions
 
 This approach allows us to complete the ALTER query instantly, without increasing the volume of old data.
 
-``` sql
+```sql
 DROP COLUMN name
 ```
 
 Deletes the column with the name 'name'.
 Deletes data from the file system. Since this deletes entire files, the query is completed almost instantly.
 
-``` sql
+```sql
 MODIFY COLUMN name [type] [default_expr]
 ```
 
@@ -86,7 +86,7 @@ A "part" in the table is part of the data from a single partition, sorted by the
 
 You can use the `system.parts` table to view the set of table parts and partitions:
 
-``` sql
+```sql
 SELECT * FROM system.parts WHERE active
 ```
 
@@ -123,7 +123,7 @@ For replicated tables, the set of parts can't be changed in any case.
 
 The `detached` directory contains parts that are not used by the server - detached from the table using the `ALTER ... DETACH` query. Parts that are damaged are also moved to this directory, instead of deleting them. You can add, delete, or modify the data in the 'detached' directory at any time – the server won't know about this until you make the `ALTER TABLE ... ATTACH` query.
 
-``` sql
+```sql
 ALTER TABLE [db.]table DETACH PARTITION 'name'
 ```
 
@@ -134,13 +134,13 @@ After the query is executed, you can do whatever you want with the data in the '
 
 The query is replicated – data will be moved to the 'detached' directory and forgotten on all replicas. The query can only be sent to a leader replica. To find out if a replica is a leader, perform SELECT to the 'system.replicas' system table. Alternatively, it is easier to make a query on all replicas, and all except one will throw an exception.
 
-``` sql
+```sql
 ALTER TABLE [db.]table DROP PARTITION 'name'
 ```
 
 The same as the `DETACH` operation. Deletes data from the table. Data parts will be tagged as inactive and will be completely deleted in approximately 10 minutes. The query is replicated – data will be deleted on all replicas.
 
-``` sql
+```sql
 ALTER TABLE [db.]table ATTACH PARTITION|PART 'name'
 ```
 
@@ -152,7 +152,7 @@ The query is replicated. Each replica checks whether there is data in the 'detac
 
 So you can put data in the 'detached' directory on one replica, and use the ALTER ... ATTACH query to add it to the table on all replicas.
 
-``` sql
+```sql
 ALTER TABLE [db.]table FREEZE PARTITION 'name'
 ```
 
@@ -196,7 +196,7 @@ For protection from device failures, you must use replication. For more informat
 Backups protect against human error (accidentally deleting data, deleting the wrong data or in the wrong cluster, or corrupting data).
 For high-volume databases, it can be difficult to copy backups to remote servers. In such cases, to protect from human error, you can keep a backup on the same server (it will reside in `/var/lib/clickhouse/shadow/`).
 
-``` sql
+```sql
 ALTER TABLE [db.]table FETCH PARTITION 'name' FROM 'path-in-zookeeper'
 ```
 
@@ -232,13 +232,13 @@ Existing tables are ready for mutations as-is (no conversion necessary), but aft
 
 Currently available commands:
 
-``` sql
+```sql
 ALTER TABLE [db.]table DELETE WHERE filter_expr
 ```
 
 The `filter_expr` must be of type UInt8. The query deletes rows in the table for which this expression takes a non-zero value.
 
-``` sql
+```sql
 ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr
 ```
 
@@ -272,5 +272,3 @@ The table contains information about mutations of MergeTree tables and their pro
 
 **is_done** - Is the mutation done? Note that even if `parts_to_do = 0` it is possible that a mutation of a replicated table is not done yet because of a long-running INSERT that will create a new data part that will need to be mutated.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/alter/) <!--hide-->
diff --git a/docs/en/query_language/create.md b/docs/en/query_language/create.md
index 4956955b3..9369c3a55 100644
--- a/docs/en/query_language/create.md
+++ b/docs/en/query_language/create.md
@@ -2,7 +2,7 @@
 
 Creating db_name databases
 
-``` sql
+```sql
 CREATE DATABASE [IF NOT EXISTS] db_name
 ```
 
@@ -15,7 +15,7 @@ If `IF NOT EXISTS` is included, the query won't return an error if the database
 
 The `CREATE TABLE` query can have several forms.
 
-``` sql
+```sql
 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
 (
     name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
@@ -30,13 +30,13 @@ The structure of the table is a list of column descriptions. If indexes are supp
 A column description is `name type` in the simplest case. Example: `RegionID UInt32`.
 Expressions can also be defined for default values (see below).
 
-``` sql
+```sql
 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name AS [db2.]name2 [ENGINE = engine]
 ```
 
 Creates a table with the same structure as another table. You can specify a different engine for the table. If the engine is not specified, the same engine will be used as for the `db2.name2` table.
 
-``` sql
+```sql
 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name ENGINE = engine AS SELECT ...
 ```
 
@@ -97,7 +97,7 @@ Distributed DDL queries (ON CLUSTER clause)
 The `CREATE`, `DROP`, `ALTER`, and `RENAME` queries support distributed execution on a cluster.
 For example, the following query creates the `all_hits` `Distributed` table on each host in `cluster`:
 
-``` sql
+```sql
 CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE = Distributed(cluster, default, hits)
 ```
 
@@ -107,7 +107,7 @@ The local version of the query will eventually be implemented on each host in th
 
 ## CREATE VIEW
 
-``` sql
+```sql
 CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...
 ```
 
@@ -121,19 +121,19 @@ Normal views don't store any data, but just perform a read from another table. I
 
 As an example, assume you've created a view:
 
-``` sql
+```sql
 CREATE VIEW view AS SELECT ...
 ```
 
 and written a query:
 
-``` sql
+```sql
 SELECT a, b, c FROM view
 ```
 
 This query is fully equivalent to using the subquery:
 
-``` sql
+```sql
 SELECT a, b, c FROM (SELECT ...)
 ```
 
@@ -152,5 +152,3 @@ The execution of `ALTER` queries on materialized views has not been fully develo
 Views look the same as normal tables. For example, they are listed in the result of the `SHOW TABLES` query.
 
 There isn't a separate query for deleting views. To delete a view, use `DROP TABLE`.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/create/) <!--hide-->
diff --git a/docs/en/query_language/dicts/external_dicts.md b/docs/en/query_language/dicts/external_dicts.md
index 8840218e4..7f1063a04 100644
--- a/docs/en/query_language/dicts/external_dicts.md
+++ b/docs/en/query_language/dicts/external_dicts.md
@@ -41,5 +41,3 @@ See also "[Functions for working with external dictionaries](../functions/ext_di
 
 !!! attention
     You can convert values for a small dictionary by describing it in a `SELECT` query (see the [transform](../functions/other_functions.md#other_functions-transform) function). This functionality is not related to external dictionaries.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/external_dicts/) <!--hide-->
diff --git a/docs/en/query_language/dicts/external_dicts_dict.md b/docs/en/query_language/dicts/external_dicts_dict.md
index ca070c808..daf00ce02 100644
--- a/docs/en/query_language/dicts/external_dicts_dict.md
+++ b/docs/en/query_language/dicts/external_dicts_dict.md
@@ -31,5 +31,3 @@ The dictionary configuration has the following structure:
 - [layout](external_dicts_dict_layout.md#dicts-external_dicts_dict_layout) — Dictionary layout in memory.
 - [structure](external_dicts_dict_structure.md#dicts-external_dicts_dict_structure) — Structure of the dictionary . A key and attributes that can be retrieved by this key.
 - [lifetime](external_dicts_dict_lifetime.md#dicts-external_dicts_dict_lifetime) — Frequency of dictionary updates.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/external_dicts_dict/) <!--hide-->
diff --git a/docs/en/query_language/dicts/external_dicts_dict_layout.md b/docs/en/query_language/dicts/external_dicts_dict_layout.md
index 932d90db6..d6802c4cc 100644
--- a/docs/en/query_language/dicts/external_dicts_dict_layout.md
+++ b/docs/en/query_language/dicts/external_dicts_dict_layout.md
@@ -292,5 +292,3 @@ dictGetString('prefix', 'asn', tuple(IPv6StringToNum('2001:db8::1')))
 Other types are not supported yet. The function returns the attribute for the prefix that corresponds to this IP address. If there are overlapping prefixes, the most specific one is returned.
 
 Data is stored in a `trie`. It must completely fit into RAM.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/external_dicts_dict_layout/) <!--hide-->
diff --git a/docs/en/query_language/dicts/external_dicts_dict_lifetime.md b/docs/en/query_language/dicts/external_dicts_dict_lifetime.md
index c04829fce..2fadfa51e 100644
--- a/docs/en/query_language/dicts/external_dicts_dict_lifetime.md
+++ b/docs/en/query_language/dicts/external_dicts_dict_lifetime.md
@@ -57,5 +57,3 @@ Example of settings:
 </dictionary>
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/external_dicts_dict_lifetime/) <!--hide-->
diff --git a/docs/en/query_language/dicts/external_dicts_dict_sources.md b/docs/en/query_language/dicts/external_dicts_dict_sources.md
index 977566afc..4c30852c7 100644
--- a/docs/en/query_language/dicts/external_dicts_dict_sources.md
+++ b/docs/en/query_language/dicts/external_dicts_dict_sources.md
@@ -111,7 +111,7 @@ Example of settings:
 ```xml
 <odbc>
     <db>DatabaseName</db>
-    <table>ShemaName.TableName</table>
+    <table>TableName</table>
     <connection_string>DSN=some_parameters</connection_string>
     <invalidate_query>SQL_QUERY</invalidate_query>
 </odbc>
@@ -120,11 +120,10 @@ Example of settings:
 Setting fields:
 
 - `db` – Name of the database. Omit it if the database name is set in the `<connection_string>` parameters.
-- `table` – Name of the table and schema if exists.
+- `table` – Name of the table.
 - `connection_string` – Connection string.
 - `invalidate_query` – Query for checking the dictionary status. Optional parameter. Read more in the section [Updating dictionaries](external_dicts_dict_lifetime.md#dicts-external_dicts_dict_lifetime).
 
-ClickHouse receives quoting symbols from ODBC-driver and quote all settings in queries to driver, so it's necessary to set table name accordingly to table name case in database.
 
 ### Known vulnerability of the ODBC dictionary functionality
 
@@ -428,5 +427,3 @@ Setting fields:
 - `password` – Password of the MongoDB user.
 - `db` – Name of the database.
 - `collection` – Name of the collection.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/external_dicts_dict_sources/) <!--hide-->
diff --git a/docs/en/query_language/dicts/external_dicts_dict_structure.md b/docs/en/query_language/dicts/external_dicts_dict_structure.md
index b499a474a..06ab8e812 100644
--- a/docs/en/query_language/dicts/external_dicts_dict_structure.md
+++ b/docs/en/query_language/dicts/external_dicts_dict_structure.md
@@ -115,5 +115,3 @@ Configuration fields:
 - `hierarchical` – Hierarchical support. Mirrored to the parent identifier. By default, ` false`.
 - `injective` – Whether the `id -> attribute` image is injective. If ` true`, then you can optimize the ` GROUP BY` clause. By default, `false`.
 - `is_object_id` – Whether the query is executed for a MongoDB document by `ObjectID`.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/external_dicts_dict_structure/) <!--hide-->
diff --git a/docs/en/query_language/dicts/index.md b/docs/en/query_language/dicts/index.md
index 1c61a6534..862dd686f 100644
--- a/docs/en/query_language/dicts/index.md
+++ b/docs/en/query_language/dicts/index.md
@@ -11,5 +11,3 @@ ClickHouse supports:
 - [Built-in dictionaries](internal_dicts.md#internal_dicts) with a specific [set of functions](../functions/ym_dict_functions.md#ym_dict_functions).
 - [Plug-in (external) dictionaries](external_dicts.md#dicts-external_dicts) with a [set of functions](../functions/ext_dict_functions.md#ext_dict_functions).
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/) <!--hide-->
diff --git a/docs/en/query_language/dicts/internal_dicts.md b/docs/en/query_language/dicts/internal_dicts.md
index 2b38ffe61..9849bd21f 100644
--- a/docs/en/query_language/dicts/internal_dicts.md
+++ b/docs/en/query_language/dicts/internal_dicts.md
@@ -46,5 +46,3 @@ Dictionary updates (other than loading at first use) do not block queries. Durin
 We recommend periodically updating the dictionaries with the geobase. During an update, generate new files and write them to a separate location. When everything is ready, rename them to the files used by the server.
 
 There are also functions for working with OS identifiers and Yandex.Metrica search engines, but they shouldn't be used.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/dicts/internal_dicts/) <!--hide-->
diff --git a/docs/en/query_language/functions/arithmetic_functions.md b/docs/en/query_language/functions/arithmetic_functions.md
index 7420fbe53..e457fe668 100644
--- a/docs/en/query_language/functions/arithmetic_functions.md
+++ b/docs/en/query_language/functions/arithmetic_functions.md
@@ -4,11 +4,11 @@ For all arithmetic functions, the result type is calculated as the smallest numb
 
 Example:
 
-``` sql
+```sql
 SELECT toTypeName(0), toTypeName(0 + 0), toTypeName(0 + 0 + 0), toTypeName(0 + 0 + 0 + 0)
 ```
 
-```
+```text
 ┌─toTypeName(0)─┬─toTypeName(plus(0, 0))─┬─toTypeName(plus(plus(0, 0), 0))─┬─toTypeName(plus(plus(plus(0, 0), 0), 0))─┐
 │ UInt8         │ UInt16                 │ UInt32                          │ UInt64                                   │
 └───────────────┴────────────────────────┴─────────────────────────────────┴──────────────────────────────────────────┘
@@ -73,5 +73,3 @@ An exception is thrown when dividing by zero or when dividing a minimal negative
 Returns the least common multiple of the numbers.
 An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/arithmetic_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/array_functions.md b/docs/en/query_language/functions/array_functions.md
index afb4f5f1e..e250479e5 100644
--- a/docs/en/query_language/functions/array_functions.md
+++ b/docs/en/query_language/functions/array_functions.md
@@ -59,7 +59,7 @@ arrayConcat(arrays)
 
 **Example**
 
-``` sql
+```sql
 SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res
 ```
 
@@ -132,7 +132,7 @@ Returns the array \[1, 2, 3, ..., length (arr) \]
 
 This function is normally used with ARRAY JOIN. It allows counting something just once for each array after applying ARRAY JOIN. Example:
 
-``` sql
+```sql
 SELECT
     count() AS Reaches,
     countIf(num = 1) AS Hits
@@ -144,7 +144,7 @@ WHERE CounterID = 160656
 LIMIT 10
 ```
 
-```
+```text
 ┌─Reaches─┬──Hits─┐
 │   95606 │ 31406 │
 └─────────┴───────┘
@@ -152,7 +152,7 @@ LIMIT 10
 
 In this example, Reaches is the number of conversions (the strings received after applying ARRAY JOIN), and Hits is the number of pageviews (strings before ARRAY JOIN). In this particular case, you can get the same result in an easier way:
 
-``` sql
+```sql
 SELECT
     sum(length(GoalsReached)) AS Reaches,
     count() AS Hits
@@ -160,7 +160,7 @@ FROM test.hits
 WHERE (CounterID = 160656) AND notEmpty(GoalsReached)
 ```
 
-```
+```text
 ┌─Reaches─┬──Hits─┐
 │   95606 │ 31406 │
 └─────────┴───────┘
@@ -176,7 +176,7 @@ For example: arrayEnumerateUniq(\[10, 20, 10, 30\]) = \[1,  1,  2,  1\].
 This function is useful when using ARRAY JOIN and aggregation of array elements.
 Example:
 
-``` sql
+```sql
 SELECT
     Goals.ID AS GoalID,
     sum(Sign) AS Reaches,
@@ -191,7 +191,7 @@ ORDER BY Reaches DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌──GoalID─┬─Reaches─┬─Visits─┐
 │   53225 │    3214 │   1097 │
 │ 2825062 │    3188 │   1097 │
@@ -210,11 +210,11 @@ In this example, each goal ID has a calculation of the number of conversions (ea
 
 The arrayEnumerateUniq function can take multiple arrays of the same size as arguments. In this case, uniqueness is considered for tuples of elements in the same positions in all the arrays.
 
-``` sql
+```sql
 SELECT arrayEnumerateUniq([1, 1, 1, 2, 2, 2], [1, 1, 2, 1, 1, 2]) AS res
 ```
 
-```
+```text
 ┌─res───────────┐
 │ [1,2,1,1,2,1] │
 └───────────────┘
@@ -236,7 +236,7 @@ arrayPopBack(array)
 
 **Example**
 
-``` sql
+```sql
 SELECT arrayPopBack([1, 2, 3]) AS res
 ```
 
@@ -260,7 +260,7 @@ arrayPopFront(array)
 
 **Example**
 
-``` sql
+```sql
 SELECT arrayPopFront([1, 2, 3]) AS res
 ```
 
@@ -285,7 +285,7 @@ arrayPushBack(array, single_value)
 
 **Example**
 
-``` sql
+```sql
 SELECT arrayPushBack(['a'], 'b') AS res
 ```
 
@@ -310,7 +310,7 @@ arrayPushFront(array, single_value)
 
 **Example**
 
-``` sql
+```sql
 SELECT arrayPushBack(['b'], 'a') AS res
 ```
 
@@ -374,7 +374,7 @@ arraySlice(array, offset[, length])
 
 **Example**
 
-``` sql
+```sql
 SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res
 ```
 
@@ -396,5 +396,3 @@ If you want to get a list of unique items in an array, you can use arrayReduce('
 ## arrayJoin(arr)
 
 A special function. See the section ["ArrayJoin function"](array_join.md#functions_arrayjoin).
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/array_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/array_join.md b/docs/en/query_language/functions/array_join.md
index 01b1d383e..6e18f8203 100644
--- a/docs/en/query_language/functions/array_join.md
+++ b/docs/en/query_language/functions/array_join.md
@@ -17,11 +17,11 @@ Note the ARRAY JOIN syntax in the SELECT query, which provides broader possibili
 
 Example:
 
-``` sql
+```sql
 SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src
 ```
 
-```
+```text
 ┌─dst─┬─\'Hello\'─┬─src─────┐
 │   1 │ Hello     │ [1,2,3] │
 │   2 │ Hello     │ [1,2,3] │
@@ -29,5 +29,3 @@ SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src
 └─────┴───────────┴─────────┘
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/array_join/) <!--hide-->
diff --git a/docs/en/query_language/functions/bit_functions.md b/docs/en/query_language/functions/bit_functions.md
index 1664664a6..523413f20 100644
--- a/docs/en/query_language/functions/bit_functions.md
+++ b/docs/en/query_language/functions/bit_functions.md
@@ -16,5 +16,3 @@ The result type is an integer with bits equal to the maximum bits of its argumen
 
 ## bitShiftRight(a, b)
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/bit_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/comparison_functions.md b/docs/en/query_language/functions/comparison_functions.md
index 39987ef28..9b95966ba 100644
--- a/docs/en/query_language/functions/comparison_functions.md
+++ b/docs/en/query_language/functions/comparison_functions.md
@@ -29,5 +29,3 @@ Note. Up until version 1.1.54134, signed and unsigned numbers were compared the
 
 ## greaterOrEquals, `>= operator`
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/comparison_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/conditional_functions.md b/docs/en/query_language/functions/conditional_functions.md
index b5c7ce583..abd8c96e4 100644
--- a/docs/en/query_language/functions/conditional_functions.md
+++ b/docs/en/query_language/functions/conditional_functions.md
@@ -46,5 +46,3 @@ Run the query `SELECT multiIf(isNull(y) x, y < 3, y, NULL) FROM t_null`. Result:
 │                                       ᴺᵁᴸᴸ │
 └────────────────────────────────────────────┘
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/conditional_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/date_time_functions.md b/docs/en/query_language/functions/date_time_functions.md
index 828214744..8be3c14ff 100644
--- a/docs/en/query_language/functions/date_time_functions.md
+++ b/docs/en/query_language/functions/date_time_functions.md
@@ -4,7 +4,7 @@ Support for time zones
 
 All functions for working with the date and time that have a logical use for the time zone can accept a second optional time zone argument. Example: Asia/Yekaterinburg. In this case, they use the specified time zone instead of the local (default) one.
 
-``` sql
+```sql
 SELECT
     toDateTime('2016-06-15 23:00:00') AS time,
     toDate(time) AS date_local,
@@ -12,7 +12,7 @@ SELECT
     toString(time, 'US/Samoa') AS time_samoa
 ```
 
-```
+```text
 ┌────────────────time─┬─date_local─┬─date_yekat─┬─time_samoa──────────┐
 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-16 │ 2016-06-15 09:00:00 │
 └─────────────────────┴────────────┴────────────┴─────────────────────┘
@@ -182,5 +182,3 @@ Supported modifiers for Format:
 |%y|Year, last two digits (00-99)|18|
 |%Y|Year|2018|
 |%%|a % sign|%|
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/date_time_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/encoding_functions.md b/docs/en/query_language/functions/encoding_functions.md
index 74ef53f82..27ba1554a 100644
--- a/docs/en/query_language/functions/encoding_functions.md
+++ b/docs/en/query_language/functions/encoding_functions.md
@@ -25,5 +25,3 @@ Accepts an integer. Returns a string containing the list of powers of two that t
 
 Accepts an integer. Returns an array of UInt64 numbers containing the list of powers of two that total the source number when summed. Numbers in the array are in ascending order.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/encoding_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/ext_dict_functions.md b/docs/en/query_language/functions/ext_dict_functions.md
index ea6927694..68f5eaab7 100644
--- a/docs/en/query_language/functions/ext_dict_functions.md
+++ b/docs/en/query_language/functions/ext_dict_functions.md
@@ -45,5 +45,3 @@ The same as the `dictGetT` functions, but the default value is taken from the fu
 
 - Check whether the dictionary has the key. Returns a UInt8 value equal to 0 if there is no key and 1 if there is a key.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/ext_dict_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/functions_for_nulls.md b/docs/en/query_language/functions/functions_for_nulls.md
index 2b088b532..d52d0c840 100644
--- a/docs/en/query_language/functions/functions_for_nulls.md
+++ b/docs/en/query_language/functions/functions_for_nulls.md
@@ -293,5 +293,3 @@ SELECT toTypeName(toNullable(10))
 └────────────────────────────┘
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/functions_for_nulls/) <!--hide-->
diff --git a/docs/en/query_language/functions/geo.md b/docs/en/query_language/functions/geo.md
index 05725b95a..91d07c364 100644
--- a/docs/en/query_language/functions/geo.md
+++ b/docs/en/query_language/functions/geo.md
@@ -25,11 +25,11 @@ Generates an exception when the input parameter values fall outside of the range
 
 **Example**
 
-``` sql
+```sql
 SELECT greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)
 ```
 
-```
+```text
 ┌─greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)─┐
 │                                                14132374.194975413 │
 └───────────────────────────────────────────────────────────────────┘
@@ -58,11 +58,11 @@ The input parameters must be `2+4⋅n`, where `n` is the number of ellipses.
 
 **Example**
 
-``` sql
+```sql
 SELECT pointInEllipses(55.755831, 37.617673, 55.755831, 37.617673, 1.0, 2.0)
 ```
 
-```
+```text
 ┌─pointInEllipses(55.755831, 37.617673, 55.755831, 37.617673, 1., 2.)─┐
 │                                                                   1 │
 └─────────────────────────────────────────────────────────────────────┘
@@ -89,7 +89,7 @@ If the point is on the polygon boundary, the function may return either 0 or 1.
 
 **Example**
 
-``` sql
+```sql
 SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res
 ```
 
@@ -99,5 +99,3 @@ SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res
 └─────┘
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/geo/) <!--hide-->
diff --git a/docs/en/query_language/functions/hash_functions.md b/docs/en/query_language/functions/hash_functions.md
index ffffe5584..42107ce59 100644
--- a/docs/en/query_language/functions/hash_functions.md
+++ b/docs/en/query_language/functions/hash_functions.md
@@ -64,5 +64,3 @@ A fast, decent-quality non-cryptographic hash function for a string obtained fro
 `URLHash(s, N)` – Calculates a hash from a string up to the N level in the URL hierarchy, without one of the trailing symbols `/`,`?` or `#` at the end, if present.
 Levels are the same as in URLHierarchy. This function is specific to Yandex.Metrica.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/hash_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/higher_order_functions.md b/docs/en/query_language/functions/higher_order_functions.md
index b00896cb4..a80d5708e 100644
--- a/docs/en/query_language/functions/higher_order_functions.md
+++ b/docs/en/query_language/functions/higher_order_functions.md
@@ -22,17 +22,17 @@ Returns an array containing only the elements in 'arr1' for which 'func' returns
 
 Examples:
 
-``` sql
+```sql
 SELECT arrayFilter(x -> x LIKE '%World%', ['Hello', 'abc World']) AS res
 ```
 
-```
+```text
 ┌─res───────────┐
 │ ['abc World'] │
 └───────────────┘
 ```
 
-``` sql
+```sql
 SELECT
     arrayFilter(
         (i, x) -> x LIKE '%World%',
@@ -41,7 +41,7 @@ SELECT
     AS res
 ```
 
-```
+```text
 ┌─res─┐
 │ [2] │
 └─────┘
@@ -77,11 +77,11 @@ Returns an array of partial sums of elements in the source array (a running sum)
 
 Example:
 
-``` sql
+```sql
 SELECT arrayCumSum([1, 1, 1, 1]) AS res
 ```
 
-```
+```text
 ┌─res──────────┐
 │ [1, 2, 3, 4] │
 └──────────────┘
@@ -95,11 +95,11 @@ The [Schwartzian transform](https://en.wikipedia.org/wiki/Schwartzian_transform)
 
 Example:
 
-``` sql
+```sql
 SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]);
 ```
 
-```
+```text
 ┌─res────────────────┐
 │ ['world', 'hello'] │
 └────────────────────┘
@@ -113,5 +113,3 @@ Returns an array as result of sorting the elements of `arr1` in descending order
 
 
  
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/higher_order_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/in_functions.md b/docs/en/query_language/functions/in_functions.md
index b9295cac7..27ac483cd 100644
--- a/docs/en/query_language/functions/in_functions.md
+++ b/docs/en/query_language/functions/in_functions.md
@@ -16,5 +16,3 @@ A function that allows getting a column from a tuple.
 'N' is the column index, starting from 1. N must be a constant. 'N' must be a constant. 'N' must be a strict postive integer no greater than the size of the tuple.
 There is no cost to execute the function.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/in_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/index.md b/docs/en/query_language/functions/index.md
index 2e3c46779..3700a2779 100644
--- a/docs/en/query_language/functions/index.md
+++ b/docs/en/query_language/functions/index.md
@@ -63,5 +63,3 @@ Another example is the `hostName` function, which returns the name of the server
 
 If a function in a query is performed on the requestor server, but you need to perform it on remote servers, you can wrap it in an 'any' aggregate function or add it to a key in `GROUP BY`.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/ip_address_functions.md b/docs/en/query_language/functions/ip_address_functions.md
index 27e1290c6..f83f75319 100644
--- a/docs/en/query_language/functions/ip_address_functions.md
+++ b/docs/en/query_language/functions/ip_address_functions.md
@@ -14,7 +14,7 @@ Similar to IPv4NumToString, but using xxx instead of the last octet.
 
 Example:
 
-``` sql
+```sql
 SELECT
     IPv4NumToStringClassC(ClientIP) AS k,
     count() AS c
@@ -24,7 +24,7 @@ ORDER BY c DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─k──────────────┬─────c─┐
 │ 83.149.9.xxx   │ 26238 │
 │ 217.118.81.xxx │ 26074 │
@@ -46,17 +46,17 @@ Since using 'xxx' is highly unusual, this may be changed in the future. We recom
 Accepts a FixedString(16) value containing the IPv6 address in binary format. Returns a string containing this address in text format.
 IPv6-mapped IPv4 addresses are output in the format ::ffff:111.222.33.44. Examples:
 
-``` sql
+```sql
 SELECT IPv6NumToString(toFixedString(unhex('2A0206B8000000000000000000000011'), 16)) AS addr
 ```
 
-```
+```text
 ┌─addr─────────┐
 │ 2a02:6b8::11 │
 └──────────────┘
 ```
 
-``` sql
+```sql
 SELECT
     IPv6NumToString(ClientIP6 AS k),
     count() AS c
@@ -67,7 +67,7 @@ ORDER BY c DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─IPv6NumToString(ClientIP6)──────────────┬─────c─┐
 │ 2a02:2168:aaa:bbbb::2                   │ 24695 │
 │ 2a02:2698:abcd:abcd:abcd:abcd:8888:5555 │ 22408 │
@@ -82,7 +82,7 @@ LIMIT 10
 └─────────────────────────────────────────┴───────┘
 ```
 
-``` sql
+```sql
 SELECT
     IPv6NumToString(ClientIP6 AS k),
     count() AS c
@@ -93,7 +93,7 @@ ORDER BY c DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─IPv6NumToString(ClientIP6)─┬──────c─┐
 │ ::ffff:94.26.111.111       │ 747440 │
 │ ::ffff:37.143.222.4        │ 529483 │
@@ -113,5 +113,3 @@ LIMIT 10
 The reverse function of IPv6NumToString. If the IPv6 address has an invalid format, it returns a string of null bytes.
 HEX can be uppercase or lowercase.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/ip_address_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/json_functions.md b/docs/en/query_language/functions/json_functions.md
index f28b32969..70f66d86b 100644
--- a/docs/en/query_language/functions/json_functions.md
+++ b/docs/en/query_language/functions/json_functions.md
@@ -35,7 +35,7 @@ Returns the value of a field, including separators.
 
 Examples:
 
-```
+```text
 visitParamExtractRaw('{"abc":"\\n\\u0000"}', 'abc') = '"\\n\\u0000"'
 visitParamExtractRaw('{"abc":{"def":[1,2,3]}}', 'abc') = '{"def":[1,2,3]}'
 ```
@@ -46,7 +46,7 @@ Parses the string in double quotes. The value is unescaped. If unescaping failed
 
 Examples:
 
-```
+```text
 visitParamExtractString('{"abc":"\\n\\u0000"}', 'abc') = '\n\0'
 visitParamExtractString('{"abc":"\\u263a"}', 'abc') = '☺'
 visitParamExtractString('{"abc":"\\u263"}', 'abc') = ''
@@ -55,5 +55,3 @@ visitParamExtractString('{"abc":"hello}', 'abc') = ''
 
 There is currently no support for code points in the format `\uXXXX\uYYYY` that are not from the basic multilingual plane (they are converted to CESU-8 instead of UTF-8).
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/json_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/logical_functions.md b/docs/en/query_language/functions/logical_functions.md
index 45c722f52..4ef0fe5fd 100644
--- a/docs/en/query_language/functions/logical_functions.md
+++ b/docs/en/query_language/functions/logical_functions.md
@@ -12,5 +12,3 @@ Zero as an argument is considered "false," while any non-zero value is considere
 
 ## xor
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/logical_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/math_functions.md b/docs/en/query_language/functions/math_functions.md
index af4c9a301..0ae7eb842 100644
--- a/docs/en/query_language/functions/math_functions.md
+++ b/docs/en/query_language/functions/math_functions.md
@@ -48,11 +48,11 @@ If 'x' is non-negative, then erf(x / σ√2)<g> is the probability that a random
 
 Example (three sigma rule):
 
-``` sql
+```sql
 SELECT erf(3 / sqrt(2))
 ```
 
-```
+```text
 ┌─erf(divide(3, sqrt(2)))─┐
 │      0.9973002039367398 │
 └─────────────────────────┘
@@ -97,5 +97,3 @@ The arc tangent.
 ## pow(x, y)
 
 Takes two numeric arguments x and y. Returns a Float64 number close to x to the power of y.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/math_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/other_functions.md b/docs/en/query_language/functions/other_functions.md
index a8090fc30..f3299bd57 100644
--- a/docs/en/query_language/functions/other_functions.md
+++ b/docs/en/query_language/functions/other_functions.md
@@ -83,7 +83,7 @@ The band is drawn with accuracy to one eighth of a symbol.
 
 Example:
 
-``` sql
+```sql
 SELECT
     toHour(EventTime) AS h,
     count() AS c,
@@ -93,7 +93,7 @@ GROUP BY h
 ORDER BY h ASC
 ```
 
-```
+```text
 ┌──h─┬──────c─┬─bar────────────────┐
 │  0 │ 292907 │ █████████▋         │
 │  1 │ 180563 │ ██████             │
@@ -153,7 +153,7 @@ If the 'x' value is equal to one of the elements in the 'array_from' array, it r
 
 Example:
 
-``` sql
+```sql
 SELECT
     transform(SearchEngineID, [2, 3], ['Yandex', 'Google'], 'Other') AS title,
     count() AS c
@@ -163,7 +163,7 @@ GROUP BY title
 ORDER BY c DESC
 ```
 
-```
+```text
 ┌─title─────┬──────c─┐
 │ Yandex    │ 498635 │
 │ Google    │ 229872 │
@@ -182,7 +182,7 @@ Types:
 
 Example:
 
-``` sql
+```sql
 SELECT
     transform(domain(Referer), ['yandex.ru', 'google.ru', 'vk.com'], ['www.yandex', 'example.com']) AS s,
     count() AS c
@@ -192,7 +192,7 @@ ORDER BY count() DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─s──────────────┬───────c─┐
 │                │ 2906259 │
 │ www.yandex     │  867767 │
@@ -212,13 +212,13 @@ Accepts the size (number of bytes). Returns a rounded size with a suffix (KiB, M
 
 Example:
 
-``` sql
+```sql
 SELECT
     arrayJoin([1, 1024, 1024*1024, 192851925]) AS filesize_bytes,
     formatReadableSize(filesize_bytes) AS filesize
 ```
 
-```
+```text
 ┌─filesize_bytes─┬─filesize───┐
 │              1 │ 1.00 B     │
 │           1024 │ 1.00 KiB   │
@@ -257,7 +257,7 @@ If you make a subquery with ORDER BY and call the function from outside the subq
 
 Example:
 
-``` sql
+```sql
 SELECT
     EventID,
     EventTime,
@@ -274,7 +274,7 @@ FROM
 )
 ```
 
-```
+```text
 ┌─EventID─┬───────────EventTime─┬─delta─┐
 │    1106 │ 2016-11-24 00:00:04 │     0 │
 │    1107 │ 2016-11-24 00:00:05 │     1 │
@@ -559,5 +559,3 @@ SELECT replicate(1, ['a', 'b', 'c'])
 └───────────────────────────────┘
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/other_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/random_functions.md b/docs/en/query_language/functions/random_functions.md
index eca7e3279..915155f4a 100644
--- a/docs/en/query_language/functions/random_functions.md
+++ b/docs/en/query_language/functions/random_functions.md
@@ -16,5 +16,3 @@ Uses a linear congruential generator.
 Returns a pseudo-random UInt64 number, evenly distributed among all UInt64-type numbers.
 Uses a linear congruential generator.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/random_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/rounding_functions.md b/docs/en/query_language/functions/rounding_functions.md
index 17407aee8..916dd0a50 100644
--- a/docs/en/query_language/functions/rounding_functions.md
+++ b/docs/en/query_language/functions/rounding_functions.md
@@ -31,7 +31,7 @@ The rounded number of the same type as the input number `x`
 
 **Example:**
 
-``` sql
+```sql
 SELECT
     number / 2 AS x,
     round(x)
@@ -66,5 +66,3 @@ Accepts a number. If the number is less than one, it returns 0. Otherwise, it ro
 
 Accepts a number. If the number is less than 18, it returns 0. Otherwise, it rounds the number down to a number from the set: 18, 25, 35, 45, 55. This function is specific to Yandex.Metrica and used for implementing the report on user age.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/rounding_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/splitting_merging_functions.md b/docs/en/query_language/functions/splitting_merging_functions.md
index 0e1cf98ee..e2fb62ac4 100644
--- a/docs/en/query_language/functions/splitting_merging_functions.md
+++ b/docs/en/query_language/functions/splitting_merging_functions.md
@@ -26,5 +26,4 @@ SELECT alphaTokens('abca1abc')
 ┌─alphaTokens('abca1abc')─┐
 │ ['abca','abc']          │
 └─────────────────────────┘
-```
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/splitting_merging_functions/) <!--hide-->
+```
\ No newline at end of file
diff --git a/docs/en/query_language/functions/string_functions.md b/docs/en/query_language/functions/string_functions.md
index 251fbd53a..61e33040a 100644
--- a/docs/en/query_language/functions/string_functions.md
+++ b/docs/en/query_language/functions/string_functions.md
@@ -74,5 +74,3 @@ If the 's' string is non-empty and does not contain the 'c' character at the end
 
 Returns the string 's' that was converted from the encoding in 'from' to the encoding in 'to'.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/string_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/string_replace_functions.md b/docs/en/query_language/functions/string_replace_functions.md
index 400e4a7ef..d37735042 100644
--- a/docs/en/query_language/functions/string_replace_functions.md
+++ b/docs/en/query_language/functions/string_replace_functions.md
@@ -19,7 +19,7 @@ Also keep in mind that a string literal requires an extra escape.
 
 Example 1. Converting the date to American format:
 
-``` sql
+```sql
 SELECT DISTINCT
     EventDate,
     replaceRegexpOne(toString(EventDate), '(\\d{4})-(\\d{2})-(\\d{2})', '\\2/\\3/\\1') AS res
@@ -28,7 +28,7 @@ LIMIT 7
 FORMAT TabSeparated
 ```
 
-```
+```text
 2014-03-17      03/17/2014
 2014-03-18      03/18/2014
 2014-03-19      03/19/2014
@@ -40,11 +40,11 @@ FORMAT TabSeparated
 
 Example 2. Copying a string ten times:
 
-``` sql
+```sql
 SELECT replaceRegexpOne('Hello, World!', '.*', '\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0') AS res
 ```
 
-```
+```text
 ┌─res────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
 │ Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World! │
 └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
@@ -54,11 +54,11 @@ SELECT replaceRegexpOne('Hello, World!', '.*', '\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0')
 
 This does the same thing, but replaces all the occurrences. Example:
 
-``` sql
+```sql
 SELECT replaceRegexpAll('Hello, World!', '.', '\\0\\0') AS res
 ```
 
-```
+```text
 ┌─res────────────────────────┐
 │ HHeelllloo,,  WWoorrlldd!! │
 └────────────────────────────┘
@@ -67,15 +67,13 @@ SELECT replaceRegexpAll('Hello, World!', '.', '\\0\\0') AS res
 As an exception, if a regular expression worked on an empty substring, the replacement is not made more than once.
 Example:
 
-``` sql
+```sql
 SELECT replaceRegexpAll('Hello, World!', '^', 'here: ') AS res
 ```
 
-```
+```text
 ┌─res─────────────────┐
 │ here: Hello, World! │
 └─────────────────────┘
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/string_replace_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/string_search_functions.md b/docs/en/query_language/functions/string_search_functions.md
index 182152e36..a038162c0 100644
--- a/docs/en/query_language/functions/string_search_functions.md
+++ b/docs/en/query_language/functions/string_search_functions.md
@@ -52,5 +52,3 @@ For other regular expressions, the code is the same as for the 'match' function.
 
 The same thing as 'like', but negative.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/string_search_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/type_conversion_functions.md b/docs/en/query_language/functions/type_conversion_functions.md
index c4b4099d7..02014987e 100644
--- a/docs/en/query_language/functions/type_conversion_functions.md
+++ b/docs/en/query_language/functions/type_conversion_functions.md
@@ -24,7 +24,7 @@ When converting dates with times to numbers or vice versa, the date with time co
 
 The date and date-with-time formats for the toDate/toDateTime functions are defined as follows:
 
-```
+```text
 YYYY-MM-DD
 YYYY-MM-DD hh:mm:ss
 ```
@@ -37,13 +37,13 @@ Conversion between numeric types uses the same rules as assignments between diff
 
 Additionally, the toString function of the DateTime argument can take a second String argument containing the name of the time zone. Example: `Asia/Yekaterinburg` In this case, the time is formatted according to the specified time zone.
 
-``` sql
+```sql
 SELECT
     now() AS now_local,
     toString(now(), 'Asia/Yekaterinburg') AS now_yekat
 ```
 
-```
+```text
 ┌───────────now_local─┬─now_yekat───────────┐
 │ 2016-06-15 00:11:21 │ 2016-06-15 02:11:21 │
 └─────────────────────┴─────────────────────┘
@@ -62,21 +62,21 @@ Accepts a String or FixedString argument. Returns the String with the content tr
 
 Example:
 
-``` sql
+```sql
 SELECT toFixedString('foo', 8) AS s, toStringCutToZero(s) AS s_cut
 ```
 
-```
+```text
 ┌─s─────────────┬─s_cut─┐
 │ foo\0\0\0\0\0 │ foo   │
 └───────────────┴───────┘
 ```
 
-``` sql
+```sql
 SELECT toFixedString('foo\0bar', 8) AS s, toStringCutToZero(s) AS s_cut
 ```
 
-```
+```text
 ┌─s──────────┬─s_cut─┐
 │ foo\0bar\0 │ foo   │
 └────────────┴───────┘
@@ -102,7 +102,7 @@ Converts 'x' to the 't' data type. The syntax CAST(x AS t) is also supported.
 
 Example:
 
-``` sql
+```sql
 SELECT
     '2016-06-15 23:00:00' AS timestamp,
     CAST(timestamp AS DateTime) AS datetime,
@@ -111,7 +111,7 @@ SELECT
     CAST(timestamp, 'FixedString(22)') AS fixed_string
 ```
 
-```
+```text
 ┌─timestamp───────────┬────────────datetime─┬───────date─┬─string──────────────┬─fixed_string──────────────┐
 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00\0\0\0 │
 └─────────────────────┴─────────────────────┴────────────┴─────────────────────┴───────────────────────────┘
@@ -137,5 +137,3 @@ SELECT toTypeName(CAST(x, 'Nullable(UInt16)')) FROM t_null
 └─────────────────────────────────────────┘
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/type_conversion_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/url_functions.md b/docs/en/query_language/functions/url_functions.md
index efe489703..ef0c7e081 100644
--- a/docs/en/query_language/functions/url_functions.md
+++ b/docs/en/query_language/functions/url_functions.md
@@ -72,7 +72,7 @@ Returns an array containing the URL, truncated at the end by the symbols /,? in
 
 The same as above, but without the protocol and host in the result. The / element (root) is not included. Example: the function is used to implement tree reports the URL in Yandex. Metric.
 
-```
+```text
 URLPathHierarchy('https://example.com/browse/CONV-6788') =
 [
     '/browse/',
@@ -85,11 +85,11 @@ URLPathHierarchy('https://example.com/browse/CONV-6788') =
 Returns the decoded URL.
 Example:
 
-``` sql
+```sql
 SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS DecodedURL;
 ```
 
-```
+```text
 ┌─DecodedURL─────────────────────────────┐
 │ http://127.0.0.1:8123/?query=SELECT 1; │
 └────────────────────────────────────────┘
@@ -119,5 +119,3 @@ Removes the query string and fragment identifier. The question mark and number s
 
 Removes the 'name' URL parameter, if present. This function works under the assumption that the parameter name is encoded in the URL exactly the same way as in the passed argument.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/url_functions/) <!--hide-->
diff --git a/docs/en/query_language/functions/ym_dict_functions.md b/docs/en/query_language/functions/ym_dict_functions.md
index 8039c77ed..7ba7e7012 100644
--- a/docs/en/query_language/functions/ym_dict_functions.md
+++ b/docs/en/query_language/functions/ym_dict_functions.md
@@ -20,7 +20,7 @@ All the dictionaries are re-loaded in runtime (once every certain number of seco
 All functions for working with regions have an optional argument at the end – the dictionary key. It is referred to as the geobase.
 Example:
 
-```
+```text
 regionToCountry(RegionID) – Uses the default dictionary: /opt/geo/regions_hierarchy.txt
 regionToCountry(RegionID, '') – Uses the default dictionary: /opt/geo/regions_hierarchy.txt
 regionToCountry(RegionID, 'ua') – Uses the dictionary for the 'ua' key: /opt/geo/regions_hierarchy_ua.txt
@@ -34,13 +34,13 @@ Accepts a UInt32 number – the region ID from the Yandex geobase. If this regio
 
 Converts a region to an area (type 5 in the geobase). In every other way, this function is the same as 'regionToCity'.
 
-``` sql
+```sql
 SELECT DISTINCT regionToName(regionToArea(toUInt32(number), 'ua'))
 FROM system.numbers
 LIMIT 15
 ```
 
-```
+```text
 ┌─regionToName(regionToArea(toUInt32(number), \'ua\'))─┐
 │                                                      │
 │ Moscow and Moscow region                             │
@@ -64,13 +64,13 @@ LIMIT 15
 
 Converts a region to a federal district (type 4 in the geobase). In every other way, this function is the same as 'regionToCity'.
 
-``` sql
+```sql
 SELECT DISTINCT regionToName(regionToDistrict(toUInt32(number), 'ua'))
 FROM system.numbers
 LIMIT 15
 ```
 
-```
+```text
 ┌─regionToName(regionToDistrict(toUInt32(number), \'ua\'))─┐
 │                                                          │
 │ Central federal district                                 │
@@ -123,5 +123,3 @@ Accepts a UInt32 number – the region ID from the Yandex geobase. A string with
 
 `ua` and `uk` both mean Ukrainian.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/functions/ym_dict_functions/) <!--hide-->
diff --git a/docs/en/query_language/index.md b/docs/en/query_language/index.md
index bc8b8cfd8..dcb59cbe6 100644
--- a/docs/en/query_language/index.md
+++ b/docs/en/query_language/index.md
@@ -6,5 +6,3 @@
 * [ALTER](alter.md#query_language_queries_alter)
 * [Other types of queries](misc.md#miscellanous-queries)
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/) <!--hide-->
diff --git a/docs/en/query_language/insert_into.md b/docs/en/query_language/insert_into.md
index d34e30677..99e111235 100644
--- a/docs/en/query_language/insert_into.md
+++ b/docs/en/query_language/insert_into.md
@@ -6,7 +6,7 @@ Adding data.
 
 Basic query format:
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...
 ```
 
@@ -19,13 +19,13 @@ If [strict_insert_defaults=1](../operations/settings/settings.md#settings-strict
 
 Data can be passed to the INSERT in any [format](../interfaces/formats.md#formats) supported by ClickHouse. The format must be specified explicitly in the query:
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] FORMAT format_name data_set
 ```
 
 For example, the following query format is identical to the basic version of INSERT ... VALUES:
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] FORMAT Values (v11, v12, v13), (v21, v22, v23), ...
 ```
 
@@ -33,7 +33,7 @@ ClickHouse removes all spaces and one line feed (if there is one) before the dat
 
 Example:
 
-``` sql
+```sql
 INSERT INTO t FORMAT TabSeparated
 11  Hello, world!
 22  Qwerty
@@ -43,7 +43,7 @@ You can insert data separately from the query by using the command-line client o
 
 ### Inserting The Results of `SELECT`
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...
 ```
 
@@ -66,5 +66,3 @@ Performance will not decrease if:
 - Data is added in real time.
 - You upload data that is usually sorted by time.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/insert_into/) <!--hide-->
diff --git a/docs/en/query_language/misc.md b/docs/en/query_language/misc.md
index e81deca4b..253c1d59e 100644
--- a/docs/en/query_language/misc.md
+++ b/docs/en/query_language/misc.md
@@ -10,7 +10,7 @@ After executing an ATTACH query, the server will know about the existence of the
 
 If the table was previously detached (``DETACH``), meaning that its structure is known, you can use shorthand without defining the structure.
 
-``` sql
+```sql
 ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
 ```
 
@@ -20,14 +20,14 @@ This query is used when starting the server. The server stores table metadata as
 
 This query has two types: `DROP DATABASE`  and `DROP TABLE`.
 
-``` sql
+```sql
 DROP DATABASE [IF EXISTS] db [ON CLUSTER cluster]
 ```
 
 Deletes all tables inside the 'db' database, then deletes the 'db' database itself.
 If `IF EXISTS` is specified, it doesn't return an error if the database doesn't exist.
 
-``` sql
+```sql
 DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
 ```
 
@@ -38,7 +38,7 @@ If `IF EXISTS` is specified, it doesn't return an error if the table doesn't exi
 
 Deletes information about the 'name' table from the server. The server stops knowing about the table's existence.
 
-``` sql
+```sql
 DETACH TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
 ```
 
@@ -51,7 +51,7 @@ There is no `DETACH DATABASE` query.
 
 Renames one or more tables.
 
-``` sql
+```sql
 RENAME TABLE [db11.]name11 TO [db12.]name12, [db21.]name21 TO [db22.]name22, ... [ON CLUSTER cluster]
 ```
 
@@ -59,7 +59,7 @@ All tables are renamed under global locking. Renaming tables is a light operatio
 
 ## SHOW DATABASES
 
-``` sql
+```sql
 SHOW DATABASES [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -70,7 +70,7 @@ See also the section "Formats".
 
 ## SHOW TABLES
 
-``` sql
+```sql
 SHOW [TEMPORARY] TABLES [FROM db] [LIKE 'pattern'] [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -85,7 +85,7 @@ See also the section "LIKE operator".
 
 ## SHOW PROCESSLIST
 
-``` sql
+```sql
 SHOW PROCESSLIST [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -117,7 +117,7 @@ watch -n1 "clickhouse-client --query='SHOW PROCESSLIST'"
 
 ## SHOW CREATE TABLE
 
-``` sql
+```sql
 SHOW CREATE [TEMPORARY] TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -125,7 +125,7 @@ Returns a single `String`-type 'statement' column, which contains a single value
 
 ## DESCRIBE TABLE
 
-``` sql
+```sql
 DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -135,7 +135,7 @@ Nested data structures are output in "expanded" format. Each column is shown sep
 
 ## EXISTS
 
-``` sql
+```sql
 EXISTS [TEMPORARY] TABLE [db.]name [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -143,7 +143,7 @@ Returns a single `UInt8`-type column, which contains the single value `0` if the
 
 ## USE
 
-``` sql
+```sql
 USE db
 ```
 
@@ -153,7 +153,7 @@ This query can't be made when using the HTTP protocol, since there is no concept
 
 ## SET
 
-``` sql
+```sql
 SET param = value
 ```
 
@@ -166,7 +166,7 @@ To make settings that persist after a server restart, you can only use the serve
 
 ## OPTIMIZE
 
-``` sql
+```sql
 OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition] [FINAL]
 ```
 
@@ -180,7 +180,7 @@ If you specify `FINAL`, optimization will be performed even when all the data is
 
 ## KILL QUERY
 
-``` sql
+```sql
 KILL QUERY [ON CLUSTER cluster]
   WHERE <where expression to SELECT FROM system.processes query>
   [SYNC|ASYNC|TEST]
@@ -192,7 +192,7 @@ The queries to terminate are selected from the system.processes table using the
 
 Examples:
 
-``` sql
+```sql
 -- Forcibly terminates all queries with the specified query_id:
 KILL QUERY WHERE query_id='2-857d-4a57-9ee0-327da5d60a90'
 
@@ -212,5 +212,3 @@ The response contains the `kill_status` column, which can take the following val
 3. The other values ​​explain why the query can't be stopped.
 
 A test query (`TEST`) only checks the user's rights and displays a list of queries to stop.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/misc/) <!--hide-->
diff --git a/docs/en/query_language/operators.md b/docs/en/query_language/operators.md
index 613dc50a5..5d3b6df4c 100644
--- a/docs/en/query_language/operators.md
+++ b/docs/en/query_language/operators.md
@@ -87,7 +87,7 @@ The conditional operator calculates the values of b and c, then checks whether c
 
 ## Conditional Expression
 
-``` sql
+```sql
 CASE [x]
     WHEN a THEN b
     [WHEN ... THEN ...]
@@ -171,5 +171,3 @@ WHERE isNotNull(y)
 
 1 rows in set. Elapsed: 0.002 sec.
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/operators/) <!--hide-->
diff --git a/docs/en/query_language/select.md b/docs/en/query_language/select.md
index 53164e67d..39f261ec2 100644
--- a/docs/en/query_language/select.md
+++ b/docs/en/query_language/select.md
@@ -2,7 +2,7 @@
 
 `SELECT` performs data retrieval.
 
-``` sql
+```sql
 SELECT [DISTINCT] expr_list
     [FROM [db.]table | (subquery) | table_function] [FINAL]
     [SAMPLE sample_coeff]
@@ -55,7 +55,7 @@ In the second case, the query will be executed on a sample of no more than 'n' r
 
 Example:
 
-``` sql
+```sql
 SELECT
     Title,
     count() * 10 AS PageViews
@@ -86,7 +86,7 @@ Allows executing JOIN with an array or nested data structure. The intent is simi
 
 `ARRAY JOIN` is essentially `INNER JOIN` with an array. Example:
 
-```
+```text
 :) CREATE TABLE arrays_test (s String, arr Array(UInt8)) ENGINE = Memory
 
 CREATE TABLE arrays_test
@@ -139,7 +139,7 @@ ARRAY JOIN arr
 
 An alias can be specified for an array in the ARRAY JOIN clause. In this case, an array item can be accessed by this alias, but the array itself by the original name. Example:
 
-```
+```text
 :) SELECT s, arr, a FROM arrays_test ARRAY JOIN arr AS a
 
 SELECT s, arr, a
@@ -159,7 +159,7 @@ ARRAY JOIN arr AS a
 
 Multiple arrays of the same size can be comma-separated in the ARRAY JOIN clause. In this case, JOIN is performed with them simultaneously (the direct sum, not the direct product). Example:
 
-```
+```text
 :) SELECT s, arr, a, num, mapped FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -> x + 1, arr) AS mapped
 
 SELECT s, arr, a, num, mapped
@@ -195,7 +195,7 @@ ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num
 
 ARRAY JOIN also works with nested data structures. Example:
 
-```
+```text
 :) CREATE TABLE nested_test (s String, nest Nested(x UInt8, y UInt32)) ENGINE = Memory
 
 CREATE TABLE nested_test
@@ -250,7 +250,7 @@ ARRAY JOIN nest
 
 When specifying names of nested data structures in ARRAY JOIN, the meaning is the same as ARRAY JOIN with all the array elements that it consists of. Example:
 
-```
+```text
 :) SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x, nest.y
 
 SELECT s, `nest.x`, `nest.y`
@@ -270,7 +270,7 @@ ARRAY JOIN `nest.x`, `nest.y`
 
 This variation also makes sense:
 
-```
+```text
 :) SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x
 
 SELECT s, `nest.x`, `nest.y`
@@ -290,7 +290,7 @@ ARRAY JOIN `nest.x`
 
 An alias may be used for a nested data structure, in order to select either the JOIN result or the source array. Example:
 
-```
+```text
 :) SELECT s, n.x, n.y, nest.x, nest.y FROM nested_test ARRAY JOIN nest AS n
 
 SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`
@@ -310,7 +310,7 @@ ARRAY JOIN nest AS n
 
 Example of using the arrayEnumerate function:
 
-```
+```text
 :) SELECT s, n.x, n.y, nest.x, nest.y, num FROM nested_test ARRAY JOIN nest AS n, arrayEnumerate(nest.x) AS num
 
 SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`, num
@@ -336,7 +336,7 @@ The corresponding conversion can be performed before the WHERE/PREWHERE clause (
 
 The normal JOIN, which is not related to ARRAY JOIN described above.
 
-``` sql
+```sql
 [GLOBAL] ANY|ALL INNER|LEFT [OUTER] JOIN (subquery)|table USING columns_list
 ```
 
@@ -371,7 +371,7 @@ When running a JOIN, there is no optimization of the order of execution in relat
 
 Example:
 
-``` sql
+```sql
 SELECT
     CounterID,
     hits,
@@ -395,7 +395,7 @@ ORDER BY hits DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─CounterID─┬───hits─┬─visits─┐
 │   1143050 │ 523264 │  13665 │
 │    731962 │ 475698 │ 102716 │
@@ -469,7 +469,7 @@ If a query contains only table columns inside aggregate functions, the GROUP BY
 
 Example:
 
-``` sql
+```sql
 SELECT
     count(),
     median(FetchTiming > 60 ? 60 : FetchTiming),
@@ -483,7 +483,7 @@ As opposed to MySQL (and conforming to standard SQL), you can't get some value o
 
 Example:
 
-``` sql
+```sql
 SELECT
     domainWithoutWWW(URL) AS domain,
     count(),
@@ -579,7 +579,7 @@ LIMIT N BY COLUMNS selects the top N rows for each group of COLUMNS. LIMIT N BY
 
 Example:
 
-``` sql
+```sql
 SELECT
     domainWithoutWWW(URL) AS domain,
     domainWithoutWWW(REFERRER_URL) AS referrer,
@@ -698,7 +698,7 @@ If there isn't an ORDER BY clause that explicitly sorts results, the result may
 
 You can use UNION ALL to combine any number of queries. Example:
 
-``` sql
+```sql
 SELECT CounterID, 1 AS table, toInt64(count()) AS c
     FROM test.hits
     GROUP BY CounterID
@@ -746,7 +746,7 @@ The left side of the operator is either a single column or a tuple.
 
 Examples:
 
-``` sql
+```sql
 SELECT UserID IN (123, 456) FROM ...
 SELECT (CounterID, UserID) IN ((34, 123), (101500, 456)) FROM ...
 ```
@@ -764,7 +764,7 @@ If the right side of the operator is a table name that has the Set engine (a pre
 The subquery may specify more than one column for filtering tuples.
 Example:
 
-``` sql
+```sql
 SELECT (CounterID, UserID) IN (SELECT CounterID, UserID FROM ...) FROM ...
 ```
 
@@ -773,7 +773,7 @@ The columns to the left and right of the IN operator should have the same type.
 The IN operator and subquery may occur in any part of the query, including in aggregate functions and lambda functions.
 Example:
 
-``` sql
+```sql
 SELECT
     EventDate,
     avg(UserID IN
@@ -787,7 +787,7 @@ GROUP BY EventDate
 ORDER BY EventDate ASC
 ```
 
-```
+```text
 ┌──EventDate─┬────ratio─┐
 │ 2014-03-17 │        1 │
 │ 2014-03-18 │ 0.807696 │
@@ -858,13 +858,13 @@ For a query to the **distributed_table**, the query will be sent to all the remo
 
 For example, the query
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table
 ```
 
 will be sent to all remote servers as
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table
 ```
 
@@ -872,7 +872,7 @@ and run on each of them in parallel, until it reaches the stage where intermedia
 
 Now let's examine a query with IN:
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
 ```
 
@@ -880,7 +880,7 @@ SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID I
 
 This query will be sent to all remote servers as
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
 ```
 
@@ -890,19 +890,19 @@ This will work correctly and optimally if you are prepared for this case and hav
 
 To correct how the query works when data is spread randomly across the cluster servers, you could specify **distributed_table** inside a subquery. The query would look like this:
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
 ```
 
 This query will be sent to all remote servers as
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
 ```
 
 The subquery will begin running on each remote server. Since the subquery uses a distributed table, the subquery that is on each remote server will be resent to every remote server as
 
-``` sql
+```sql
 SELECT UserID FROM local_table WHERE CounterID = 34
 ```
 
@@ -910,19 +910,19 @@ For example, if you have a cluster of 100 servers, executing the entire query wi
 
 In such cases, you should always use GLOBAL IN instead of IN. Let's look at how it works for the query
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
 ```
 
 The requestor server will run the subquery
 
-``` sql
+```sql
 SELECT UserID FROM distributed_table WHERE CounterID = 34
 ```
 
 and the result will be put in a temporary table in RAM. Then the request will be sent to each remote server as
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL IN _data1
 ```
 
@@ -964,5 +964,3 @@ You can put an asterisk in any part of a query instead of an expression. When th
 - In subqueries (since columns that aren't needed for the external query are excluded from subqueries).
 
 In all other cases, we don't recommend using the asterisk, since it only gives you the drawbacks of a columnar DBMS instead of the advantages. In other words using the asterisk is not recommended.
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/select/) <!--hide-->
diff --git a/docs/en/query_language/syntax.md b/docs/en/query_language/syntax.md
index 548a4771a..11ffb2e6a 100644
--- a/docs/en/query_language/syntax.md
+++ b/docs/en/query_language/syntax.md
@@ -4,7 +4,7 @@ There are two types of parsers in the system: the full SQL parser (a recursive d
 In all cases except the INSERT query, only the full SQL parser is used.
 The INSERT query uses both parsers:
 
-``` sql
+```sql
 INSERT INTO t VALUES (1, 'Hello, world'), (2, 'abc'), (3, 'def')
 ```
 
@@ -103,7 +103,7 @@ Data types and table engines in the `CREATE` query are written the same way as i
 
 In the SELECT query, expressions can specify synonyms using the AS keyword. Any expression is placed to the left of AS. The identifier name for the synonym is placed to the right of AS. As opposed to standard SQL, synonyms are not only declared on the top level of expressions:
 
-``` sql
+```sql
 SELECT (1 AS n) + 2, n
 ```
 
@@ -119,5 +119,3 @@ An expression is a function, identifier, literal, application of an operator, ex
 A list of expressions is one or more expressions separated by commas.
 Functions and operators, in turn, can have expressions as arguments.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/syntax/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/file.md b/docs/en/query_language/table_functions/file.md
index 8d1c336b0..67eb57429 100644
--- a/docs/en/query_language/table_functions/file.md
+++ b/docs/en/query_language/table_functions/file.md
@@ -12,9 +12,7 @@ structure -  table structure in 'UserID UInt64, URL String' format. Determines c
 
 **Example**
 
-``` sql
+```sql
 -- getting the first 10 lines of a table that contains 3 columns of UInt32 type from a CSV file
 SELECT * FROM file('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32') LIMIT 10
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/file/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/index.md b/docs/en/query_language/table_functions/index.md
index f694c5e35..fd479dd3f 100644
--- a/docs/en/query_language/table_functions/index.md
+++ b/docs/en/query_language/table_functions/index.md
@@ -4,5 +4,3 @@ Table functions can be specified in the FROM clause instead of the database and
 Table functions can only be used if 'readonly' is not set.
 Table functions aren't related to other functions.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/jdbc.md b/docs/en/query_language/table_functions/jdbc.md
deleted file mode 100644
index 240e12551..000000000
--- a/docs/en/query_language/table_functions/jdbc.md
+++ /dev/null
@@ -1,25 +0,0 @@
-<a name="table_functions-jdbc"></a>
-
-# jdbc
-
-`jdbc(jdbc_connection_uri, schema, table)` - returns table that is connected via JDBC driver.
-
-This table function requires separate `clickhouse-jdbc-bridge` program to be running.
-It supports Nullable types (based on DDL of remote table that is queried).
-
-
-**Examples**
-
-``` sql
-SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
-```
-
-``` sql
-SELECT * FROM jdbc('mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
-```
-
-``` sql
-SELECT * FROM jdbc('datasource://mysql-local', 'schema', 'table')
-```
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/jdbc/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/merge.md b/docs/en/query_language/table_functions/merge.md
index 37a206d36..d1ddc12dc 100644
--- a/docs/en/query_language/table_functions/merge.md
+++ b/docs/en/query_language/table_functions/merge.md
@@ -4,5 +4,3 @@
 
 The table structure is taken from the first table encountered that matches the regular expression.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/merge/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/numbers.md b/docs/en/query_language/table_functions/numbers.md
index e8b025d92..4486fece3 100644
--- a/docs/en/query_language/table_functions/numbers.md
+++ b/docs/en/query_language/table_functions/numbers.md
@@ -7,7 +7,7 @@ Similar to the `system.numbers` table, it can be used for testing and generating
 
 The following queries are equivalent:
 
-``` sql
+```sql
 SELECT * FROM numbers(10);
 SELECT * FROM numbers(0, 10);
 SELECT * FROM system.numbers LIMIT 10;
@@ -15,10 +15,8 @@ SELECT * FROM system.numbers LIMIT 10;
 
 Examples:
 
-``` sql
+```sql
 -- Generate a sequence of dates from 2010-01-01 to 2010-12-31
 select toDate('2010-01-01') + number as d FROM numbers(365);
 ```
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/numbers/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/remote.md b/docs/en/query_language/table_functions/remote.md
index 834dbf30b..425c6f81a 100644
--- a/docs/en/query_language/table_functions/remote.md
+++ b/docs/en/query_language/table_functions/remote.md
@@ -6,7 +6,7 @@ Allows you to access remote servers without creating a `Distributed` table.
 
 Signatures:
 
-``` sql
+```sql
 remote('addresses_expr', db, table[, 'user'[, 'password']])
 remote('addresses_expr', db.table[, 'user'[, 'password']])
 ```
@@ -18,7 +18,7 @@ remote('addresses_expr', db.table[, 'user'[, 'password']])
 
 Examples:
 
-```
+```text
 example01-01-1
 example01-01-1:9000
 localhost
@@ -31,19 +31,19 @@ Multiple addresses can be comma-separated. In this case, ClickHouse will use dis
 
 Example:
 
-```
+```text
 example01-01-1,example01-02-1
 ```
 
 Part of the expression can be specified in curly brackets. The previous example can be written as follows:
 
-```
+```text
 example01-0{1,2}-1
 ```
 
 Curly brackets can contain a range of numbers separated by two dots (non-negative integers). In this case, the range is expanded to a set of values that generate shard addresses. If the first number starts with zero, the values are formed with the same zero alignment. The previous example can be written as follows:
 
-```
+```text
 example01-{01..02}-1
 ```
 
@@ -53,7 +53,7 @@ Addresses and parts of addresses in curly brackets can be separated by the pipe
 
 Example:
 
-```
+```text
 example01-{01..02}-{1|2}
 ```
 
@@ -73,5 +73,3 @@ The `remote` table function can be useful in the following cases:
 If the user is not specified, `default` is used.
 If the password is not specified, an empty password is used.
 
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/remote/) <!--hide-->
diff --git a/docs/en/query_language/table_functions/url.md b/docs/en/query_language/table_functions/url.md
index 4e012cff0..7e30936bd 100644
--- a/docs/en/query_language/table_functions/url.md
+++ b/docs/en/query_language/table_functions/url.md
@@ -13,9 +13,7 @@ structure - table structure in `'UserID UInt64, Name String'` format. Determines
 
 **Example**
 
-``` sql
+```sql
 -- getting the first 3 lines of a table that contains columns of String and UInt32 type from HTTP-server which answers in CSV format.
 SELECT * FROM url('http://127.0.0.1:12345/', CSV, 'column1 String, column2 UInt32') LIMIT 3
 ```
-
-[Original article](https://clickhouse.yandex/docs/en/query_language/table_functions/url/) <!--hide-->
diff --git a/docs/en/roadmap.md b/docs/en/roadmap.md
index 490e570b6..51b6df941 100644
--- a/docs/en/roadmap.md
+++ b/docs/en/roadmap.md
@@ -1,14 +1,20 @@
 # Roadmap
 
+## Q3 2018
+
+- `ALTER UPDATE` for batch changing the data with approach similar to `ALTER DELETE`
+- Protobuf and Parquet input and output formats
+- Improved compatibility with Tableau and other BI tools
+
 ## Q4 2018
 
 - JOIN syntax compatible with SQL standard:
     - Mutliple `JOIN`s in single `SELECT`
+    - Connecting tables with `ON`
+    - Support table reference instead of subquery
 
 - JOIN execution improvements:
     - Distributed join not limited by memory
+    - Predicate pushdown through join
 
-- Protobuf and Parquet input and output formats
 - Resource pools for more precise distribution of cluster capacity between users
-
-[Original article](https://clickhouse.yandex/docs/en/roadmap/) <!--hide-->
diff --git a/docs/en/security_changelog.md b/docs/en/security_changelog.md
index 62e98614e..11e0557a8 100644
--- a/docs/en/security_changelog.md
+++ b/docs/en/security_changelog.md
@@ -19,5 +19,3 @@ Credits: Andrey Krasichkov and Evgeny Sidorov of Yandex Information Security Tea
 Incorrect configuration in deb package could lead to unauthorized use of the database.
 
 Credits: the UK's National Cyber Security Centre (NCSC)
-
-[Original article](https://clickhouse.yandex/docs/en/security_changelog/) <!--hide-->
diff --git a/docs/fa/data_types/array.md b/docs/fa/data_types/array.md
index e14f698f6..9b7c70618 100644
--- a/docs/fa/data_types/array.md
+++ b/docs/fa/data_types/array.md
@@ -4,5 +4,4 @@
 
 آرایه ای از عناصر با تایپ T. تایپ T می تواند هر Type باشد، از جمله یک آرایه. ما توصیه به استفاده از آرایه های multidimensional نمی کنیم، چون آنها به خوبی پشتیبانی نمی شوند (برای مثال، شما نمی تونید در جداولی که موتور آنها MergeTree است، آرایه های multidimensional ذخیره سازی کنید).
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/array/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/boolean.md b/docs/fa/data_types/boolean.md
index 90f540468..7fe6763ec 100644
--- a/docs/fa/data_types/boolean.md
+++ b/docs/fa/data_types/boolean.md
@@ -5,5 +5,3 @@
 type مخصوص مقادیر boolean وجود ندارد. از Uint8 و محدود شده به 0 و 1 می توان استفاده کرد.
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/boolean/) <!--hide-->
diff --git a/docs/fa/data_types/date.md b/docs/fa/data_types/date.md
index 2a71cbd24..3b77e5ce0 100644
--- a/docs/fa/data_types/date.md
+++ b/docs/fa/data_types/date.md
@@ -6,5 +6,4 @@ Date، دو بایت به ازای هر تاریخ که به صورت عددی 
 
 Date بدون time zone ذخیره می شود.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/date/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/datetime.md b/docs/fa/data_types/datetime.md
index d36243f7a..6731b1bea 100644
--- a/docs/fa/data_types/datetime.md
+++ b/docs/fa/data_types/datetime.md
@@ -13,5 +13,3 @@
 پس در هنگام کار با تاریخ متنی (برای مثال زمانی که دامپ text می گیرید)، به خاطر داشته باشید که ممکن است به دلیل تغییرات DST، نتایج مبهمی در خروجی ببینید، و ممکن است در صورت تغییر time zone مشکلی با مطابقت خروجی با داده ها وجود داشته باشد.
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/datetime/) <!--hide-->
diff --git a/docs/fa/data_types/enum.md b/docs/fa/data_types/enum.md
index d84f2a1aa..bf678647e 100644
--- a/docs/fa/data_types/enum.md
+++ b/docs/fa/data_types/enum.md
@@ -8,7 +8,7 @@ Enum8 یا Enum16، به شما اجازه ی ذخیره سازی مجموعه 
 
 </div>
 
-```
+```text
 Enum8('hello' = 1, 'world' = 2)
 ```
 
@@ -30,5 +30,4 @@ Enum8('hello' = 1, 'world' = 2)
 
 استفاده از ALTER برای تبدیل Enum8 به Enum16 یا برعکس، ممکن است، دقیقا شبیه به Int8 به Int16.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/enum/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/fixedstring.md b/docs/fa/data_types/fixedstring.md
index 962340693..f337fe0ef 100644
--- a/docs/fa/data_types/fixedstring.md
+++ b/docs/fa/data_types/fixedstring.md
@@ -6,5 +6,4 @@
 
 توابع کمتری نسبت به String برای FixedString(N) وجود دارد، و برای استفاده کمتر مناسب است.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/fixedstring/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/float.md b/docs/fa/data_types/float.md
index 77a17d6ec..60fb9fb38 100644
--- a/docs/fa/data_types/float.md
+++ b/docs/fa/data_types/float.md
@@ -17,7 +17,7 @@ Type های float در ClickHouse مشابه C می باشد:
 
 </div>
 
-``` sql
+```sql
 SELECT 1 - 0.9
 ```
 ```
@@ -40,7 +40,7 @@ SELECT 1 - 0.9
 
 </div>
 
-``` sql
+```sql
 SELECT 0.5 / 0
 ```
 
@@ -56,7 +56,7 @@ SELECT 0.5 / 0
 
 </div>
 
-``` sql
+```sql
 SELECT -0.5 / 0
 ```
 
@@ -86,5 +86,4 @@ SELECT 0 / 0
 
 قوانین مربوط به مرتب سازی ` Nan ` را در بخش [ORDER BY clause](../query_language/select.md#query_language-queries-order_by) ببینید.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/float/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/index.md b/docs/fa/data_types/index.md
index 5ffabb434..acd91711b 100644
--- a/docs/fa/data_types/index.md
+++ b/docs/fa/data_types/index.md
@@ -9,5 +9,3 @@ ClickHouse قابلیت ذخیره سازی انواع type های مختلف ب
 این بخش انواع data type های قابل پشتیبانی در ClickHouse را شرح می دهد، همچنین ملاحطات آنها در هنگام استفاده آنها را شرح می دهد.
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/) <!--hide-->
diff --git a/docs/fa/data_types/int_uint.md b/docs/fa/data_types/int_uint.md
index 0a1bbfe4a..fee85d72f 100644
--- a/docs/fa/data_types/int_uint.md
+++ b/docs/fa/data_types/int_uint.md
@@ -19,5 +19,4 @@
 - UInt32 - [0 : 4294967295]
 - UInt64 - [0 : 18446744073709551615]
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/int_uint/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/nested_data_structures/aggregatefunction.md b/docs/fa/data_types/nested_data_structures/aggregatefunction.md
index d57a3eddb..8d5615639 100644
--- a/docs/fa/data_types/nested_data_structures/aggregatefunction.md
+++ b/docs/fa/data_types/nested_data_structures/aggregatefunction.md
@@ -4,5 +4,4 @@
 
 حالت متوسط از توابع aggregate. برای دریافت آن، از توابع aggregate به همراه پسوند '-State' استفاده کنید. برای اطلاعات بیشتر قسمت  "AggregatingMergeTree" را ببینید.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/nested_data_structures/aggregatefunction/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/nested_data_structures/index.md b/docs/fa/data_types/nested_data_structures/index.md
index b5a381864..def4991e5 100644
--- a/docs/fa/data_types/nested_data_structures/index.md
+++ b/docs/fa/data_types/nested_data_structures/index.md
@@ -3,5 +3,3 @@
 # Nested data structures
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/nested_data_structures/) <!--hide-->
diff --git a/docs/fa/data_types/nested_data_structures/nested.md b/docs/fa/data_types/nested_data_structures/nested.md
index 1f3b675ee..a02aec748 100644
--- a/docs/fa/data_types/nested_data_structures/nested.md
+++ b/docs/fa/data_types/nested_data_structures/nested.md
@@ -8,7 +8,7 @@
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE test.visits
 (
     CounterID UInt32,
@@ -43,7 +43,7 @@ CREATE TABLE test.visits
 
 </div>
 
-``` sql
+```sql
 SELECT
     Goals.ID,
     Goals.EventTime
@@ -52,7 +52,7 @@ WHERE CounterID = 101500 AND length(Goals.ID) < 5
 LIMIT 10
 ```
 
-```
+```text
 ┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐
 │ [1073752,591325,591325]        │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27']                       │
 │ [1073752]                      │ ['2014-03-17 00:28:25']                                                                   │
@@ -75,7 +75,7 @@ LIMIT 10
 
 </div>
 
-``` sql
+```sql
 SELECT
     Goal.ID,
     Goal.EventTime
@@ -85,7 +85,7 @@ WHERE CounterID = 101500 AND length(Goals.ID) < 5
 LIMIT 10
 ```
 
-```
+```text
 ┌─Goal.ID─┬──────Goal.EventTime─┐
 │ 1073752 │ 2014-03-17 16:38:10 │
 │  591325 │ 2014-03-17 16:38:48 │
@@ -110,5 +110,4 @@ LIMIT 10
 
 دستور ALTER برای عناصر داخل nested بسیار محدود است.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/nested_data_structures/nested/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/special_data_types/expression.md b/docs/fa/data_types/special_data_types/expression.md
index ada0eb085..69ac90a39 100644
--- a/docs/fa/data_types/special_data_types/expression.md
+++ b/docs/fa/data_types/special_data_types/expression.md
@@ -4,5 +4,4 @@
 
 برای نشان دادن توابع لامبدا در توابع high-order استفاده می شود.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/special_data_types/expression/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/special_data_types/index.md b/docs/fa/data_types/special_data_types/index.md
index 257b75087..88f320128 100644
--- a/docs/fa/data_types/special_data_types/index.md
+++ b/docs/fa/data_types/special_data_types/index.md
@@ -4,5 +4,4 @@
 
 مقادیر نوع داده special، نمیتوانند در در جدول ذخیره و یا در نتایج خروجی قرار بگیرند، اما در نتایج متوسط (intermediate) یک query در حال اجرا استفاده می شوند.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/special_data_types/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/special_data_types/set.md b/docs/fa/data_types/special_data_types/set.md
index f2332e9eb..e51899727 100644
--- a/docs/fa/data_types/special_data_types/set.md
+++ b/docs/fa/data_types/special_data_types/set.md
@@ -4,5 +4,4 @@
 
 برای نصف سمت راست IN استفاده می شود.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/special_data_types/set/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/string.md b/docs/fa/data_types/string.md
index 71685574c..517340073 100644
--- a/docs/fa/data_types/string.md
+++ b/docs/fa/data_types/string.md
@@ -8,5 +8,4 @@ String یک type برای قرار دادن رشته با طول دلخواه م
 
 ClickHouse مفهومی به نام encoding ندارد. String ها می توانند شامل مجموعه ای بایت ها باشند که با همان شکل که نوشته می شوند به همان شکل هم در خروجی دیده شوند. اگر شما نیاز به ذخیره سازی متن دارید، توصیه می کنیم از UTF-8 استفاده کنید. حداقل اگر ترمینال شما از UTF-8 (پیشنهاد شده)، استفاده می کند، شما می توانید به راحتی مقادیر خود را نوشته و بخوانید.به طور مشابه توابع خاصی برای کار با رشته های متنوع وجود دارند که تخل این فرضیه عمل می کنند که رشته شامل مجوعه ای از بایت ها می باشند که نماینده ی متن های UTF-8 هستند. برای مثال تابع 'length' برای محاسبه طول رشته براساس بایت است، در حالی که تابع 'lengthUTF8' برای محاسبه طول رشته بر اساس UNICODE می باشد.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/string/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/data_types/tuple.md b/docs/fa/data_types/tuple.md
index 3d0a22cb1..1acb21fb2 100644
--- a/docs/fa/data_types/tuple.md
+++ b/docs/fa/data_types/tuple.md
@@ -6,5 +6,4 @@ Tuple ها نمیتوانند در جدول نوشته شوند (به غیر ج
 
 Tuple می توانند در خروجی نتیجه query در حال اجرا باشند. در این مورد، برای فرمت های text به غیر از JSON\*، مقادیر به صورت comma-separate داخل براکت قرار میگیرند. در فرمت های JSON\* مقادیر tuple به صورت آرایه در خروجی می آیند.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/data_types/tuple/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/getting_started/example_datasets/amplab_benchmark.md b/docs/fa/getting_started/example_datasets/amplab_benchmark.md
index 6ed6f97a3..acdf6dc87 100644
--- a/docs/fa/getting_started/example_datasets/amplab_benchmark.md
+++ b/docs/fa/getting_started/example_datasets/amplab_benchmark.md
@@ -29,7 +29,7 @@ cd ..
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE rankings_tiny
 (
     pageURL String,
@@ -112,7 +112,7 @@ query های گرفتن data sample
 
 </div>
 
-``` sql
+```sql
 SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank > 1000
 
 SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8)
@@ -134,5 +134,3 @@ GROUP BY sourceIP
 ORDER BY totalRevenue DESC
 LIMIT 1
 ```
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/example_datasets/amplab_benchmark/) <!--hide-->
diff --git a/docs/fa/getting_started/example_datasets/criteo.md b/docs/fa/getting_started/example_datasets/criteo.md
index 47013f7aa..57f4007ef 100644
--- a/docs/fa/getting_started/example_datasets/criteo.md
+++ b/docs/fa/getting_started/example_datasets/criteo.md
@@ -8,7 +8,7 @@
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log
 ```
 
@@ -28,7 +28,7 @@ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE criteo
 (
     date Date,
@@ -81,11 +81,9 @@ CREATE TABLE criteo
 
 </div>
 
-``` sql
+```sql
 INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log;
 
 DROP TABLE criteo_log;
 ```
 
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/example_datasets/criteo/) <!--hide-->
diff --git a/docs/fa/getting_started/example_datasets/nyc_taxi.md b/docs/fa/getting_started/example_datasets/nyc_taxi.md
index 892b706e6..bf4785833 100644
--- a/docs/fa/getting_started/example_datasets/nyc_taxi.md
+++ b/docs/fa/getting_started/example_datasets/nyc_taxi.md
@@ -33,7 +33,7 @@ PostgreSQL تقریبا 20 تا 30 دقیقه برای پردازش هر ماه
 
 </div>
 
-```
+```text
 time psql nyc-taxi-data -c "SELECT count(*) FROM trips;"
 ##    count
  1298979494
@@ -52,7 +52,7 @@ Export گیری داده ها از PostgreSQL:
 
 </div>
 
-``` sql
+```sql
 COPY
 (
     SELECT trips.id,
@@ -130,7 +130,7 @@ snapshot از داده ها با سرعت 50 مگابایت در ثانیه ان
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE trips
 (
 trip_id                 UInt32,
@@ -193,7 +193,7 @@ dropoff_puma            Nullable(String)
 
 </div>
 
-```
+```text
 time clickhouse-client --query="INSERT INTO trips FORMAT TabSeparated" < trips.tsv
 
 real    75m56.214s
@@ -213,7 +213,7 @@ real    75m56.214s
 
 </div>
 
-```
+```text
 CREATE TABLE trips_mergetree
 ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
 AS SELECT
@@ -283,7 +283,7 @@ FROM trips
 
 </div>
 
-```
+```text
 :) SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active
 
 SELECT formatReadableSize(sum(bytes))
@@ -305,7 +305,7 @@ WHERE (table = 'trips_mergetree') AND active
 
 Q1:
 
-``` sql
+```sql
 SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
 ```
 
@@ -313,7 +313,7 @@ SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
 
 Q2:
 
-``` sql
+```sql
 SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count
 ```
 
@@ -321,7 +321,7 @@ SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenge
 
 Q3:
 
-``` sql
+```sql
 SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year
 ```
 
@@ -329,7 +329,7 @@ SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetr
 
 Q4:
 
-``` sql
+```sql
 SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*)
 FROM trips_mergetree
 GROUP BY passenger_count, year, distance
@@ -354,7 +354,7 @@ Two Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz, 16 physical kernels total,
 
 </div>
 
-```
+```text
 CREATE TABLE default.trips_mergetree_third ( trip_id UInt32,  vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14),  pickup_date Date,  pickup_datetime DateTime,  dropoff_date Date,  dropoff_datetime DateTime,  store_and_fwd_flag UInt8,  rate_code_id UInt8,  pickup_longitude Float64,  pickup_latitude Float64,  dropoff_longitude Float64,  dropoff_latitude Float64,  passenger_count UInt8,  trip_distance Float64,  fare_amount Float32,  extra Float32,  mta_tax Float32,  tip_amount Float32,  tolls_amount Float32,  ehail_fee Float32,  improvement_surcharge Float32,  total_amount Float32,  payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),  trip_type UInt8,  pickup FixedString(25),  dropoff FixedString(25),  cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),  pickup_nyct2010_gid UInt8,  pickup_ctlabel Float32,  pickup_borocode UInt8,  pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  pickup_ct2010 FixedString(6),  pickup_boroct2010 FixedString(7),  pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  pickup_ntacode FixedString(4),  pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  pickup_puma UInt16,  dropoff_nyct2010_gid UInt8,  dropoff_ctlabel Float32,  dropoff_borocode UInt8,  dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  dropoff_ct2010 FixedString(6),  dropoff_boroct2010 FixedString(7),  dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  dropoff_ntacode FixedString(4),  dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
 ```
 
@@ -364,7 +364,7 @@ CREATE TABLE default.trips_mergetree_third ( trip_id UInt32,  vendor_id Enum8('1
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())
 ```
 
@@ -374,7 +374,7 @@ query زیر دادها را توزیع مجدد می کند:
 
 </div>
 
-``` sql
+```sql
 INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree
 ```
 
@@ -408,5 +408,4 @@ Q4: 0.072 ثانیه.
 |     3 | 0.212 | 0.438 | 0.733 | 1.241 |
 |   140 | 0.028 | 0.043 | 0.051 | 0.072 |
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/example_datasets/nyc_taxi/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/getting_started/example_datasets/ontime.md b/docs/fa/getting_started/example_datasets/ontime.md
index 443e37ed9..f5bf5b865 100644
--- a/docs/fa/getting_started/example_datasets/ontime.md
+++ b/docs/fa/getting_started/example_datasets/ontime.md
@@ -26,7 +26,7 @@ done
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE `ontime` (
   `Year` UInt16,
   `Quarter` UInt8,
@@ -157,7 +157,7 @@ Q0.
 
 </div>
 
-``` sql
+```sql
 select avg(c1) from (select Year, Month, count(*) as c1 from ontime group by Year, Month);
 ```
 
@@ -167,7 +167,7 @@ Q1. تعداد پروازهای به تفکیک روز از تاریخ 2000 تا
 
 </div>
 
-``` sql
+```sql
 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE Year >= 2000 AND Year <= 2008 GROUP BY DayOfWeek ORDER BY c DESC;
 ```
 
@@ -177,7 +177,7 @@ Q2. تعداد پروازهای بیش از 10 دقیقه تاخیر خورده
 
 </div>
 
-``` sql
+```sql
 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE DepDelay>10 AND Year >= 2000 AND Year <= 2008 GROUP BY DayOfWeek ORDER BY c DESC
 ```
 
@@ -187,7 +187,7 @@ Q3. تعداد تاخیرها براساس airport از سال 2000 تا 2008
 
 </div>
 
-``` sql
+```sql
 SELECT Origin, count(*) AS c FROM ontime WHERE DepDelay>10 AND Year >= 2000 AND Year <= 2008 GROUP BY Origin ORDER BY c DESC LIMIT 10
 ```
 
@@ -197,7 +197,7 @@ Q4. تعداد تاخیرها براساس carrier در سال 78
 
 </div>
 
-``` sql
+```sql
 SELECT Carrier, count(*) FROM ontime WHERE DepDelay>10  AND Year = 2007 GROUP BY Carrier ORDER BY count(*) DESC
 ```
 
@@ -207,7 +207,7 @@ Q5. درصد تاخیر ها براساس carrier در سال 2007
 
 </div>
 
-``` sql
+```sql
 SELECT Carrier, c, c2, c*1000/c2 as c3
 FROM
 (
@@ -237,7 +237,7 @@ ORDER BY c3 DESC;
 
 </div>
 
-``` sql
+```sql
 SELECT Carrier, avg(DepDelay > 10) * 1000 AS c3 FROM ontime WHERE Year = 2007 GROUP BY Carrier ORDER BY Carrier
 ```
 
@@ -247,7 +247,7 @@ Q6. مانند query قبلی اما برای طیف وسیعی از سال ها
 
 </div>
 
-``` sql
+```sql
 SELECT Carrier, c, c2, c*1000/c2 as c3
 FROM
 (
@@ -277,7 +277,7 @@ ORDER BY c3 DESC;
 
 </div>
 
-``` sql
+```sql
 SELECT Carrier, avg(DepDelay > 10) * 1000 AS c3 FROM ontime WHERE Year >= 2000 AND Year <= 2008 GROUP BY Carrier ORDER BY Carrier
 ```
 
@@ -287,7 +287,7 @@ Q7. درصد تاخیر بیش از 10 دقیقه پروازها به تفکیک
 
 </div>
 
-``` sql
+```sql
 SELECT Year, c1/c2
 FROM
 (
@@ -315,7 +315,7 @@ ORDER BY Year
 
 </div>
 
-``` sql
+```sql
 SELECT Year, avg(DepDelay > 10) FROM ontime GROUP BY Year ORDER BY Year
 ```
 
@@ -325,7 +325,7 @@ Q8. مقصدهای پرطرفدار براساس تعداد اتصال های م
 
 </div>
 
-``` sql
+```sql
 SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime WHERE Year >= 2000 and Year <= 2010 GROUP BY DestCityName ORDER BY u DESC LIMIT 10;
 ```
 
@@ -335,7 +335,7 @@ Q9.
 
 </div>
 
-``` sql
+```sql
 select Year, count(*) as c1 from ontime group by Year;
 ```
 
@@ -345,7 +345,7 @@ Q10.
 
 </div>
 
-``` sql
+```sql
 select
    min(Year), max(Year), Carrier, count(*) as cnt,
    sum(ArrDelayMinutes>30) as flights_delayed,
@@ -367,7 +367,7 @@ query های بیشتر:
 
 </div>
 
-``` sql
+```sql
 SELECT avg(cnt) FROM (SELECT Year,Month,count(*) AS cnt FROM ontime WHERE DepDel15=1 GROUP BY Year,Month)
 
 select avg(c1) from (select Year,Month,count(*) as c1 from ontime group by Year,Month)
@@ -391,5 +391,3 @@ SELECT OriginCityName, count() AS c FROM ontime GROUP BY OriginCityName ORDER BY
 - <http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html>
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/example_datasets/ontime/) <!--hide-->
diff --git a/docs/fa/getting_started/example_datasets/star_schema.md b/docs/fa/getting_started/example_datasets/star_schema.md
index 90bf5246e..8d09090f3 100644
--- a/docs/fa/getting_started/example_datasets/star_schema.md
+++ b/docs/fa/getting_started/example_datasets/star_schema.md
@@ -33,7 +33,7 @@ make
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE lineorder (
         LO_ORDERKEY             UInt32,
         LO_LINENUMBER           UInt8,
@@ -96,5 +96,3 @@ CREATE TABLE partd AS part ENGINE = Distributed(perftest_3shards_1replicas, defa
 cat customer.tbl | sed 's/$/2000-01-01/' | clickhouse-client --query "INSERT INTO customer FORMAT CSV"
 cat lineorder.tbl | clickhouse-client --query "INSERT INTO lineorder FORMAT CSV"
 ```
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/example_datasets/star_schema/) <!--hide-->
diff --git a/docs/fa/getting_started/example_datasets/wikistat.md b/docs/fa/getting_started/example_datasets/wikistat.md
index c49fd4ae2..543a41901 100644
--- a/docs/fa/getting_started/example_datasets/wikistat.md
+++ b/docs/fa/getting_started/example_datasets/wikistat.md
@@ -8,7 +8,7 @@
 
 </div>
 
-``` sql
+```sql
 CREATE TABLE wikistat
 (
     date Date,
@@ -33,5 +33,3 @@ for i in {2007..2016}; do for j in {01..12}; do echo $i-$j >&2; curl -sSL "http:
 cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pagecounts-raw/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1/')/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1-\2/')/$link; done
 ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time="$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\.gz/\1-\2-\3 \4-00-00/')" | clickhouse-client --query="INSERT INTO wikistat FORMAT TabSeparated"; done
 ```
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/example_datasets/wikistat/) <!--hide-->
diff --git a/docs/fa/getting_started/index.md b/docs/fa/getting_started/index.md
index dba3d4d08..8d5a0bcfa 100644
--- a/docs/fa/getting_started/index.md
+++ b/docs/fa/getting_started/index.md
@@ -26,7 +26,7 @@ grep -q sse4_2 /proc/cpuinfo && echo "SSE 4.2 supported" || echo "SSE 4.2 not su
 
 </div>
 
-```
+```text
 deb http://repo.yandex.ru/clickhouse/deb/stable/ main/
 ```
 
@@ -58,7 +58,7 @@ ClickHouse دارای تنظیمات محدودیت دسترسی می باشد.
 
 </div>
 
-```
+```text
 Client: dbms/programs/clickhouse-client
 Server: dbms/programs/clickhouse-server
 ```
@@ -69,7 +69,7 @@ Server: dbms/programs/clickhouse-server
 
 </div>
 
-```
+```text
 /opt/clickhouse/data/default/
 /opt/clickhouse/metadata/default/
 ```
@@ -167,5 +167,4 @@ SELECT 1
 
 برای ادامه آزمایشات، شما میتوانید دیتاست های تستی را دریافت و امتحان کنید.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/getting_started/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/index.md b/docs/fa/index.md
index d5f6c20d2..aa93081b9 100644
--- a/docs/fa/index.md
+++ b/docs/fa/index.md
@@ -84,8 +84,9 @@ ClickHouse یک مدیریت دیتابیس (DBMS) ستون گرا برای پر
 
  برای مثال، query "تعداد رکوردها به ازای هر بستر نیازمندی" نیازمند خواندن ستون "آیدی بستر آگهی"، که 1 بایت بدون فشرده طول می کشد، خواهد بود. اگر بیشتر ترافیک مربوط به بستر های نیازمندی نبود، شما می توانید انتظار حداقل 10 برابر فشرده سازی این ستون را داشته باشید. زمانی که از الگوریتم فشرده سازی quick استفاده می کنید، عملیات decompression داده ها با سرعت حداقل چندین گیگابایت در ثانیه انجام می شود. به عبارت دیگر، این query توانایی پردازش تقریبا چندین میلیارد رکورد در ثانیه به ازای یک سرور را دارد. این سرعت در عمل واقعی و دست یافتنی است.
 
-<details markdown="1"><summary>مثال</summary>
-```
+<details><summary>مثال</summary>
+<p>
+<pre>
 $ clickhouse-client
 ClickHouse client version 0.0.52053.
 Connecting to localhost:9000.
@@ -127,7 +128,8 @@ LIMIT 20
 20 rows in set. Elapsed: 0.153 sec. Processed 1.00 billion rows, 4.00 GB (6.53 billion rows/s., 26.10 GB/s.)
 
 :)
-```
+</pre>
+</p>
 </details>
 
 ### CPU
@@ -144,6 +146,4 @@ LIMIT 20
 
 توجه کنید که برای کارایی CPU، query language باید SQL یا MDX باشد، یا حداقل یک بردارد (J, K) باشد. query برای بهینه سازی باید فقط دارای حلقه های implicit باشد.
 
-</div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/interfaces/cli.md b/docs/fa/interfaces/cli.md
index b4c0bc7e0..6c7372c83 100644
--- a/docs/fa/interfaces/cli.md
+++ b/docs/fa/interfaces/cli.md
@@ -120,5 +120,3 @@ command line برا پایه 'readline' (و 'history' یا 'libedit'، یه بد
 </config>
 ```
 
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/cli/) <!--hide-->
diff --git a/docs/fa/interfaces/formats.md b/docs/fa/interfaces/formats.md
index 197946df7..c2aaf8514 100644
--- a/docs/fa/interfaces/formats.md
+++ b/docs/fa/interfaces/formats.md
@@ -34,117 +34,39 @@ Format | INSERT | SELECT
 [XML](formats.md#xml) | ✗ | ✔ |
 [CapnProto](formats.md#capnproto) | ✔ | ✔ |
 
+<a name="format_capnproto"></a>
 
-## TabSeparated
-
-در فرمت TabSeparated، داده ها به صورت سطر نوشته می شوند. هر سطر شامل مقادیر جدا شده با tab می باشد. هر مقدار با یک tab دنبال می شود، به جز آخرین مقدار یک سطر، که با line feed دنبال می شود. line feed unix در همه جا مورد تسافده قرار می گیرد. آخرین سطر از خروجی هم باید شامل line feed در انتها باشد. مقادیر در فرمت متنی بدون enclose با کوتیشون، و یا escape با کاراکترهای ویژه، نوشته می شوند.
-
-اعداد Integer با فرم decimal نوشته می شوند. اعداد می توانند شامل کاراکتر اضافه "+" در ابتدای خود باشند. (در هنگام پارس کردن نادیده گرفته می شوند، و در هنگام فرمت کردن، ثبت نمی شوند). اعداد غیر منفی نمیتوانند شامل علامت منفی باشند. در هنگام خواندن، اجازه داده می شود که رشته خالی را به عنوان صفر، پارس کرد، یا (برای تایپ های sign) یک رشته که شامل فقط یک علامت منفی است به عنوان صفر پارس کرد. اعدادی که در data type مربوطه فیت نشوند ممکن است به عددی متفاوت تبدیل شوند و پیغام خطایی هم نمایش ندهند.
-
-اعداد Floating-point به فرم decimal نوشته می شوند. از دات به عنوان جدا کننده decimal استفاده می شود. نوشته های نمایشی مثل 'inf'، '+inf'، '-inf' و 'nan' پشتیبانی می شوند. ورودی اعداد floating-point می تواند با یه نقطه اعشار شروع یا پایان یابد. در هنگام فرمت، دقت اعداد floating-point ممکن است گم شوند. در هنگام پارس کردن، دقیقا نیازی به خواندن نزدیکترین عدد machine-representable نیست.
-
-Dates با فرمت YYY-MM-DD نوشته می شوند و به همین حالت پارس می شوند، اما با هر کاراکتری به عنوان جدا کننده. Dates به همراه زمان با فرمت YYYY-MM-DD hh:mm:ss نوشته می شوند و با همین فرمت پارس می شوند، اما با هر کاراکتری به عنوان جداکننده.  این در منطقه زمان سیستم در زمانی که کلاینت یا سرور شروع می شود (بسته به اینکه کدام یک از داده ها را تشکیل می دهد) رخ می دهد. برای تاریخ همراه با زمان DST مشخص نمی شود. پس اگر یک دامپ دارای زمان DST باشد، دامپ، داده ها را به طور غیرمستقیم مطابقت نمی دهد و پارسینگ، یکی از دو ساعت را انتخاب خواهد کرد. در طول عملیات خواندن، تاریخ ها و تاریخ و ساعت های نادرست می توانند به صورت null و یا natural overflow پارس شوند، بدون اینکه پیغام خطایی نمایش دهند.
-
-به عنوان یک استثنا، پارس کردن تاریخ به همراه ساعت، اگر مقدار دقیقا شامل 10 عدد decimal باشد، به عنوان فرمت unix timestamp پشتیبانی خواهد کرد. خروجی وابسته به time-zone نمی باشد.  فرمت های YYYY-MM-DD hh: mm: ss و NNNNNNNNNN به صورت خودکار تمایز می یابند.
-
-رشته های دارای کاراکتر های ویژه backslash-escaped چاپ می شوند. escape های در ادامه برای خروجی استفاده می شوند: `\b`، `\f`، `\r`، `\n`، `\t`، `\0`, `\'`، `\\`. پارسر همچنین از `\a`، `\v`، و `\xHH` (hex escape) و هر `\c`  پشتیبانی می کند. بدین ترتیب خواندن داده ها از فرمت line feed که می تواند به صورت `\n` یا `\`  نوشته شود پشتیبانی می کند. برای مثال، رشته ی `Hello world` به همراه line feed بین کلمات به جای space می تواند به هر یک از حالات زیر پارس شود::
-
-</div>
-
-```
-Hello\nworld
-
-Hello\
-world
-```
-
-<div dir="rtl" markdown="1">
-
-نوع دوم به دلیل پشتیبانی MySQL در هنگام نوشتن دامپ به صورت tab-separate، پشتیبانی می شود.
-
-حداقل مجموعه از کاراکترهایی که در هنگام پاس دادن داده در فرمت TabSeperate نیاز به escape آن دارید: tab، line feed (LF) بک اسلش.
-
-فقط مجموعه ی کمی از نماد ها escape می شوند. شما به راحتی می توانید بر روی مقدار رشته که در ترمینال شما در خروجی نمایش داده می شود حرکت کنید.
-
-آرایه ها به صورت لیستی از مقادیر که به comma از هم جدا شده اند و در داخل براکت قرار گرفته اند نوشته می شوند. آیتم های عددی در آرای به صورت نرمال فرمت می شوند، اما تاریخ و تاریخ با ساعت و رشته ها در داخل تک کوتیشن به همراه قوانین escape که بالا اشاره شد، نوشته می شوند.
+## CapnProto
 
-فرمت TabSeparate برای پردازش داده ها با استفاده از برنامه های شخصی سازی شده و اسکریپت ها مناسب است. TabSeparate به صورت پیش فرض در HTTP interface و در حالت batch کلاینت command-line مورد استفاده قرار می گیرد. همچنین این فرمت اجازه ی انتقال داده ها بین DBMS های مختلف را می دهد. برای مثال، شما می توانید از MySQL با این روش دامپ بگیرید و آن را در ClickHouse یا vice versa آپلود کنید.
+Cap'n Proto یک فرمت پیام باینری شبیه به Protocol Buffer و Thrift می باشد، اما شبیه به JSON یا MessagePack نیست.
 
-فرمت TabSeparated از خروحی total values (هنگام استفاده از WITH TOTALS) و extreme values (در هنگامی که 'extreme' برابر با 1 است) پشتیبانی می کند. در این موارد، total value و extreme بعد از داده های اصلی در خروجی می آیند. نتایج اصلی، total values و extreme همگی با یک empty line از هم جدا می شوند. مثال:
+پیغام های Cap'n Proto به صورت self-describing نیستند، به این معنی که آنها نیاز دارند که به صورت external، schema آنها شرح داده شود. schema به صورت on the fly اضافه می شود و برای هر query، cache می شود.
 
 </div>
 
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
-```
-
-```
-2014-03-17      1406958
-2014-03-18      1383658
-2014-03-19      1405797
-2014-03-20      1353623
-2014-03-21      1245779
-2014-03-22      1031592
-2014-03-23      1046491
-
-0000-00-00      8873898
-
-2014-03-17      1031592
-2014-03-23      1406958
+```sql
+SELECT SearchPhrase, count() AS c FROM test.hits
+       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
 ```
 
 <div dir="rtl" markdown="1">
 
-این فرمت نیز تحت نام `TSV` موجود است.
-
-
-
-## TabSeparatedRaw
-
-تفاوت آن با `TabSeperated` در این است که در این فرمت سطرها بدون escape نوشته می شوند. این فرمت فقط مناسب خروجی نتایج query ها می باشد، نه برای پارس کردن (دریافت داده ها و درج آن در جدول).
-
-همچنین این فرمت تحت عنوان ` TSVRaw`وجود دارد.
-
-## TabSeparatedWithNames
-
-تفاوت آن با فرمت `TabSeparated` در این است که، در این فرمت نام ستون ها در سطر اول قرار می گیرد. در طول پارس کردن، سطر اول به طور کامل نادیده گرفته می شود. شما نمی توانید نام ستون ها را برای تعیین موقعیت آنها یا بررسی صحت آنها استفاده کنید. (پشتیبانی از پارس کردن سطر header ممکن است در آینده اضافه شود.)
-
-همچنین این فرمت تحت عنوان ` TSVWithNames`وجود دارد.
-
-## TabSeparatedWithNamesAndTypes
-
-تفاوت آن با `TabSeparated` در این است که در این فرمت نام ستون ها در سطر اول نوشته می شود، و type ستون ها در سطر دوم نوشته می شود. در طی پارسینگ، سطر اول و دوم به طور کامل نادیده گرفته می شوند.
-
-همچنین این فرمت تحت عنوان ` TSVWithNamesAndTypes`وجود دارد.
-
-## TSKV
-
-مشابه فرمت TabSeparated، اما خروجی به صورت name=value می باشد. نام ها مشابه روش TabSeparated، escape می شوند، و همچنین = symbol هم escape می شود.
+جایی که `schema.capnp` شبیه این است:
 
 </div>
 
 ```
-SearchPhrase=   count()=8267016
-SearchPhrase=bathroom interior design    count()=2166
-SearchPhrase=yandex     count()=1655
-SearchPhrase=spring 2014 fashion    count()=1549
-SearchPhrase=freeform photos       count()=1480
-SearchPhrase=angelina jolia    count()=1245
-SearchPhrase=omsk       count()=1112
-SearchPhrase=photos of dog breeds    count()=1091
-SearchPhrase=curtain design        count()=1064
-SearchPhrase=baku       count()=1000
+struct Message {
+  SearchPhrase @0 :Text;
+  c @1 :Uint64;
+}
 ```
 
 <div dir="rtl" markdown="1">
 
-وقتی تعداد زیادی از ستون ها وجود دارد، این فرمت بی فایده است، و در حالت کلی دلیلی بر استفاده از این فرمت در این مواقع وجود ندارد. این فرمت در بعضی از دپارتمان های Yandex استفاده می شد.
-
-خروجی داده ها و پارس کردن هر دو در این فرمت پشتیبانی می شوند. برای پارس کردن، هر ترتیبی برای مقادیر ستون های مختلف پشتیبانی می شود. حذف بعضی از مقادیر قابل قبول است. این مقادیر با مقادیر پیش فرض خود برابر هستند. در این مورد، صفر و سطر خالی، توسط مقادیر پیش فرض پر می شوند. مقادیر پیچیده ای که می تواند در جدول مشخص شود به عنوان پیش فرض در این فرمت پشتیبانی نمیشوند.
-
-پارس کردن، اجازه می دهد که فیلد اضافه ی `tskv` بدون علامت و مقدار وجود داشته باشد. این فیلد نادیده گرفته می شود.
+فایل های Schema در فایلی قرار دارند که این فایل در دایرکتوری مشخص شده کانفیگ [ format_schema_path](../operations/server_settings/settings.md#server_settings-format_schema_path) قرار گرفته اند.
 
-<a name="csv"></a>
+عملیات Deserialization موثر است و معمولا لود سیستم را افزایش نمی دهد.
 
 ## CSV
 
@@ -176,7 +98,7 @@ clickhouse-client --format_csv_delimiter="|" --query="INSERT INTO test.csv FORMA
 
 </div>
 
-``` sql
+```sql
 SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON
 ```
 
@@ -350,11 +272,11 @@ JSON با جاوااسکریپت سازگار است. برای اطمینان ا
 
 </div>
 
-``` sql
+```sql
 SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact
 ```
 
-```
+```text
 ┌──EventDate─┬───────c─┐
 │ 2014-03-17 │ 1406958 │
 │ 2014-03-18 │ 1383658 │
@@ -425,6 +347,115 @@ watch -n1 "clickhouse-client --query='SELECT event, value FROM system.events FOR
 
 آرایه به عنوان variant length نشان داده می شود (unsigned [LEB128](https://en.wikipedia.org/wiki/LEB128))، دنباله ای از عانصر پیوسته آرایه
 
+## TabSeparated
+
+در فرمت TabSeparated، داده ها به صورت سطر نوشته می شوند. هر سطر شامل مقادیر جدا شده با tab می باشد. هر مقدار با یک tab دنبال می شود، به جز آخرین مقدار یک سطر، که با line feed دنبال می شود. line feed unix در همه جا مورد تسافده قرار می گیرد. آخرین سطر از خروجی هم باید شامل line feed در انتها باشد. مقادیر در فرمت متنی بدون enclose با کوتیشون، و یا escape با کاراکترهای ویژه، نوشته می شوند.
+
+اعداد Integer با فرم decimal نوشته می شوند. اعداد می توانند شامل کاراکتر اضافه "+" در ابتدای خود باشند. (در هنگام پارس کردن نادیده گرفته می شوند، و در هنگام فرمت کردن، ثبت نمی شوند). اعداد غیر منفی نمیتوانند شامل علامت منفی باشند. در هنگام خواندن، اجازه داده می شود که رشته خالی را به عنوان صفر، پارس کرد، یا (برای تایپ های sign) یک رشته که شامل فقط یک علامت منفی است به عنوان صفر پارس کرد. اعدادی که در data type مربوطه فیت نشوند ممکن است به عددی متفاوت تبدیل شوند و پیغام خطایی هم نمایش ندهند.
+
+اعداد Floating-point به فرم decimal نوشته می شوند. از دات به عنوان جدا کننده decimal استفاده می شود. نوشته های نمایشی مثل 'inf'، '+inf'، '-inf' و 'nan' پشتیبانی می شوند. ورودی اعداد floating-point می تواند با یه نقطه اعشار شروع یا پایان یابد. در هنگام فرمت، دقت اعداد floating-point ممکن است گم شوند. در هنگام پارس کردن، دقیقا نیازی به خواندن نزدیکترین عدد machine-representable نیست.
+
+Dates با فرمت YYY-MM-DD نوشته می شوند و به همین حالت پارس می شوند، اما با هر کاراکتری به عنوان جدا کننده. Dates به همراه زمان با فرمت YYYY-MM-DD hh:mm:ss نوشته می شوند و با همین فرمت پارس می شوند، اما با هر کاراکتری به عنوان جداکننده.  این در منطقه زمان سیستم در زمانی که کلاینت یا سرور شروع می شود (بسته به اینکه کدام یک از داده ها را تشکیل می دهد) رخ می دهد. برای تاریخ همراه با زمان DST مشخص نمی شود. پس اگر یک دامپ دارای زمان DST باشد، دامپ، داده ها را به طور غیرمستقیم مطابقت نمی دهد و پارسینگ، یکی از دو ساعت را انتخاب خواهد کرد. در طول عملیات خواندن، تاریخ ها و تاریخ و ساعت های نادرست می توانند به صورت null و یا natural overflow پارس شوند، بدون اینکه پیغام خطایی نمایش دهند.
+
+به عنوان یک استثنا، پارس کردن تاریخ به همراه ساعت، اگر مقدار دقیقا شامل 10 عدد decimal باشد، به عنوان فرمت unix timestamp پشتیبانی خواهد کرد. خروجی وابسته به time-zone نمی باشد.  فرمت های YYYY-MM-DD hh: mm: ss و NNNNNNNNNN به صورت خودکار تمایز می یابند.
+
+رشته های دارای کاراکتر های ویژه backslash-escaped چاپ می شوند. escape های در ادامه برای خروجی استفاده می شوند: `\b`، `\f`، `\r`، `\n`، `\t`، `\0`, `\'`، `\\`. پارسر همچنین از `\a`، `\v`، و `\xHH` (hex escape) و هر `\c`  پشتیبانی می کند. بدین ترتیب خواندن داده ها از فرمت line feed که می تواند به صورت `\n` یا `\`  نوشته شود پشتیبانی می کند. برای مثال، رشته ی `Hello world` به همراه line feed بین کلمات به جای space می تواند به هر یک از حالات زیر پارس شود::
+
+</div>
+
+```text
+Hello\nworld
+
+Hello\
+world
+```
+
+<div dir="rtl" markdown="1">
+
+نوع دوم به دلیل پشتیبانی MySQL در هنگام نوشتن دامپ به صورت tab-separate، پشتیبانی می شود.
+
+حداقل مجموعه از کاراکترهایی که در هنگام پاس دادن داده در فرمت TabSeperate نیاز به escape آن دارید: tab، line feed (LF) بک اسلش.
+
+فقط مجموعه ی کمی از نماد ها escape می شوند. شما به راحتی می توانید بر روی مقدار رشته که در ترمینال شما در خروجی نمایش داده می شود حرکت کنید.
+
+آرایه ها به صورت لیستی از مقادیر که به comma از هم جدا شده اند و در داخل براکت قرار گرفته اند نوشته می شوند. آیتم های عددی در آرای به صورت نرمال فرمت می شوند، اما تاریخ و تاریخ با ساعت و رشته ها در داخل تک کوتیشن به همراه قوانین escape که بالا اشاره شد، نوشته می شوند.
+
+فرمت TabSeparate برای پردازش داده ها با استفاده از برنامه های شخصی سازی شده و اسکریپت ها مناسب است. TabSeparate به صورت پیش فرض در HTTP interface و در حالت batch کلاینت command-line مورد استفاده قرار می گیرد. همچنین این فرمت اجازه ی انتقال داده ها بین DBMS های مختلف را می دهد. برای مثال، شما می توانید از MySQL با این روش دامپ بگیرید و آن را در ClickHouse یا vice versa آپلود کنید.
+
+فرمت TabSeparated از خروحی total values (هنگام استفاده از WITH TOTALS) و extreme values (در هنگامی که 'extreme' برابر با 1 است) پشتیبانی می کند. در این موارد، total value و extreme بعد از داده های اصلی در خروجی می آیند. نتایج اصلی، total values و extreme همگی با یک empty line از هم جدا می شوند. مثال:
+
+</div>
+
+```sql
+SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
+```
+
+```text
+2014-03-17      1406958
+2014-03-18      1383658
+2014-03-19      1405797
+2014-03-20      1353623
+2014-03-21      1245779
+2014-03-22      1031592
+2014-03-23      1046491
+
+0000-00-00      8873898
+
+2014-03-17      1031592
+2014-03-23      1406958
+```
+
+<div dir="rtl" markdown="1">
+
+این فرمت نیز تحت نام `TSV` موجود است.
+
+
+
+## TabSeparatedRaw
+
+تفاوت آن با `TabSeperated` در این است که در این فرمت سطرها بدون escape نوشته می شوند. این فرمت فقط مناسب خروجی نتایج query ها می باشد، نه برای پارس کردن (دریافت داده ها و درج آن در جدول).
+
+همچنین این فرمت تحت عنوان ` TSVRaw`وجود دارد.
+
+## TabSeparatedWithNames
+
+تفاوت آن با فرمت `TabSeparated` در این است که، در این فرمت نام ستون ها در سطر اول قرار می گیرد. در طول پارس کردن، سطر اول به طور کامل نادیده گرفته می شود. شما نمی توانید نام ستون ها را برای تعیین موقعیت آنها یا بررسی صحت آنها استفاده کنید. (پشتیبانی از پارس کردن سطر header ممکن است در آینده اضافه شود.)
+
+همچنین این فرمت تحت عنوان ` TSVWithNames`وجود دارد.
+
+## TabSeparatedWithNamesAndTypes
+
+تفاوت آن با `TabSeparated` در این است که در این فرمت نام ستون ها در سطر اول نوشته می شود، و type ستون ها در سطر دوم نوشته می شود. در طی پارسینگ، سطر اول و دوم به طور کامل نادیده گرفته می شوند.
+
+همچنین این فرمت تحت عنوان ` TSVWithNamesAndTypes`وجود دارد.
+
+## TSKV
+
+مشابه فرمت TabSeparated، اما خروجی به صورت name=value می باشد. نام ها مشابه روش TabSeparated، escape می شوند، و همچنین = symbol هم escape می شود.
+
+</div>
+
+```text
+SearchPhrase=   count()=8267016
+SearchPhrase=bathroom interior design    count()=2166
+SearchPhrase=yandex     count()=1655
+SearchPhrase=spring 2014 fashion    count()=1549
+SearchPhrase=freeform photos       count()=1480
+SearchPhrase=angelina jolia    count()=1245
+SearchPhrase=omsk       count()=1112
+SearchPhrase=photos of dog breeds    count()=1091
+SearchPhrase=curtain design        count()=1064
+SearchPhrase=baku       count()=1000
+```
+
+<div dir="rtl" markdown="1">
+
+وقتی تعداد زیادی از ستون ها وجود دارد، این فرمت بی فایده است، و در حالت کلی دلیلی بر استفاده از این فرمت در این مواقع وجود ندارد. این فرمت در بعضی از دپارتمان های Yandex استفاده می شد.
+
+خروجی داده ها و پارس کردن هر دو در این فرمت پشتیبانی می شوند. برای پارس کردن، هر ترتیبی برای مقادیر ستون های مختلف پشتیبانی می شود. حذف بعضی از مقادیر قابل قبول است. این مقادیر با مقادیر پیش فرض خود برابر هستند. در این مورد، صفر و سطر خالی، توسط مقادیر پیش فرض پر می شوند. مقادیر پیچیده ای که می تواند در جدول مشخص شود به عنوان پیش فرض در این فرمت پشتیبانی نمیشوند.
+
+پارس کردن، اجازه می دهد که فیلد اضافه ی `tskv` بدون علامت و مقدار وجود داشته باشد. این فیلد نادیده گرفته می شود.
+
 ## Values
 
 هر سطر داخل براکت چاپ می شود. سطر ها توسط comma جدا می شوند. برای آخرین سطر comma وجود ندارد. مقادیر داخل براکت همچنین توسط comma جدا می شوند. اعداد با فرمت decimal و بدون کوتیشن چاپ می شوند. آرایه ها در براکت ها چاپ می شوند. رشته ها، تاریخ و تاریخ با ساعت داخل کوتیشن قرار می گیرند. قوانین escape و پارس کردن شبیه به فرمت TabSeparated انجام می شود. در طول فرمت، extra spaces درج نمی شوند، اما در هنگام پارس کردن، آنها مجاز و skip می شوند. (به جز space های داخل مقادیر آرایه، که مجاز نیستند).
@@ -548,40 +579,4 @@ test: string with \'quotes\' and \t with some special \n characters
 
 آرایه ها به شکل `<array><elem>Hello</elem><elem>World</elem>...</array>` و tuple ها به صورت `<tuple><elem>Hello</elem><elem>World</elem>...</tuple>` در خروجی می آیند.
 
-<a name="format_capnproto"></a>
-
-## CapnProto
-
-Cap'n Proto یک فرمت پیام باینری شبیه به Protocol Buffer و Thrift می باشد، اما شبیه به JSON یا MessagePack نیست.
-
-پیغام های Cap'n Proto به صورت self-describing نیستند، به این معنی که آنها نیاز دارند که به صورت external، schema آنها شرح داده شود. schema به صورت on the fly اضافه می شود و برای هر query، cache می شود.
-
-</div>
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits
-       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
-```
-
-<div dir="rtl" markdown="1">
-
-جایی که `schema.capnp` شبیه این است:
-
-</div>
-
-```
-struct Message {
-  SearchPhrase @0 :Text;
-  c @1 :Uint64;
-}
-```
-
-<div dir="rtl" markdown="1">
-
-فایل های Schema در فایلی قرار دارند که این فایل در دایرکتوری مشخص شده کانفیگ [ format_schema_path](../operations/server_settings/settings.md#server_settings-format_schema_path) قرار گرفته اند.
-
-عملیات Deserialization موثر است و معمولا لود سیستم را افزایش نمی دهد.
-
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/formats/) <!--hide-->
diff --git a/docs/fa/interfaces/http_interface.md b/docs/fa/interfaces/http_interface.md
index 418a92f4d..3251b2d4c 100644
--- a/docs/fa/interfaces/http_interface.md
+++ b/docs/fa/interfaces/http_interface.md
@@ -280,5 +280,3 @@ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&buffer_size=3000000&wa
 از بافرینگ به منظور اجتناب از شرایطی که یک خطای پردازش query رخ داده بعد از response کد و هدر های ارسال شده به کلاینت استفاده کنید. در این شرایط، پیغام خطا در انتهای بنده response نوشته می شود، و در سمت کلاینت، پیغام خطا فقط از طریق مرحله پارس کردن قابل شناسایی است.
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/http_interface/) <!--hide-->
diff --git a/docs/fa/interfaces/index.md b/docs/fa/interfaces/index.md
index d06c8ec8f..4d4600bd2 100644
--- a/docs/fa/interfaces/index.md
+++ b/docs/fa/interfaces/index.md
@@ -6,5 +6,4 @@
 
 برای کشف قابلیت های سیستم، دانلو داده ها به جداول، یا ساخت query های دستی، از برنامه clikhouse-client استفاده کنید.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/interfaces/jdbc.md b/docs/fa/interfaces/jdbc.md
index e72b5504c..dc0acb159 100644
--- a/docs/fa/interfaces/jdbc.md
+++ b/docs/fa/interfaces/jdbc.md
@@ -11,5 +11,4 @@ JDBC drivers implemented by other organizations:
 
 - [ClickHouse-Native-JDBC](https://github.com/housepower/ClickHouse-Native-JDBC)
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/jdbc/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/interfaces/tcp.md b/docs/fa/interfaces/tcp.md
index 5e767b876..963555b54 100644
--- a/docs/fa/interfaces/tcp.md
+++ b/docs/fa/interfaces/tcp.md
@@ -4,5 +4,4 @@
 
 native interface در محیط  ترمینال "clickhouse-client" برای تعامل بین سرور با پردازش query توزیع شده مورد استفاده قرار می گیرد. همچنین native interface در برنامه های C++ مورد استفاده قرار می گیرد. ما فقط کلاینت command-line را پوشش میدیم.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/tcp/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/interfaces/third-party_client_libraries.md b/docs/fa/interfaces/third-party_client_libraries.md
index 3f6536ccf..2fb5de3cc 100644
--- a/docs/fa/interfaces/third-party_client_libraries.md
+++ b/docs/fa/interfaces/third-party_client_libraries.md
@@ -47,5 +47,4 @@
 
 ما این کتابخانه ها را تست نکردیم. آنها به صورت تصادفی انتخاب شده اند.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/third-party_client_libraries/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/interfaces/third-party_gui.md b/docs/fa/interfaces/third-party_gui.md
index 7f537b1c2..8972edd88 100644
--- a/docs/fa/interfaces/third-party_gui.md
+++ b/docs/fa/interfaces/third-party_gui.md
@@ -35,5 +35,4 @@ interface تحت وب برای ClickHouse در پروژه [Tabix](https://github
 - مانیتورینگ کافکا و جداول replicate (بزودی);
 - و بسیاری از ویژگی های دیگر برای شما.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/interfaces/third-party_gui/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/introduction/distinctive_features.md b/docs/fa/introduction/distinctive_features.md
index 94d4198e1..61b2115a3 100644
--- a/docs/fa/introduction/distinctive_features.md
+++ b/docs/fa/introduction/distinctive_features.md
@@ -63,5 +63,4 @@ ClickHouse از روش asynchronous multimaster replication استفاده می
 
 برای اطلاعات بیشتر، به بخش [replication داده ها](../operations/table_engines/replication.md#table_engines-replication) مراجعه کنید.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/introduction/distinctive_features/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/introduction/features_considered_disadvantages.md b/docs/fa/introduction/features_considered_disadvantages.md
index 3a963d051..0c9e1b0cf 100644
--- a/docs/fa/introduction/features_considered_disadvantages.md
+++ b/docs/fa/introduction/features_considered_disadvantages.md
@@ -7,5 +7,3 @@
 3. Sparse index باعث می شود ClickHouse چندان مناسب اجرای پرسمان های point query برای دریافت یک ردیف از داده ها با استفاده از کلید آنها نباشد.
 
 </div>
-
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/introduction/features_considered_disadvantages/) <!--hide-->
diff --git a/docs/fa/introduction/performance.md b/docs/fa/introduction/performance.md
index 2fb2a9dc0..8d738cdbf 100644
--- a/docs/fa/introduction/performance.md
+++ b/docs/fa/introduction/performance.md
@@ -24,5 +24,4 @@ benchmark های زیادی وجود دارند که این نتایج را تا
 
 پیشنهاد می کنیم درج داده ها را به صورت دسته ای و حداقل 100 سطر در هر دسته انجام دهید و یا بیش از یک درخواست insert در ثانیه را نداشته باشید. در هنگام درج داده در جدول MergeTree از یک dump جدا شده با tab، سرعت درج داده از 50 تا 200 مگابایت در ثانیه می باشد. اگر سطر های درج شده حدود 1 کیلوبایت باشند، سرعت حدود 50 هزار تا 200 هزار سطر در ثانیه می باشد. اگر سطر ها کوچک باشند بازدهی بالایی در تعداد سطر در ثانیه خواهیم داشت. در Banner System Data -`>` 500 هزار سطر در ثانیه، در Graphite data -`>` 1 میلیون سطر در ثانیه). برای بهبود کارایی، شما می توانید چندین insert را به صورت موازی اجرا کنید، که در این حالت کارایی سیستم به صورت خطی افزایش می یابد.
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/introduction/performance/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/introduction/ya_metrika_task.md b/docs/fa/introduction/ya_metrika_task.md
index 1ea434f24..d1f4aa1d1 100644
--- a/docs/fa/introduction/ya_metrika_task.md
+++ b/docs/fa/introduction/ya_metrika_task.md
@@ -45,5 +45,4 @@ Yandex.Metrica دارای یک سیستم تخصصی برای aggregate کردن
 
 برای حذف محدودیت های OLAPServer و حل مشکلات کار با داده های Non-Aggregate برای تمام گزارش ها، ما مدیریت دیتابیس ClicHouse را توسعه دادیم..
 
-</div>
-[مقاله اصلی](https://clickhouse.yandex/docs/fa/introduction/ya_metrika_task/) <!--hide-->
+</div>
\ No newline at end of file
diff --git a/docs/fa/query_language/table_functions/jdbc.md b/docs/fa/query_language/table_functions/jdbc.md
deleted file mode 120000
index 73bec80ca..000000000
--- a/docs/fa/query_language/table_functions/jdbc.md
+++ /dev/null
@@ -1 +0,0 @@
-../../../en/query_language/table_functions/jdbc.md
\ No newline at end of file
diff --git a/docs/ru/data_types/array.md b/docs/ru/data_types/array.md
index 450c4cf51..73b86d7c9 100644
--- a/docs/ru/data_types/array.md
+++ b/docs/ru/data_types/array.md
@@ -81,5 +81,3 @@ Code: 386. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception
 
 0 rows in set. Elapsed: 0.246 sec.
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/array/) <!--hide-->
diff --git a/docs/ru/data_types/boolean.md b/docs/ru/data_types/boolean.md
index b85f1393a..541f8ef03 100644
--- a/docs/ru/data_types/boolean.md
+++ b/docs/ru/data_types/boolean.md
@@ -1,5 +1,3 @@
 # Булевы значения
 
 Отдельного типа для булевых значений нет. Для них используется тип UInt8, в котором используются только значения 0 и 1.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/boolean/) <!--hide-->
diff --git a/docs/ru/data_types/date.md b/docs/ru/data_types/date.md
index 887bb19e9..c458deaf6 100644
--- a/docs/ru/data_types/date.md
+++ b/docs/ru/data_types/date.md
@@ -6,5 +6,3 @@
 Минимальное значение выводится как 0000-00-00.
 
 Дата хранится без учёта часового пояса.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/date/) <!--hide-->
diff --git a/docs/ru/data_types/datetime.md b/docs/ru/data_types/datetime.md
index d9f5457e3..dfc54e43a 100644
--- a/docs/ru/data_types/datetime.md
+++ b/docs/ru/data_types/datetime.md
@@ -14,5 +14,3 @@
 Поддерживаются только часовые пояса, для которых для всего диапазона времён, с которым вы будете работать, не существовало моментов времени, в которые время отличалось от UTC на нецелое число часов (без учёта секунд координации).
 
 То есть, при работе с датой в виде текста (например, при сохранении текстовых дампов), следует иметь ввиду о проблемах с неоднозначностью во время перевода стрелок назад, и о проблемах с соответствием данных, при смене часового пояса.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/datetime/) <!--hide-->
diff --git a/docs/ru/data_types/decimal.md b/docs/ru/data_types/decimal.md
index cc0a3e476..9e554395a 100644
--- a/docs/ru/data_types/decimal.md
+++ b/docs/ru/data_types/decimal.md
@@ -93,5 +93,3 @@ SELECT toDecimal32(1, 8) < 100
 ```
 DB::Exception: Can't compare.
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/decimal/) <!--hide-->
diff --git a/docs/ru/data_types/enum.md b/docs/ru/data_types/enum.md
index 62250d866..aaae8aad3 100644
--- a/docs/ru/data_types/enum.md
+++ b/docs/ru/data_types/enum.md
@@ -110,5 +110,3 @@ INSERT INTO t_enum_null Values('hello'),('world'),(NULL)
 При ALTER, есть возможность бесплатно изменить тип Enum-а, если меняется только множество значений. При этом, можно добавлять новые значения; можно удалять старые значения (это безопасно только если они ни разу не использовались, так как это не проверяется). В качестве "защиты от дурака", нельзя менять числовые значения у имеющихся строк - в этом случае, кидается исключение.
 
 При ALTER, есть возможность поменять Enum8 на Enum16 и обратно - так же, как можно поменять Int8 на Int16.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/enum/) <!--hide-->
diff --git a/docs/ru/data_types/fixedstring.md b/docs/ru/data_types/fixedstring.md
index 5c7fa19d5..75fb9650b 100644
--- a/docs/ru/data_types/fixedstring.md
+++ b/docs/ru/data_types/fixedstring.md
@@ -7,5 +7,3 @@
 Обратите внимание, как это поведение отличается от поведения MySQL для типа CHAR (строки дополняются пробелами, пробелы перед выводом вырезаются).
 
 С типом FixedString(N) умеет работать меньше функций, чем с типом String - то есть, он менее удобен в использовании.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/fixedstring/) <!--hide-->
diff --git a/docs/ru/data_types/float.md b/docs/ru/data_types/float.md
index 8f50419cf..228fd9319 100644
--- a/docs/ru/data_types/float.md
+++ b/docs/ru/data_types/float.md
@@ -13,7 +13,7 @@
 
 - При вычислениях с числами с плавающей запятой возможна ошибка округления.
 
-``` sql
+```sql
 SELECT 1 - 0.9
 ```
 ```
@@ -32,7 +32,7 @@ SELECT 1 - 0.9
 
 - `Inf` - бесконечность.
 
-``` sql
+```sql
 SELECT 0.5 / 0
 ```
 
@@ -44,7 +44,7 @@ SELECT 0.5 / 0
 
 - `-Inf` - отрицательная бесконечность;
 
-``` sql
+```sql
 SELECT -0.5 / 0
 ```
 
@@ -66,5 +66,3 @@ SELECT 0 / 0
 ```
 
   Смотрите правила сортировки `NaN` в разделе [Секция ORDER BY](../query_language/select.md#query_language-queries-order_by).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/float/) <!--hide-->
diff --git a/docs/ru/data_types/index.md b/docs/ru/data_types/index.md
index a167ec385..e41d457ee 100644
--- a/docs/ru/data_types/index.md
+++ b/docs/ru/data_types/index.md
@@ -4,5 +4,4 @@
 
 ClickHouse может сохранять в ячейках таблиц данные различных типов. 
 
-Раздел содержит описания поддерживаемых типов данных и специфику их использования и/или реализации, если таковые имеются.
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/) <!--hide-->
+Раздел содержит описания поддерживаемых типов данных и специфику их использования и/или реализации, если таковые имеются.
\ No newline at end of file
diff --git a/docs/ru/data_types/int_uint.md b/docs/ru/data_types/int_uint.md
index 562da33e7..49b7bbbbc 100644
--- a/docs/ru/data_types/int_uint.md
+++ b/docs/ru/data_types/int_uint.md
@@ -19,5 +19,3 @@
 - UInt16 - [ 0 : 65535 ]
 - UInt32 - [ 0 : 4294967295 ]
 - UInt64 - [ 0 : 18446744073709551615 ]
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/int_uint/) <!--hide-->
diff --git a/docs/ru/data_types/nested_data_structures/aggregatefunction.md b/docs/ru/data_types/nested_data_structures/aggregatefunction.md
index 51acc27b2..a15205cf1 100644
--- a/docs/ru/data_types/nested_data_structures/aggregatefunction.md
+++ b/docs/ru/data_types/nested_data_structures/aggregatefunction.md
@@ -1,5 +1,3 @@
 # AggregateFunction(name, types_of_arguments...)
 
 Промежуточное состояние агрегатной функции. Чтобы его получить, используются агрегатные функции с суффиксом -State. Подробнее смотрите в разделе "AggregatingMergeTree".
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/nested_data_structures/aggregatefunction/) <!--hide-->
diff --git a/docs/ru/data_types/nested_data_structures/index.md b/docs/ru/data_types/nested_data_structures/index.md
index 3e5ea1bd6..b7bc28de3 100644
--- a/docs/ru/data_types/nested_data_structures/index.md
+++ b/docs/ru/data_types/nested_data_structures/index.md
@@ -1,2 +1 @@
-# Вложенные структуры данных
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/nested_data_structures/) <!--hide-->
+# Вложенные структуры данных
\ No newline at end of file
diff --git a/docs/ru/data_types/nested_data_structures/nested.md b/docs/ru/data_types/nested_data_structures/nested.md
index 06a62801e..6db4e9e83 100644
--- a/docs/ru/data_types/nested_data_structures/nested.md
+++ b/docs/ru/data_types/nested_data_structures/nested.md
@@ -4,7 +4,7 @@
 
 Пример:
 
-``` sql
+```sql
 CREATE TABLE test.visits
 (
     CounterID UInt32,
@@ -35,7 +35,7 @@ CREATE TABLE test.visits
 
 Пример:
 
-``` sql
+```sql
 SELECT
     Goals.ID,
     Goals.EventTime
@@ -44,7 +44,7 @@ WHERE CounterID = 101500 AND length(Goals.ID) < 5
 LIMIT 10
 ```
 
-```
+```text
 ┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐
 │ [1073752,591325,591325]        │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27']                       │
 │ [1073752]                      │ ['2014-03-17 00:28:25']                                                                   │
@@ -63,7 +63,7 @@ LIMIT 10
 
 Единственное место, где в запросе SELECT можно указать имя целой вложенной структуры данных, а не отдельных столбцов - секция ARRAY JOIN. Подробнее см. раздел "Секция ARRAY JOIN". Пример:
 
-``` sql
+```sql
 SELECT
     Goal.ID,
     Goal.EventTime
@@ -73,7 +73,7 @@ WHERE CounterID = 101500 AND length(Goals.ID) < 5
 LIMIT 10
 ```
 
-```
+```text
 ┌─Goal.ID─┬──────Goal.EventTime─┐
 │ 1073752 │ 2014-03-17 16:38:10 │
 │  591325 │ 2014-03-17 16:38:48 │
@@ -95,5 +95,3 @@ LIMIT 10
 При запросе DESCRIBE, столбцы вложенной структуры данных перечисляются так же по отдельности.
 
 Работоспособность запроса ALTER для элементов вложенных структур данных, является сильно ограниченной.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/nested_data_structures/nested/) <!--hide-->
diff --git a/docs/ru/data_types/nullable.md b/docs/ru/data_types/nullable.md
index e4b3bb830..f098cac2f 100644
--- a/docs/ru/data_types/nullable.md
+++ b/docs/ru/data_types/nullable.md
@@ -61,5 +61,3 @@ FROM t_null
 2 rows in set. Elapsed: 0.144 sec.
 
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/nullable/) <!--hide-->
diff --git a/docs/ru/data_types/special_data_types/expression.md b/docs/ru/data_types/special_data_types/expression.md
index 438492c53..8451a0347 100644
--- a/docs/ru/data_types/special_data_types/expression.md
+++ b/docs/ru/data_types/special_data_types/expression.md
@@ -1,5 +1,3 @@
 # Expression
 
 Используется для представления лямбда-выражений в функциях высшего порядка.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/special_data_types/expression/) <!--hide-->
diff --git a/docs/ru/data_types/special_data_types/index.md b/docs/ru/data_types/special_data_types/index.md
index b9eda72c5..9adb03853 100644
--- a/docs/ru/data_types/special_data_types/index.md
+++ b/docs/ru/data_types/special_data_types/index.md
@@ -1,5 +1,3 @@
 # Служебные типы данных
 
 Значения служебных типов данных не могут сохраняться в таблицу и выводиться в качестве результата, а возникают как промежуточный результат выполнения запроса.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/special_data_types/) <!--hide-->
diff --git a/docs/ru/data_types/special_data_types/nothing.md b/docs/ru/data_types/special_data_types/nothing.md
index c452aa2f1..6b83c354d 100644
--- a/docs/ru/data_types/special_data_types/nothing.md
+++ b/docs/ru/data_types/special_data_types/nothing.md
@@ -17,5 +17,3 @@ SELECT toTypeName([])
 
 1 rows in set. Elapsed: 0.062 sec.
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/special_data_types/nothing/) <!--hide-->
diff --git a/docs/ru/data_types/special_data_types/set.md b/docs/ru/data_types/special_data_types/set.md
index fe31450ab..72a9a2647 100644
--- a/docs/ru/data_types/special_data_types/set.md
+++ b/docs/ru/data_types/special_data_types/set.md
@@ -1,5 +1,3 @@
 # Set
 
 Используется для представления правой части выражения IN.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/special_data_types/set/) <!--hide-->
diff --git a/docs/ru/data_types/string.md b/docs/ru/data_types/string.md
index 18d18eecb..f4a100470 100644
--- a/docs/ru/data_types/string.md
+++ b/docs/ru/data_types/string.md
@@ -11,5 +11,3 @@
 Если вам нужно хранить тексты, рекомендуется использовать кодировку UTF-8. По крайней мере, если у вас терминал работает в кодировке UTF-8 (это рекомендуется), вы сможете читать и писать свои значения без каких-либо преобразований.
 Также, некоторые функции по работе со строками, имеют отдельные варианты, которые работают при допущении, что строка содержит набор байт, представляющий текст в кодировке UTF-8.
 Например, функция length вычисляет длину строки в байтах, а функция lengthUTF8 - длину строки в кодовых точках Unicode, при допущении, что значение в кодировке UTF-8.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/string/) <!--hide-->
diff --git a/docs/ru/data_types/tuple.md b/docs/ru/data_types/tuple.md
index cd954a2e4..66aca46d9 100644
--- a/docs/ru/data_types/tuple.md
+++ b/docs/ru/data_types/tuple.md
@@ -51,5 +51,3 @@ SELECT
 
 1 rows in set. Elapsed: 0.002 sec.
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/data_types/tuple/) <!--hide-->
diff --git a/docs/ru/development/style.md b/docs/ru/development/style.md
index 4bcff77ca..f5470b48f 100644
--- a/docs/ru/development/style.md
+++ b/docs/ru/development/style.md
@@ -836,5 +836,3 @@ function(
       const & RangesInDataParts ranges,
       size_t limit)
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/development/style/) <!--hide-->
diff --git a/docs/ru/faq/general.md b/docs/ru/faq/general.md
index 8cecad1aa..15a19bbeb 100644
--- a/docs/ru/faq/general.md
+++ b/docs/ru/faq/general.md
@@ -9,5 +9,4 @@
 Распределённая сортировка не является оптимальным способом выполнения операции reduce, если результат выполнения операции и все промежуточные результаты, при их наличии, помещаются в оперативку на одном сервере, как обычно бывает в запросах, выполняющихся в режиме онлайн. В таком случае, оптимальным способом выполнения операции reduce является хэш-таблица. Частым способом оптимизации map-reduce задач является предагрегация (частичный reduce) с использованием хэш-таблицы в оперативной памяти. Эта оптимизация делается пользователем в ручном режиме.
 Распределённая сортировка является основной причиной тормозов при выполнении несложных map-reduce задач.
 
-Большинство реализаций MapReduce позволяют выполнять произвольный код на кластере. Но для OLAP задач лучше подходит декларативный язык запросов, который позволяет быстро проводить исследования. Для примера, для Hadoop существует Hive и Pig. Также смотрите Cloudera Impala, Shark (устаревший) для Spark, а также Spark SQL, Presto, Apache Drill. Впрочем, производительность при выполнении таких задач является сильно неоптимальной по сравнению со специализированными системами, а сравнительно высокая latency не позволяет использовать эти системы в качестве бэкенда для веб-интерфейса.
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/faq/general/) <!--hide-->
+Большинство реализаций MapReduce позволяют выполнять произвольный код на кластере. Но для OLAP задач лучше подходит декларативный язык запросов, который позволяет быстро проводить исследования. Для примера, для Hadoop существует Hive и Pig. Также смотрите Cloudera Impala, Shark (устаревший) для Spark, а также Spark SQL, Presto, Apache Drill. Впрочем, производительность при выполнении таких задач является сильно неоптимальной по сравнению со специализированными системами, а сравнительно высокая latency не позволяет использовать эти системы в качестве бэкенда для веб-интерфейса.
\ No newline at end of file
diff --git a/docs/ru/getting_started/example_datasets/amplab_benchmark.md b/docs/ru/getting_started/example_datasets/amplab_benchmark.md
index 87b8de2be..602a12eae 100644
--- a/docs/ru/getting_started/example_datasets/amplab_benchmark.md
+++ b/docs/ru/getting_started/example_datasets/amplab_benchmark.md
@@ -22,7 +22,7 @@ cd ..
 
 Выполните следующие запросы к ClickHouse:
 
-``` sql
+```sql
 CREATE TABLE rankings_tiny
 (
     pageURL String,
@@ -97,7 +97,7 @@ for i in 5nodes/uservisits/*.deflate; do echo $i; zlib-flate -uncompress < $i |
 
 Запросы для получения выборок данных:
 
-``` sql
+```sql
 SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank > 1000
 
 SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8)
@@ -119,5 +119,3 @@ GROUP BY sourceIP
 ORDER BY totalRevenue DESC
 LIMIT 1
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/example_datasets/amplab_benchmark/) <!--hide-->
diff --git a/docs/ru/getting_started/example_datasets/criteo.md b/docs/ru/getting_started/example_datasets/criteo.md
index 32fc8e234..2d2f44242 100644
--- a/docs/ru/getting_started/example_datasets/criteo.md
+++ b/docs/ru/getting_started/example_datasets/criteo.md
@@ -4,7 +4,7 @@
 
 Создайте таблицу для импорта лога:
 
-``` sql
+```sql
 CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log
 ```
 
@@ -16,7 +16,7 @@ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/
 
 Создайте таблицу для сконвертированных данных:
 
-``` sql
+```sql
 CREATE TABLE criteo
 (
     date Date,
@@ -65,10 +65,8 @@ CREATE TABLE criteo
 
 Преобразуем данные из сырого лога и положим во вторую таблицу:
 
-``` sql
+```sql
 INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log;
 
 DROP TABLE criteo_log;
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/example_datasets/criteo/) <!--hide-->
diff --git a/docs/ru/getting_started/example_datasets/nyc_taxi.md b/docs/ru/getting_started/example_datasets/nyc_taxi.md
index d0402b9e2..f66f37de8 100644
--- a/docs/ru/getting_started/example_datasets/nyc_taxi.md
+++ b/docs/ru/getting_started/example_datasets/nyc_taxi.md
@@ -24,7 +24,7 @@ mv data/yellow_tripdata_2010-03.csv_ data/yellow_tripdata_2010-03.csv
 
 Проверить количество загруженных строк можно следующим образом:
 
-```
+```text
 time psql nyc-taxi-data -c "SELECT count(*) FROM trips;"
 ##    count
  1298979494
@@ -39,7 +39,7 @@ real    7m9.164s
 
 Экспорт данных из PostgreSQL:
 
-``` sql
+```sql
 COPY
 (
     SELECT trips.id,
@@ -114,7 +114,7 @@ COPY
 
 Создание временной таблицы в ClickHouse:
 
-``` sql
+```sql
 CREATE TABLE trips
 (
 trip_id                 UInt32,
@@ -173,7 +173,7 @@ dropoff_puma            Nullable(String)
 
 Она нужна для преобразование полей к более правильным типам данных и, если возможно, чтобы избавиться от NULL'ов.
 
-```
+```text
 time clickhouse-client --query="INSERT INTO trips FORMAT TabSeparated" < trips.tsv
 
 real    75m56.214s
@@ -191,7 +191,7 @@ real    75m56.214s
 
 Создадим и заполним итоговую таблицу:
 
-```
+```text
 CREATE TABLE trips_mergetree
 ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
 AS SELECT
@@ -258,7 +258,7 @@ FROM trips
 
 Таблица заняла 126 Гб дискового пространства.
 
-```
+```text
 :) SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active
 
 SELECT formatReadableSize(sum(bytes))
@@ -276,7 +276,7 @@ WHERE (table = 'trips_mergetree') AND active
 
 Q1:
 
-``` sql
+```sql
 SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
 ```
 
@@ -284,7 +284,7 @@ SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type
 
 Q2:
 
-``` sql
+```sql
 SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count
 ```
 
@@ -292,7 +292,7 @@ SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenge
 
 Q3:
 
-``` sql
+```sql
 SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year
 ```
 
@@ -300,7 +300,7 @@ SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetr
 
 Q4:
 
-``` sql
+```sql
 SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*)
 FROM trips_mergetree
 GROUP BY passenger_count, year, distance
@@ -322,19 +322,19 @@ ORDER BY year, count(*) DESC
 
 На каждом сервере:
 
-```
+```text
 CREATE TABLE default.trips_mergetree_third ( trip_id UInt32,  vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14),  pickup_date Date,  pickup_datetime DateTime,  dropoff_date Date,  dropoff_datetime DateTime,  store_and_fwd_flag UInt8,  rate_code_id UInt8,  pickup_longitude Float64,  pickup_latitude Float64,  dropoff_longitude Float64,  dropoff_latitude Float64,  passenger_count UInt8,  trip_distance Float64,  fare_amount Float32,  extra Float32,  mta_tax Float32,  tip_amount Float32,  tolls_amount Float32,  ehail_fee Float32,  improvement_surcharge Float32,  total_amount Float32,  payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4),  trip_type UInt8,  pickup FixedString(25),  dropoff FixedString(25),  cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3),  pickup_nyct2010_gid UInt8,  pickup_ctlabel Float32,  pickup_borocode UInt8,  pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  pickup_ct2010 FixedString(6),  pickup_boroct2010 FixedString(7),  pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  pickup_ntacode FixedString(4),  pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  pickup_puma UInt16,  dropoff_nyct2010_gid UInt8,  dropoff_ctlabel Float32,  dropoff_borocode UInt8,  dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5),  dropoff_ct2010 FixedString(6),  dropoff_boroct2010 FixedString(7),  dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2),  dropoff_ntacode FixedString(4),  dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195),  dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)
 ```
 
 На исходном сервере:
 
-``` sql
+```sql
 CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())
 ```
 
 Следующим запрос перераспределит данные:
 
-``` sql
+```sql
 INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree
 ```
 
@@ -366,5 +366,3 @@ Q4: 0.072 sec.
 |       1 | 0.490 | 1.224 | 2.104 | 3.593 |
 |       3 | 0.212 | 0.438 | 0.733 | 1.241 |
 |     140 | 0.028 | 0.043 | 0.051 | 0.072 |
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/example_datasets/nyc_taxi/) <!--hide-->
diff --git a/docs/ru/getting_started/example_datasets/ontime.md b/docs/ru/getting_started/example_datasets/ontime.md
index fa832c998..afd4de068 100644
--- a/docs/ru/getting_started/example_datasets/ontime.md
+++ b/docs/ru/getting_started/example_datasets/ontime.md
@@ -18,7 +18,7 @@ done
 
 Создание таблицы:
 
-``` sql
+```sql
 CREATE TABLE `ontime` (
   `Year` UInt16,
   `Quarter` UInt8,
@@ -142,37 +142,37 @@ for i in *.zip; do echo $i; unzip -cq $i '*.csv' | sed 's/\.00//g' | clickhouse-
 
 Q0.
 
-``` sql
+```sql
 select avg(c1) from (select Year, Month, count(*) as c1 from ontime group by Year, Month);
 ```
 
 Q1. Количество полетов в день с 2000 по 2008 года
 
-``` sql
+```sql
 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE Year >= 2000 AND Year <= 2008 GROUP BY DayOfWeek ORDER BY c DESC;
 ```
 
 Q2. Количество полетов, задержанных более чем на 10 минут, с группировкой по дням неделе, за 2000-2008 года
 
-``` sql
+```sql
 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE DepDelay>10 AND Year >= 2000 AND Year <= 2008 GROUP BY DayOfWeek ORDER BY c DESC
 ```
 
 Q3. Количество задержек по аэропортам за 2000-2008
 
-``` sql
+```sql
 SELECT Origin, count(*) AS c FROM ontime WHERE DepDelay>10 AND Year >= 2000 AND Year <= 2008 GROUP BY Origin ORDER BY c DESC LIMIT 10
 ```
 
 Q4. Количество задержек по перевозчикам за 2007 год
 
-``` sql
+```sql
 SELECT Carrier, count(*) FROM ontime WHERE DepDelay>10  AND Year = 2007 GROUP BY Carrier ORDER BY count(*) DESC
 ```
 
 Q5. Процент задержек по перевозчикам за 2007 год
 
-``` sql
+```sql
 SELECT Carrier, c, c2, c*1000/c2 as c3
 FROM
 (
@@ -198,13 +198,13 @@ ORDER BY c3 DESC;
 
 Более оптимальная версия того же запроса:
 
-``` sql
+```sql
 SELECT Carrier, avg(DepDelay > 10) * 1000 AS c3 FROM ontime WHERE Year = 2007 GROUP BY Carrier ORDER BY Carrier
 ```
 
 Q6. Предыдущий запрос за более широкий диапазон лет, 2000-2008
 
-``` sql
+```sql
 SELECT Carrier, c, c2, c*1000/c2 as c3
 FROM
 (
@@ -230,13 +230,13 @@ ORDER BY c3 DESC;
 
 Более оптимальная версия того же запроса:
 
-``` sql
+```sql
 SELECT Carrier, avg(DepDelay > 10) * 1000 AS c3 FROM ontime WHERE Year >= 2000 AND Year <= 2008 GROUP BY Carrier ORDER BY Carrier
 ```
 
 Q7. Процент полетов, задержанных на более 10 минут, в разбивке по годам
 
-``` sql
+```sql
 SELECT Year, c1/c2
 FROM
 (
@@ -260,25 +260,25 @@ ORDER BY Year
 
 Более оптимальная версия того же запроса:
 
-``` sql
+```sql
 SELECT Year, avg(DepDelay > 10) FROM ontime GROUP BY Year ORDER BY Year
 ```
 
 Q8. Самые популярные направления по количеству напрямую соединенных городов для различных диапазонов лет
 
-``` sql
+```sql
 SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime WHERE Year >= 2000 and Year <= 2010 GROUP BY DestCityName ORDER BY u DESC LIMIT 10;
 ```
 
 Q9.
 
-``` sql
+```sql
 select Year, count(*) as c1 from ontime group by Year;
 ```
 
 Q10.
 
-``` sql
+```sql
 select
    min(Year), max(Year), Carrier, count(*) as cnt,
    sum(ArrDelayMinutes>30) as flights_delayed,
@@ -296,7 +296,7 @@ LIMIT 1000;
 
 Бонус:
 
-``` sql
+```sql
 SELECT avg(cnt) FROM (SELECT Year,Month,count(*) AS cnt FROM ontime WHERE DepDel15=1 GROUP BY Year,Month)
 
 select avg(c1) from (select Year,Month,count(*) as c1 from ontime group by Year,Month)
@@ -316,5 +316,3 @@ SELECT OriginCityName, count() AS c FROM ontime GROUP BY OriginCityName ORDER BY
 -   <https://www.percona.com/blog/2014/04/21/using-apache-hadoop-and-impala-together-with-mysql-for-data-analysis/>
 -   <https://www.percona.com/blog/2016/01/07/apache-spark-with-air-ontime-performance-data/>
 -   <http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html>
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/example_datasets/ontime/) <!--hide-->
diff --git a/docs/ru/getting_started/example_datasets/star_schema.md b/docs/ru/getting_started/example_datasets/star_schema.md
index ddff4c592..279daed3f 100644
--- a/docs/ru/getting_started/example_datasets/star_schema.md
+++ b/docs/ru/getting_started/example_datasets/star_schema.md
@@ -21,7 +21,7 @@ make
 
 Создание таблиц в ClickHouse:
 
-``` sql
+```sql
 CREATE TABLE lineorder (
         LO_ORDERKEY             UInt32,
         LO_LINENUMBER           UInt8,
@@ -82,5 +82,3 @@ CREATE TABLE partd AS part ENGINE = Distributed(perftest_3shards_1replicas, defa
 cat customer.tbl | sed 's/$/2000-01-01/' | clickhouse-client --query "INSERT INTO customer FORMAT CSV"
 cat lineorder.tbl | clickhouse-client --query "INSERT INTO lineorder FORMAT CSV"
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/example_datasets/star_schema/) <!--hide-->
diff --git a/docs/ru/getting_started/example_datasets/wikistat.md b/docs/ru/getting_started/example_datasets/wikistat.md
index ed8037ffc..5f0798926 100644
--- a/docs/ru/getting_started/example_datasets/wikistat.md
+++ b/docs/ru/getting_started/example_datasets/wikistat.md
@@ -4,7 +4,7 @@
 
 Создание таблицы:
 
-``` sql
+```sql
 CREATE TABLE wikistat
 (
     date Date,
@@ -24,5 +24,3 @@ for i in {2007..2016}; do for j in {01..12}; do echo $i-$j >&2; curl -sSL "http:
 cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pagecounts-raw/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1/')/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\.gz/\1-\2/')/$link; done
 ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time="$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\.gz/\1-\2-\3 \4-00-00/')" | clickhouse-client --query="INSERT INTO wikistat FORMAT TabSeparated"; done
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/example_datasets/wikistat/) <!--hide-->
diff --git a/docs/ru/getting_started/index.md b/docs/ru/getting_started/index.md
index 037dfb737..18e006f01 100644
--- a/docs/ru/getting_started/index.md
+++ b/docs/ru/getting_started/index.md
@@ -24,7 +24,7 @@ ClickHouse также работает на FreeBSD и Mac OS X; может бы
 
 Пропишите в `/etc/apt/sources.list` (или в отдельный файл `/etc/apt/sources.list.d/clickhouse.list`) репозитории:
 
-```
+```text
 deb http://repo.yandex.ru/clickhouse/deb/stable/ main/
 ```
 
@@ -51,14 +51,14 @@ ClickHouse содержит настройки ограничения досту
 Вы можете собрать пакеты и установить их.
 Также вы можете использовать программы без установки пакетов.
 
-```
+```text
 Client: dbms/programs/clickhouse-client
 Server: dbms/programs/clickhouse-server
 ```
 
 Для сервера создаёте директории с данными, например:
 
-```
+```text
 /opt/clickhouse/data/default/
 /opt/clickhouse/metadata/default/
 ```
@@ -136,5 +136,3 @@ SELECT 1
 **Поздравляем, система работает!**
 
 Для дальнейших экспериментов можно попробовать загрузить из тестовых наборов данных.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/getting_started/) <!--hide-->
diff --git a/docs/ru/index.md b/docs/ru/index.md
index a2f6945bd..f93ba6957 100644
--- a/docs/ru/index.md
+++ b/docs/ru/index.md
@@ -52,7 +52,7 @@ ClickHouse - столбцовая система управления базам
 -   транзакции отсутствуют;
 -   низкие требования к консистентности данных;
 -   в запросе одна большая таблица, все таблицы кроме одной маленькие;
--   результат выполнения запроса существенно меньше исходных данных - то есть, данные фильтруются или агрегируются; результат выполнения помещается в оперативку на одном сервере.
+-   результат выполнения запроса существенно меньше исходных данных - то есть, данные фильтруются или агрегируются; результат выполнения помещается в оперативку на одном сервере;
 
 Легко видеть, что OLAP сценарий работы существенно отличается от других распространённых сценариев работы (например, OLTP или Key-Value сценариев работы). Таким образом, не имеет никакого смысла пытаться использовать OLTP или Key-Value БД для обработки аналитических запросов, если вы хотите получить приличную производительность ("выше плинтуса"). Например, если вы попытаетесь использовать для аналитики MongoDB или Redis - вы получите анекдотически низкую производительность по сравнению с OLAP-СУБД.
 
@@ -78,8 +78,9 @@ ClickHouse - столбцовая система управления базам
 
 Для примера, для запроса "посчитать количество записей для каждой рекламной системы", требуется прочитать один столбец "идентификатор рекламной системы", который занимает 1 байт в несжатом виде. Если большинство переходов было не с рекламных систем, то можно рассчитывать хотя бы на десятикратное сжатие этого столбца. При использовании быстрого алгоритма сжатия, возможно разжатие данных со скоростью более нескольких гигабайт несжатых данных в секунду. То есть, такой запрос может выполняться со скоростью около нескольких миллиардов строк в секунду на одном сервере. На практике, такая скорость действительно достигается.
 
-<details markdown="1"><summary>Пример</summary>
-```
+<details><summary>Пример</summary>
+<p>
+<pre>
 $ clickhouse-client
 ClickHouse client version 0.0.52053.
 Connecting to localhost:9000.
@@ -121,7 +122,8 @@ LIMIT 20
 20 rows in set. Elapsed: 0.153 sec. Processed 1.00 billion rows, 4.00 GB (6.53 billion rows/s., 26.10 GB/s.)
 
 :)
-```
+</pre>
+</p>
 </details>
 
 ### По вычислениям
@@ -138,5 +140,3 @@ LIMIT 20
 В "обычных" БД этого не делается, так как не имеет смысла при выполнении простых запросов. Хотя есть исключения. Например, в MemSQL кодогенерация используется для уменьшения latency при выполнении SQL запросов. (Для сравнения - в аналитических СУБД, требуется оптимизация throughput, а не latency).
 
 Стоит заметить, что для эффективности по CPU требуется, чтобы язык запросов был декларативным (SQL, MDX) или хотя бы векторным (J, K). То есть, чтобы запрос содержал циклы только в неявном виде, открывая возможности для оптимизации.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/) <!--hide-->
diff --git a/docs/ru/interfaces/cli.md b/docs/ru/interfaces/cli.md
index ec7bc7d2c..09f21a64f 100644
--- a/docs/ru/interfaces/cli.md
+++ b/docs/ru/interfaces/cli.md
@@ -115,5 +115,4 @@ cat file.csv | clickhouse-client --database=test --query="INSERT INTO test FORMA
     <user>username</user>
     <password>password</password>
 </config>
-```
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/cli/) <!--hide-->
+```
\ No newline at end of file
diff --git a/docs/ru/interfaces/formats.md b/docs/ru/interfaces/formats.md
index 8d31b6707..6944713c9 100644
--- a/docs/ru/interfaces/formats.md
+++ b/docs/ru/interfaces/formats.md
@@ -32,130 +32,32 @@ ClickHouse может принимать (`INSERT`) и отдавать (`SELECT
 [XML](#xml) | ✗ | ✔ |
 [CapnProto](#capnproto) | ✔ | ✔ |
 
-<a name="tabseparated"></a>
-
-## TabSeparated
-
-В TabSeparated формате данные пишутся по строкам. Каждая строчка содержит значения, разделённые табами. После каждого значения идёт таб, кроме последнего значения в строке, после которого идёт перевод строки. Везде подразумеваются исключительно unix-переводы строк. Последняя строка также обязана содержать перевод строки на конце. Значения пишутся в текстовом виде, без обрамляющих кавычек, с экранированием служебных символов.
-
-Этот формат также доступен под именем `TSV`.
-
-Формат `TabSeparated` удобен для обработки данных произвольными программами и скриптами. Он используется по умолчанию в HTTP-интерфейсе, а также в batch-режиме клиента командной строки. Также формат позволяет переносить данные между разными СУБД. Например, вы можете получить дамп из MySQL и загрузить его в ClickHouse, или наоборот.
-
-Формат `TabSeparated` поддерживает вывод тотальных значений (при использовании WITH TOTALS) и экстремальных значений (при настройке extremes выставленной в 1). В этих случаях, после основных данных выводятся тотальные значения, и экстремальные значения. Основной результат, тотальные значения и экстремальные значения, отделяются друг от друга пустой строкой. Пример:
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
-```
-
-```
-2014-03-17      1406958
-2014-03-18      1383658
-2014-03-19      1405797
-2014-03-20      1353623
-2014-03-21      1245779
-2014-03-22      1031592
-2014-03-23      1046491
-
-0000-00-00      8873898
-
-2014-03-17      1031592
-2014-03-23      1406958
-```
-
-### Форматирование данных
-
-Целые числа пишутся в десятичной форме. Числа могут содержать лишний символ "+" в начале (игнорируется при парсинге, а при форматировании не пишется). Неотрицательные числа не могут содержать знак отрицания. При чтении допустим парсинг пустой строки, как числа ноль, или (для знаковых типов) строки, состоящей из одного минуса, как числа ноль. Числа, не помещающиеся в соответствующий тип данных, могут парсится, как некоторое другое число, без сообщения об ошибке.
-
-Числа с плавающей запятой пишутся в десятичной форме. При этом, десятичный разделитель - точка. Поддерживается экспоненциальная запись, а также inf, +inf, -inf, nan. Запись числа с плавающей запятой может начинаться или заканчиваться на десятичную точку.
-При форматировании возможна потеря точности чисел с плавающей запятой.
-При парсинге, допустимо чтение не обязательно наиболее близкого к десятичной записи машинно-представимого числа.
-
-Даты выводятся в формате YYYY-MM-DD, парсятся в том же формате, но с любыми символами в качестве разделителей.
-Даты-с-временем выводятся в формате YYYY-MM-DD hh:mm:ss, парсятся в том же формате, но с любыми символами в качестве разделителей.
-Всё это происходит в системном часовом поясе на момент старта клиента (если клиент занимается форматированием данных) или сервера. Для дат-с-временем не указывается, действует ли daylight saving time. То есть, если в дампе есть времена во время перевода стрелок назад, то дамп не соответствует данным однозначно, и при парсинге будет выбрано какое-либо из двух времён.
-При парсинге, некорректные даты и даты-с-временем могут парситься с естественным переполнением или как нулевые даты/даты-с-временем без сообщения об ошибке.
-
-В качестве исключения, поддерживается также парсинг даты-с-временем в формате unix timestamp, если он состоит ровно из 10 десятичных цифр. Результат не зависит от часового пояса. Различение форматов YYYY-MM-DD hh:mm:ss и NNNNNNNNNN делается автоматически.
-
-Строки выводятся с экранированием спец-символов с помощью обратного слеша. При выводе, используются следующие escape-последовательности: `\b`, `\f`, `\r`, `\n`, `\t`, `\0`, `\'`, `\\`. При парсинге, также поддерживаются последовательности `\a`, `\v`, а также `\xHH` (hex escape-последовательности) и любые последовательности вида `\c`, где `c` - любой символ - такие последовательности преобразуется в `c`. Таким образом, при чтении поддерживаются форматы, где перевод строки может быть записан как `\n` и как `\` и перевод строки. Например, строка `Hello world`, где между словами вместо пробела стоит перевод строки, может быть считана в любом из следующих вариантов:
-
-```
-Hello\nworld
-
-Hello\
-world
-```
-
-Второй вариант поддерживается, так как его использует MySQL при записи tab-separated дампа.
-
-Минимальный набор символов, которых вам необходимо экранировать при передаче в TabSeparated формате: таб, перевод строки (LF) и обратный слеш.
-
-Экранируется лишь небольшой набор символов. Вы можете легко наткнуться на строковое значение, которое испортит ваш терминал при выводе в него.
-
-Массивы форматируются в виде списка значений через запятую в квадратных скобках. Элементы массива - числа форматируются как обычно, а даты, даты-с-временем и строки - в одинарных кавычках с такими же правилами экранирования, как указано выше.
-
-[NULL](../query_language/syntax.md#null-literal) форматируется в виде `\N`.
-
-<a name="tabseparatedraw"></a>
-
-## TabSeparatedRaw
-
-Отличается от формата `TabSeparated` тем, что строки выводятся без экранирования.
-Этот формат подходит только для вывода результата выполнения запроса, но не для парсинга (приёма данных для вставки в таблицу).
-
-Этот формат также доступен под именем `TSVRaw`.
-<a name="tabseparatedwithnames"></a>
-
-## TabSeparatedWithNames
-
-Отличается от формата `TabSeparated` тем, что в первой строке пишутся имена столбцов.
-При парсинге, первая строка полностью игнорируется: вы не можете использовать имена столбцов, чтобы указать их порядок расположения, или чтобы проверить их корректность.
-(Поддержка обработки заголовка при парсинге может быть добавлена в будущем.)
-
-Этот формат также доступен под именем `TSVWithNames`.
-<a name="tabseparatedwithnamesandtypes"></a>
-
-## TabSeparatedWithNamesAndTypes
-
-Отличается от формата `TabSeparated` тем, что в первой строке пишутся имена столбцов, а во второй - типы столбцов.
-При парсинге, первая и вторая строка полностью игнорируется.
+<a name="format_capnproto"></a>
 
-Этот формат также доступен под именем `TSVWithNamesAndTypes`.
-<a name="tskv"></a>
+## CapnProto
 
-## TSKV
+Cap'n Proto - формат бинарных сообщений, похож на Protocol Buffers и Thrift, но не похож на JSON или MessagePack.
 
-Похож на TabSeparated, но выводит значения в формате name=value. Имена экранируются так же, как строки в формате TabSeparated и, дополнительно, экранируется также символ =.
+Сообщения Cap'n Proto строго типизированы и не самоописывающиеся, т.е. нуждаются во внешнем описании схемы. Схема применяется "на лету" и кешируется для каждого запроса.
 
-```
-SearchPhrase=   count()=8267016
-SearchPhrase=интерьер ванной комнаты    count()=2166
-SearchPhrase=яндекс     count()=1655
-SearchPhrase=весна 2014 мода    count()=1549
-SearchPhrase=фриформ фото       count()=1480
-SearchPhrase=анджелина джоли    count()=1245
-SearchPhrase=омск       count()=1112
-SearchPhrase=фото собак разных пород    count()=1091
-SearchPhrase=дизайн штор        count()=1064
-SearchPhrase=баку       count()=1000
+```sql
+SELECT SearchPhrase, count() AS c FROM test.hits
+       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
 ```
 
-[NULL](../query_language/syntax.md#null-literal) форматируется в виде `\N`.
+Где `schema.capnp` выглядит следующим образом:
 
-``` sql
-SELECT * FROM t_null FORMAT TSKV
-```
 ```
-x=1	y=\N
+struct Message {
+  SearchPhrase @0 :Text;
+  c @1 :Uint64;
+}
 ```
 
-При большом количестве маленьких столбцов, этот формат существенно неэффективен, и обычно нет причин его использовать. Он реализован, так как используется в некоторых отделах Яндекса.
-
-Поддерживается как вывод, так и парсинг данных в этом формате. При парсинге, поддерживается расположение значений разных столбцов в произвольном порядке. Допустимо отсутствие некоторых значений - тогда они воспринимаются как равные значениям по умолчанию. При этом, в качестве значений по умолчанию используются нули, пустые строки и не поддерживаются сложные значения по умолчанию, которые могут быть заданы в таблице.
 
-При парсинге, в качестве дополнительного поля, может присутствовать `tskv` без знака равенства и без значения. Это поле игнорируется.
+Файлы со схемами находятся в файле, который находится в каталоге указанном в параметре [format_schema_path](../operations/server_settings/settings.md#server_settings-format_schema_path) конфигурации сервера.
 
+Десериализация эффективна и обычно не повышает нагрузку на систему.
 <a name="csv"></a>
 
 ## CSV
@@ -185,7 +87,7 @@ clickhouse-client --format_csv_delimiter="|" --query="INSERT INTO test.csv FORMA
 
 Выводит данные в формате JSON. Кроме таблицы с данными, также выводятся имена и типы столбцов, и некоторая дополнительная информация - общее количество выведенных строк, а также количество строк, которое могло бы быть выведено, если бы не было LIMIT-а. Пример:
 
-``` sql
+```sql
 SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON
 ```
 
@@ -362,7 +264,7 @@ ClickHouse поддерживает [NULL](../query_language/syntax.md#null-lite
 
 [NULL](../query_language/syntax.md#null-literal) выводится как `ᴺᵁᴸᴸ`.
 
-``` sql
+```sql
 SELECT * FROM t_null
 ```
 ```
@@ -376,11 +278,11 @@ SELECT * FROM t_null
 
 Формат Pretty поддерживает вывод тотальных значений (при использовании WITH TOTALS) и экстремальных значений (при настройке extremes выставленной в 1). В этих случаях, после основных данных выводятся тотальные значения, и экстремальные значения, в отдельных табличках. Пример (показан для формата PrettyCompact):
 
-``` sql
+```sql
 SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact
 ```
 
-```
+```text
 ┌──EventDate─┬───────c─┐
 │ 2014-03-17 │ 1406958 │
 │ 2014-03-18 │ 1383658 │
@@ -457,6 +359,133 @@ Array представлены как длина в формате varint (unsig
 
 Для поддержки [NULL](../query_language/syntax.md#null-literal) перед каждым значением типа [Nullable](../data_types/nullable.md#data_type-nullable) в строке добавлен дополнительный байт, который содержит 1 или 0. Если 1, то значение — `NULL` и этот байт трактуется как отдельное значение. Если 0, то после байта идёт не `NULL`-значение.
 
+<a name="tabseparated"></a>
+
+## TabSeparated
+
+В TabSeparated формате данные пишутся по строкам. Каждая строчка содержит значения, разделённые табами. После каждого значения идёт таб, кроме последнего значения в строке, после которого идёт перевод строки. Везде подразумеваются исключительно unix-переводы строк. Последняя строка также обязана содержать перевод строки на конце. Значения пишутся в текстовом виде, без обрамляющих кавычек, с экранированием служебных символов.
+
+Этот формат также доступен под именем `TSV`.
+
+Формат `TabSeparated` удобен для обработки данных произвольными программами и скриптами. Он используется по умолчанию в HTTP-интерфейсе, а также в batch-режиме клиента командной строки. Также формат позволяет переносить данные между разными СУБД. Например, вы можете получить дамп из MySQL и загрузить его в ClickHouse, или наоборот.
+
+Формат `TabSeparated` поддерживает вывод тотальных значений (при использовании WITH TOTALS) и экстремальных значений (при настройке extremes выставленной в 1). В этих случаях, после основных данных выводятся тотальные значения, и экстремальные значения. Основной результат, тотальные значения и экстремальные значения, отделяются друг от друга пустой строкой. Пример:
+
+```sql
+SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
+```
+
+```text
+2014-03-17      1406958
+2014-03-18      1383658
+2014-03-19      1405797
+2014-03-20      1353623
+2014-03-21      1245779
+2014-03-22      1031592
+2014-03-23      1046491
+
+0000-00-00      8873898
+
+2014-03-17      1031592
+2014-03-23      1406958
+```
+
+
+
+## Форматирование данных
+
+Целые числа пишутся в десятичной форме. Числа могут содержать лишний символ "+" в начале (игнорируется при парсинге, а при форматировании не пишется). Неотрицательные числа не могут содержать знак отрицания. При чтении допустим парсинг пустой строки, как числа ноль, или (для знаковых типов) строки, состоящей из одного минуса, как числа ноль. Числа, не помещающиеся в соответствующий тип данных, могут парсится, как некоторое другое число, без сообщения об ошибке.
+
+Числа с плавающей запятой пишутся в десятичной форме. При этом, десятичный разделитель - точка. Поддерживается экспоненциальная запись, а также inf, +inf, -inf, nan. Запись числа с плавающей запятой может начинаться или заканчиваться на десятичную точку.
+При форматировании возможна потеря точности чисел с плавающей запятой.
+При парсинге, допустимо чтение не обязательно наиболее близкого к десятичной записи машинно-представимого числа.
+
+Даты выводятся в формате YYYY-MM-DD, парсятся в том же формате, но с любыми символами в качестве разделителей.
+Даты-с-временем выводятся в формате YYYY-MM-DD hh:mm:ss, парсятся в том же формате, но с любыми символами в качестве разделителей.
+Всё это происходит в системном часовом поясе на момент старта клиента (если клиент занимается форматированием данных) или сервера. Для дат-с-временем не указывается, действует ли daylight saving time. То есть, если в дампе есть времена во время перевода стрелок назад, то дамп не соответствует данным однозначно, и при парсинге будет выбрано какое-либо из двух времён.
+При парсинге, некорректные даты и даты-с-временем могут парситься с естественным переполнением или как нулевые даты/даты-с-временем без сообщения об ошибке.
+
+В качестве исключения, поддерживается также парсинг даты-с-временем в формате unix timestamp, если он состоит ровно из 10 десятичных цифр. Результат не зависит от часового пояса. Различение форматов YYYY-MM-DD hh:mm:ss и NNNNNNNNNN делается автоматически.
+
+Строки выводятся с экранированием спец-символов с помощью обратного слеша. При выводе, используются следующие escape-последовательности: `\b`, `\f`, `\r`, `\n`, `\t`, `\0`, `\'`, `\\`. При парсинге, также поддерживаются последовательности `\a`, `\v`, а также `\xHH` (hex escape-последовательности) и любые последовательности вида `\c`, где `c` - любой символ - такие последовательности преобразуется в `c`. Таким образом, при чтении поддерживаются форматы, где перевод строки может быть записан как `\n` и как `\` и перевод строки. Например, строка `Hello world`, где между словами вместо пробела стоит перевод строки, может быть считана в любом из следующих вариантов:
+
+```text
+Hello\nworld
+
+Hello\
+world
+```
+
+Второй вариант поддерживается, так как его использует MySQL при записи tab-separated дампа.
+
+Минимальный набор символов, которых вам необходимо экранировать при передаче в TabSeparated формате: таб, перевод строки (LF) и обратный слеш.
+
+Экранируется лишь небольшой набор символов. Вы можете легко наткнуться на строковое значение, которое испортит ваш терминал при выводе в него.
+
+Массивы форматируются в виде списка значений через запятую в квадратных скобках. Элементы массива - числа форматируются как обычно, а даты, даты-с-временем и строки - в одинарных кавычках с такими же правилами экранирования, как указано выше.
+
+[NULL](../query_language/syntax.md#null-literal) форматируется в виде `\N`.
+
+<a name="tabseparatedraw"></a>
+
+## TabSeparatedRaw
+
+Отличается от формата `TabSeparated` тем, что строки выводятся без экранирования.
+Этот формат подходит только для вывода результата выполнения запроса, но не для парсинга (приёма данных для вставки в таблицу).
+
+Этот формат также доступен под именем `TSVRaw`.
+<a name="tabseparatedwithnames"></a>
+
+## TabSeparatedWithNames
+
+Отличается от формата `TabSeparated` тем, что в первой строке пишутся имена столбцов.
+При парсинге, первая строка полностью игнорируется: вы не можете использовать имена столбцов, чтобы указать их порядок расположения, или чтобы проверить их корректность.
+(Поддержка обработки заголовка при парсинге может быть добавлена в будущем.)
+
+Этот формат также доступен под именем `TSVWithNames`.
+<a name="tabseparatedwithnamesandtypes"></a>
+
+## TabSeparatedWithNamesAndTypes
+
+Отличается от формата `TabSeparated` тем, что в первой строке пишутся имена столбцов, а во второй - типы столбцов.
+При парсинге, первая и вторая строка полностью игнорируется.
+
+Этот формат также доступен под именем `TSVWithNamesAndTypes`.
+<a name="tskv"></a>
+
+## TSKV
+
+Похож на TabSeparated, но выводит значения в формате name=value. Имена экранируются так же, как строки в формате TabSeparated и, дополнительно, экранируется также символ =.
+
+```text
+SearchPhrase=   count()=8267016
+SearchPhrase=интерьер ванной комнаты    count()=2166
+SearchPhrase=яндекс     count()=1655
+SearchPhrase=весна 2014 мода    count()=1549
+SearchPhrase=фриформ фото       count()=1480
+SearchPhrase=анджелина джоли    count()=1245
+SearchPhrase=омск       count()=1112
+SearchPhrase=фото собак разных пород    count()=1091
+SearchPhrase=дизайн штор        count()=1064
+SearchPhrase=баку       count()=1000
+```
+
+[NULL](../query_language/syntax.md#null-literal) форматируется в виде `\N`.
+
+```sql
+SELECT * FROM t_null FORMAT TSKV
+```
+```
+x=1	y=\N
+```
+
+При большом количестве маленьких столбцов, этот формат существенно неэффективен, и обычно нет причин его использовать. Он реализован, так как используется в некоторых отделах Яндекса.
+
+Поддерживается как вывод, так и парсинг данных в этом формате. При парсинге, поддерживается расположение значений разных столбцов в произвольном порядке. Допустимо отсутствие некоторых значений - тогда они воспринимаются как равные значениям по умолчанию. При этом, в качестве значений по умолчанию используются нули, пустые строки и не поддерживаются сложные значения по умолчанию, которые могут быть заданы в таблице.
+
+При парсинге, в качестве дополнительного поля, может присутствовать `tskv` без знака равенства и без значения. Это поле игнорируется.
+
+
 ## Values
 
 Выводит каждую строку в скобках. Строки разделены запятыми. После последней строки запятой нет. Значения внутри скобок также разделены запятыми. Числа выводятся в десятичном виде без кавычек. Массивы выводятся в квадратных скобках. Строки, даты, даты-с-временем выводятся в кавычках. Правила экранирования и особенности парсинга аналогичны формату [TabSeparated](#tabseparated). При форматировании, лишние пробелы не ставятся, а при парсинге - допустимы и пропускаются (за исключением пробелов внутри значений типа массив, которые недопустимы). [NULL](../query_language/syntax.md#null-literal) представляется как `NULL`.
@@ -475,7 +504,7 @@ Array представлены как длина в формате varint (unsig
 
 Пример:
 
-``` sql
+```sql
 SELECT * FROM t_null FORMAT Vertical
 ```
 ```
@@ -590,32 +619,3 @@ test: string with \'quotes\' and \t with some special \n characters
 
 Массивы выводятся как `<array><elem>Hello</elem><elem>World</elem>...</array>`,
 а кортежи как `<tuple><elem>Hello</elem><elem>World</elem>...</tuple>`.
-
-<a name="format_capnproto"></a>
-
-## CapnProto
-
-Cap'n Proto - формат бинарных сообщений, похож на Protocol Buffers и Thrift, но не похож на JSON или MessagePack.
-
-Сообщения Cap'n Proto строго типизированы и не самоописывающиеся, т.е. нуждаются во внешнем описании схемы. Схема применяется "на лету" и кешируется для каждого запроса.
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits
-       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
-```
-
-Где `schema.capnp` выглядит следующим образом:
-
-```
-struct Message {
-  SearchPhrase @0 :Text;
-  c @1 :Uint64;
-}
-```
-
-
-Файлы со схемами находятся в файле, который находится в каталоге указанном в параметре [format_schema_path](../operations/server_settings/settings.md#server_settings-format_schema_path) конфигурации сервера.
-
-Десериализация эффективна и обычно не повышает нагрузку на систему.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/formats/) <!--hide-->
diff --git a/docs/ru/interfaces/http_interface.md b/docs/ru/interfaces/http_interface.md
index 6c8c0de1c..6aa2d786a 100644
--- a/docs/ru/interfaces/http_interface.md
+++ b/docs/ru/interfaces/http_interface.md
@@ -219,5 +219,3 @@ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&buffer_size=3000000&wa
 ```
 
 Буферизация позволяет избежать ситуации когда код ответа и HTTP-заголовки были отправлены клиенту, после чего возникла ошибка выполнения запроса. В такой ситуации сообщение об ошибке записывается в конце тела ответа, и на стороне клиента ошибка может быть обнаружена только на этапе парсинга.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/http_interface/) <!--hide-->
diff --git a/docs/ru/interfaces/index.md b/docs/ru/interfaces/index.md
index 4560cda1f..348cf975f 100644
--- a/docs/ru/interfaces/index.md
+++ b/docs/ru/interfaces/index.md
@@ -3,5 +3,3 @@
 # Интерфейсы
 
 Для изучения возможностей системы, загрузки данных в таблицы, ручных запросов, используйте программу clickhouse-client.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/) <!--hide-->
diff --git a/docs/ru/interfaces/jdbc.md b/docs/ru/interfaces/jdbc.md
index 8c54419d3..90051ad46 100644
--- a/docs/ru/interfaces/jdbc.md
+++ b/docs/ru/interfaces/jdbc.md
@@ -2,5 +2,3 @@
 
 - [Официальный драйвер](https://github.com/yandex/clickhouse-jdbc).
 - Драйвер от сторонней огранизации [ClickHouse-Native-JDBC](https://github.com/housepower/ClickHouse-Native-JDBC).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/jdbc/) <!--hide-->
diff --git a/docs/ru/interfaces/tcp.md b/docs/ru/interfaces/tcp.md
index 98672b505..e73a90ddf 100644
--- a/docs/ru/interfaces/tcp.md
+++ b/docs/ru/interfaces/tcp.md
@@ -1,5 +1,3 @@
 # Родной интерфейс (TCP)
 
 Родной интерфейс используется в клиенте командной строки clickhouse-client, при межсерверном взаимодействии для распределённой обработки запроса, а также в программах на C++. Будет рассмотрен только клиент командной строки.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/tcp/) <!--hide-->
diff --git a/docs/ru/interfaces/third-party_client_libraries.md b/docs/ru/interfaces/third-party_client_libraries.md
index 6cebd9827..c12522a19 100644
--- a/docs/ru/interfaces/third-party_client_libraries.md
+++ b/docs/ru/interfaces/third-party_client_libraries.md
@@ -7,7 +7,6 @@
     - [sqlalchemy-clickhouse](https://github.com/cloudflare/sqlalchemy-clickhouse)
     - [clickhouse-driver](https://github.com/mymarilyn/clickhouse-driver)
     - [clickhouse-client](https://github.com/yurial/clickhouse-client)
-    - [aiochclient](https://github.com/maximdanilchenko/aiochclient)
 - PHP
     - [phpClickHouse](https://github.com/smi2/phpClickHouse)
     - [clickhouse-php-client](https://github.com/8bitov/clickhouse-php-client)
@@ -43,9 +42,5 @@
     - [clickhouse_ecto](https://github.com/appodeal/clickhouse_ecto)
 - Java
     - [clickhouse-client-java](https://github.com/VirtusAI/clickhouse-client-java)
-- Kotlin
-    - [AORM](https://github.com/TanVD/AORM)
 - Nim
     - [nim-clickhouse](https://github.com/leonardoce/nim-clickhouse)
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/third-party_client_libraries/) <!--hide-->
diff --git a/docs/ru/interfaces/third-party_gui.md b/docs/ru/interfaces/third-party_gui.md
index 33b3e59ab..daa077a6e 100644
--- a/docs/ru/interfaces/third-party_gui.md
+++ b/docs/ru/interfaces/third-party_gui.md
@@ -6,10 +6,10 @@
 
 Основные возможности:
 
-- Работает с ClickHouse напрямую из браузера, без необходимости установки дополнительного ПО;
-- Редактор запросов с подсветкой синтаксиса;
-- Автодополнение команд;
-- Инструменты графического анализа выполнения запросов;
+- Работает с ClickHouse напрямую из браузера, без необходимости установки дополнительного ПО.
+- Редактор запросов с подсветкой синтаксиса.
+- Автодополнение команд.
+- Инструменты графического анализа выполнения запросов.
 - Цветовые схемы на выбор.
 
 [Документация Tabix](https://tabix.io/doc/).
@@ -20,33 +20,18 @@
 
 Основные возможности:
 
-- Построение запросов с подсветкой синтаксиса;
-- Просмотр ответа в табличном или JSON представлении;
-- Экспортирование результатов запроса в формате CSV или JSON;
-- Список процессов с описанием;
-- Режим записи;
-- Возможность остановки (`KILL`) запроса;
-- Граф базы данных. Показывает все таблицы и их столбцы с дополнительной информацией;
-- Быстрый просмотр размера столбца;
+- Построение запросов с подсветкой синтаксиса. Просмотр ответа в табличном или JSON представлении.
+- Экспортирование результатов запроса в формате CSV или JSON.
+- Список процессов с описанием. Режим записи. Возможность остановки (`KILL`) процесса.
+- Граф базы данных. Показывает все таблицы и их столбцы с дополнительной информацией.
+- Быстрый просмотр размера столбца.
 - Конфигурирование сервера.
 
 Планируется разработка следующих возможностей:
 
-- Управление базами;
-- Управление пользователями;
-- Анализ данных в режиме реального времени;
-- Мониторинг кластера;
-- Управление кластером;
+- Управление базами.
+- Управление пользователями.
+- Анализ данных  в режиме реального времени.
+- Мониторинг кластера.
+- Управление кластером.
 - Мониторинг реплицированных и Kafka таблиц.
-
-## DBeaver
-
-[DBeaver](https://dbeaver.io/) - универсальный desktop клиент баз данных с поддержкой ClickHouse.
-
-Основные возможности:
-
-- Построение запросов с подсветкой синтаксиса;
-- Просмотр таблиц;
-- Автодополнение команд.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/interfaces/third-party_gui/) <!--hide-->
diff --git a/docs/ru/introduction/distinctive_features.md b/docs/ru/introduction/distinctive_features.md
index db00c6af6..7851e5809 100644
--- a/docs/ru/introduction/distinctive_features.md
+++ b/docs/ru/introduction/distinctive_features.md
@@ -60,5 +60,3 @@ ClickHouse предоставляет различные способы разм
 Используется асинхронная multimaster репликация. После записи на любую доступную реплику, данные распространяются на все остальные реплики в фоне. Система поддерживает полную идентичность данных на разных репликах. Восстановление после большинства сбоев осуществляется автоматически, а в сложных случаях — полуавтоматически. При необходимости, можно [включить кворумную запись](../operations/settings/settings.md#setting-insert_quorum) данных.
 
 Подробнее смотрите раздел [Репликация данных](../operations/table_engines/replication.md#table_engines-replication).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/introduction/distinctive_features/) <!--hide-->
diff --git a/docs/ru/introduction/features_considered_disadvantages.md b/docs/ru/introduction/features_considered_disadvantages.md
index 9e04f747c..b7ac877cc 100644
--- a/docs/ru/introduction/features_considered_disadvantages.md
+++ b/docs/ru/introduction/features_considered_disadvantages.md
@@ -4,5 +4,3 @@
 2. Возможность изменять или удалять ранее записанные данные с низкими задержками и высокой частотой запросов не предоставляется. Есть массовое удаление данных для очистки более не нужного или соответствия [GDPR](https://gdpr-info.eu). Массовое изменение данных находится в разработке (на момент июля 2018).
 3. Разреженный индекс делает ClickHouse плохо пригодным для точечных чтений одиночных строк по своим
 ключам.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/introduction/features_considered_disadvantages/) <!--hide-->
diff --git a/docs/ru/introduction/performance.md b/docs/ru/introduction/performance.md
index f9ac88697..95e1d1cd0 100644
--- a/docs/ru/introduction/performance.md
+++ b/docs/ru/introduction/performance.md
@@ -21,5 +21,3 @@
 ## Производительность при вставке данных
 
 Данные рекомендуется вставлять пачками не менее 1000 строк или не более одного запроса в секунду. При вставке в таблицу типа MergeTree из tab-separated дампа, скорость вставки будет в районе 50-200 МБ/сек. Если вставляются строчки размером около 1 КБ, то скорость будет в районе 50 000 - 200 000 строчек в секунду. Если строчки маленькие - производительность в строчках в секунду будет выше (на данных БК - `>` 500 000 строк в секунду, на данных Graphite - `>` 1 000 000 строк в секунду). Для увеличения производительности, можно производить несколько запросов INSERT параллельно - при этом производительность растёт линейно.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/introduction/performance/) <!--hide-->
diff --git a/docs/ru/introduction/ya_metrika_task.md b/docs/ru/introduction/ya_metrika_task.md
index c7e22346a..b4030376b 100644
--- a/docs/ru/introduction/ya_metrika_task.md
+++ b/docs/ru/introduction/ya_metrika_task.md
@@ -45,5 +45,3 @@ ClickHouse имеет более десятка инсталляций в дру
 OLAPServer хорошо подходил для неагрегированных данных, но содержал много ограничений, не позволяющих использовать его для всех отчётов так, как хочется: отсутствие поддержки типов данных (только числа), невозможность инкрементального обновления данных в реальном времени (только перезаписью данных за сутки). OLAPServer не является СУБД, а является специализированной БД.
 
 Чтобы снять ограничения OLAPServer-а и решить задачу работы с неагрегированными данными для всех отчётов, разработана СУБД ClickHouse.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/introduction/ya_metrika_task/) <!--hide-->
diff --git a/docs/ru/operations/access_rights.md b/docs/ru/operations/access_rights.md
index 93e496b3d..7f09a917c 100644
--- a/docs/ru/operations/access_rights.md
+++ b/docs/ru/operations/access_rights.md
@@ -98,5 +98,3 @@
 Пользователь может получить список всех БД и таблиц в них с помощью запросов `SHOW` или системных таблиц, даже если у него нет доступа к отдельным БД.
 
 Доступ к БД не связан с настройкой [readonly](settings/query_complexity.md#query_complexity_readonly). Невозможно дать полный доступ к одной БД и `readonly` к другой.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/access_rights/) <!--hide-->
diff --git a/docs/ru/operations/configuration_files.md b/docs/ru/operations/configuration_files.md
index 563696a78..ab4f3d4eb 100644
--- a/docs/ru/operations/configuration_files.md
+++ b/docs/ru/operations/configuration_files.md
@@ -40,5 +40,3 @@ $ cat /etc/clickhouse-server/users.d/alice.xml
 Для каждого конфигурационного файла, сервер при запуске генерирует также файлы `file-preprocessed.xml`. Эти файлы содержат все выполненные подстановки и переопределения, и предназначены для информационных целей. Если в конфигурационных файлах были использованы ZooKeeper-подстановки, но при старте сервера ZooKeeper недоступен, то сервер загрузит конфигурацию из preprocessed-файла.
 
 Сервер следит за изменениями конфигурационных файлов, а также файлов и ZooKeeper-узлов, которые были использованы при выполнении подстановок и переопределений, и перезагружает настройки пользователей и кластеров на лету. То есть, можно изменять кластера, пользователей и их настройки без перезапуска сервера.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/configuration_files/) <!--hide-->
diff --git a/docs/ru/operations/index.md b/docs/ru/operations/index.md
index f16d6b3f8..75f20597e 100644
--- a/docs/ru/operations/index.md
+++ b/docs/ru/operations/index.md
@@ -1,3 +1 @@
 # Эксплуатация
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/) <!--hide-->
diff --git a/docs/ru/operations/quotas.md b/docs/ru/operations/quotas.md
index 93a795973..1a56ff9fe 100644
--- a/docs/ru/operations/quotas.md
+++ b/docs/ru/operations/quotas.md
@@ -103,5 +103,3 @@
 При распределённой обработке запроса, накопленные величины хранятся на сервере-инициаторе запроса. То есть, если пользователь пойдёт на другой сервер - там квота будет действовать "с нуля".
 
 При перезапуске сервера, квоты сбрасываются.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/quotas/) <!--hide-->
diff --git a/docs/ru/operations/server_settings/index.md b/docs/ru/operations/server_settings/index.md
index c208f9b41..644b1af79 100644
--- a/docs/ru/operations/server_settings/index.md
+++ b/docs/ru/operations/server_settings/index.md
@@ -9,5 +9,3 @@
 Прочие настройки описаны в разделе "[Настройки](../settings/index.md#settings)".
 
 Перед изучением настроек ознакомьтесь с разделом [Конфигурационные файлы](../configuration_files.md#configuration_files), обратите внимание на использование подстановок (атрибуты `incl` и `optional`).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/server_settings/) <!--hide-->
diff --git a/docs/ru/operations/server_settings/settings.md b/docs/ru/operations/server_settings/settings.md
index 567551cc3..5e71c8716 100644
--- a/docs/ru/operations/server_settings/settings.md
+++ b/docs/ru/operations/server_settings/settings.md
@@ -719,5 +719,3 @@ ClickHouse использует ZooKeeper для хранения метадан
 ```xml
 <zookeeper incl="zookeeper-servers" optional="true" />
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/server_settings/settings/) <!--hide-->
diff --git a/docs/ru/operations/settings/index.md b/docs/ru/operations/settings/index.md
index 1fa2f55bd..3de41c00b 100644
--- a/docs/ru/operations/settings/index.md
+++ b/docs/ru/operations/settings/index.md
@@ -22,5 +22,3 @@
     -   При использовании HTTP API передавайте cgi-параметры (`URL?setting_1=value&setting_2=value...`).
 
 Настройки, которые можно задать только в конфигурационном файле сервера, в разделе не рассматриваются.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/settings/) <!--hide-->
diff --git a/docs/ru/operations/settings/query_complexity.md b/docs/ru/operations/settings/query_complexity.md
index 3be6205e5..ccb206a58 100644
--- a/docs/ru/operations/settings/query_complexity.md
+++ b/docs/ru/operations/settings/query_complexity.md
@@ -196,5 +196,3 @@
 ## transfer_overflow_mode
 
 Что делать, когда количество данных превысило одно из ограничений: throw или break. По умолчанию: throw.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/settings/query_complexity/) <!--hide-->
diff --git a/docs/ru/operations/settings/settings.md b/docs/ru/operations/settings/settings.md
index 91271b2de..9648620be 100644
--- a/docs/ru/operations/settings/settings.md
+++ b/docs/ru/operations/settings/settings.md
@@ -413,5 +413,3 @@ ClickHouse применяет настройку в тех случаях, ко
 
 - [insert_quorum](#setting-insert_quorum)
 - [insert_quorum_timeout](#setting-insert_quorum_timeout)
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/settings/settings/) <!--hide-->
diff --git a/docs/ru/operations/settings/settings_profiles.md b/docs/ru/operations/settings/settings_profiles.md
index 600b7c117..8e30d7610 100644
--- a/docs/ru/operations/settings/settings_profiles.md
+++ b/docs/ru/operations/settings/settings_profiles.md
@@ -9,7 +9,7 @@
 
 Установить профиль `web`.
 
-``` sql
+```sql
 SET profile = 'web'
 ```
 
@@ -62,5 +62,3 @@ SET profile = 'web'
 В примере задано два профиля: `default` и `web`. Профиль `default` имеет специальное значение - он всегда обязан присутствовать и применяется при запуске сервера. То есть, профиль `default` содержит настройки по умолчанию. Профиль `web` - обычный профиль, который может быть установлен с помощью запроса `SET` или с помощью параметра URL при запросе по HTTP.
 
 Профили настроек могут наследоваться от друг-друга - это реализуется указанием настройки `profile` перед остальными настройками, перечисленными в профиле.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/settings/settings_profiles/) <!--hide-->
diff --git a/docs/ru/operations/system_tables.md b/docs/ru/operations/system_tables.md
index d92274b3b..ee03d00ac 100644
--- a/docs/ru/operations/system_tables.md
+++ b/docs/ru/operations/system_tables.md
@@ -19,7 +19,7 @@
 Содержит информацию о доступных в конфигурационном файле кластерах и серверах, которые в них входят.
 Столбцы:
 
-```
+```text
 cluster String      - имя кластера
 shard_num UInt32    - номер шарда в кластере, начиная с 1
 shard_weight UInt32 - относительный вес шарда при записи данных
@@ -34,7 +34,7 @@ user String         - имя пользователя, которого испо
 Содержит информацию о столбцах всех таблиц.
 С помощью этой таблицы можно получить информацию аналогично запросу `DESCRIBE TABLE`, но для многих таблиц сразу.
 
-```
+```text
 database String           - имя базы данных, в которой находится таблица
 table String              - имя таблицы
 name String               - имя столбца
@@ -159,7 +159,7 @@ default_expression String - выражение для значения по ум
 Эта системная таблица используется для реализации запроса `SHOW PROCESSLIST`.
 Столбцы:
 
-```
+```text
 user String              - имя пользователя, который задал запрос. При распределённой обработке запроса, относится к пользователю, с помощью которого сервер-инициатор запроса отправил запрос на данный сервер, а не к имени пользователя, который задал распределённый запрос на сервер-инициатор запроса.
 
 address String           - IP-адрес, с которого задан запрос. При распределённой обработке запроса, аналогично.
@@ -185,14 +185,14 @@ query_id String          - идентификатор запроса, если 
 
 Пример:
 
-``` sql
+```sql
 SELECT *
 FROM system.replicas
 WHERE table = 'visits'
 FORMAT Vertical
 ```
 
-```
+```text
 Row 1:
 ──────
 database:           merge
@@ -218,7 +218,7 @@ active_replicas:    2
 
 Столбцы:
 
-```
+```text
 database:           имя БД
 table:              имя таблицы
 engine:             имя движка таблицы
@@ -271,7 +271,7 @@ active_replicas:    число реплик этой таблицы, имеющ
 
 Например, так можно проверить, что всё хорошо:
 
-``` sql
+```sql
 SELECT
     database,
     table,
@@ -309,7 +309,7 @@ WHERE
 
 Столбцы:
 
-```
+```text
 name String   - имя настройки
 value String  - значение настройки
 changed UInt8 - была ли настройка явно задана в конфиге или изменена явным образом
@@ -317,13 +317,13 @@ changed UInt8 - была ли настройка явно задана в кон
 
 Пример:
 
-``` sql
+```sql
 SELECT *
 FROM system.settings
 WHERE changed
 ```
 
-```
+```text
 ┌─name───────────────────┬─value───────┬─changed─┐
 │ max_threads            │ 8           │       1 │
 │ use_uncompressed_cache │ 0           │       1 │
@@ -368,14 +368,14 @@ WHERE changed
 
 Пример:
 
-``` sql
+```sql
 SELECT *
 FROM system.zookeeper
 WHERE path = '/clickhouse/tables/01-08/visits/replicas'
 FORMAT Vertical
 ```
 
-```
+```text
 Row 1:
 ──────
 name:           example01-08-1.yandex.ru
@@ -410,5 +410,3 @@ numChildren:    7
 pzxid:          987021252247
 path:           /clickhouse/tables/01-08/visits/replicas
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/system_tables/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/aggregatingmergetree.md b/docs/ru/operations/table_engines/aggregatingmergetree.md
index 1cc880e16..cff911698 100644
--- a/docs/ru/operations/table_engines/aggregatingmergetree.md
+++ b/docs/ru/operations/table_engines/aggregatingmergetree.md
@@ -8,7 +8,7 @@
 
 Примеры:
 
-``` sql
+```sql
 CREATE TABLE t
 (
     column1 AggregateFunction(uniq, UInt64),
@@ -34,7 +34,7 @@ CREATE TABLE t
 То есть, агрегатная функция с суффиксом Merge берёт множество состояний, объединяет их, и возвращает готовый результат.
 Для примера, эти два запроса возвращают один и тот же результат:
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM table
 
 SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP BY RegionID)
@@ -52,7 +52,7 @@ SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP
 
 Создаём материализованное представление типа `AggregatingMergeTree`, следящее за таблицей `test.visits`:
 
-``` sql
+```sql
 CREATE MATERIALIZED VIEW test.basic
 ENGINE = AggregatingMergeTree(StartDate, (CounterID, StartDate), 8192)
 AS SELECT
@@ -66,13 +66,13 @@ GROUP BY CounterID, StartDate;
 
 Вставляем данные в таблицу `test.visits`. Данные будут также вставлены в представление, где они будут агрегированы:
 
-``` sql
+```sql
 INSERT INTO test.visits ...
 ```
 
 Делаем `SELECT` из представления, используя `GROUP BY`, чтобы доагрегировать данные:
 
-``` sql
+```sql
 SELECT
     StartDate,
     sumMerge(Visits) AS Visits,
@@ -85,5 +85,3 @@ ORDER BY StartDate;
 Вы можете создать такое материализованное представление и навесить на него обычное представление, выполняющее доагрегацию данных.
 
 Заметим, что в большинстве случаев, использование `AggregatingMergeTree` является неоправданным, так как можно достаточно эффективно выполнять запросы по неагрегированным данным.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/aggregatingmergetree/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/buffer.md b/docs/ru/operations/table_engines/buffer.md
index 24e456da4..7f947ea91 100644
--- a/docs/ru/operations/table_engines/buffer.md
+++ b/docs/ru/operations/table_engines/buffer.md
@@ -2,7 +2,7 @@
 
 Буферизует записываемые данные в оперативке, периодически сбрасывая их в другую таблицу. При чтении, производится чтение данных одновременно из буфера и из другой таблицы.
 
-```
+```text
 Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes)
 ```
 
@@ -22,7 +22,7 @@ min_bytes, max_bytes - условие на количество байт в бу
 
 Пример:
 
-``` sql
+```sql
 CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000)
 ```
 
@@ -57,5 +57,3 @@ CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10
 Таблицы типа Buffer используются в тех случаях, когда от большого количества серверов поступает слишком много INSERT-ов в единицу времени, и нет возможности заранее самостоятельно буферизовать данные перед вставкой, в результате чего, INSERT-ы не успевают выполняться.
 
 Заметим, что даже для таблиц типа Buffer не имеет смысла вставлять данные по одной строке, так как таким образом будет достигнута скорость всего лишь в несколько тысяч строк в секунду, тогда как при вставке более крупными блоками, достижимо более миллиона строк в секунду (смотрите раздел "Производительность").
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/buffer/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/collapsingmergetree.md b/docs/ru/operations/table_engines/collapsingmergetree.md
index d283a8dbe..83dfaf99d 100644
--- a/docs/ru/operations/table_engines/collapsingmergetree.md
+++ b/docs/ru/operations/table_engines/collapsingmergetree.md
@@ -14,7 +14,7 @@
 
 CollapsingMergeTree принимает дополнительный параметр - имя столбца типа Int8, содержащего "знак" строки. Пример:
 
-``` sql
+```sql
 CollapsingMergeTree(EventDate, (CounterID, EventDate, intHash32(UniqID), VisitID), 8192, Sign)
 ```
 
@@ -35,5 +35,3 @@ CollapsingMergeTree(EventDate, (CounterID, EventDate, intHash32(UniqID), VisitID
 
 1.  Написать запрос с GROUP BY и агрегатными функциями, учитывающими знак. Например, чтобы посчитать количество, надо вместо count() написать sum(Sign); чтобы посчитать сумму чего-либо, надо вместо sum(x) написать sum(Sign \* x) и т. п., а также добавить HAVING sum(Sign) `>` 0. Не все величины можно посчитать подобным образом. Например, агрегатные функции min, max не могут быть переписаны.
 2.  Если необходимо вынимать данные без агрегации (например, проверить наличие строк, самые новые значения которых удовлетворяют некоторым условиям), можно использовать модификатор FINAL для секции FROM. Это вариант существенно менее эффективен.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/collapsingmergetree/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/custom_partitioning_key.md b/docs/ru/operations/table_engines/custom_partitioning_key.md
index 8d228f5c4..dc7abf818 100644
--- a/docs/ru/operations/table_engines/custom_partitioning_key.md
+++ b/docs/ru/operations/table_engines/custom_partitioning_key.md
@@ -11,7 +11,7 @@ ENGINE [=] Name(...) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTI
 ```
 
 Для MergeTree таблиц выражение партиционирования указывается после `PARTITION BY`, первичный ключ после `ORDER BY`, ключ сэмплирования после `SAMPLE BY`, а в `SETTINGS` можно указать `index_granularity` (не обязательно, значение по умолчанию 8192), а также другие настройки из [MergeTreeSettings.h](https://github.com/yandex/ClickHouse/blob/master/dbms/src/Storages/MergeTree/MergeTreeSettings.h). Остальные параметры движка по-прежнему указываются в скобках после его названия. Пример:
-``` sql
+```sql
 ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)
     PARTITION BY (toMonday(StartDate), EventType)
     ORDER BY (CounterID, StartDate, intHash32(UserID))
@@ -25,7 +25,7 @@ ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Si
 После создания такой таблицы слияние кусков будет работать только для кусков с одинаковым значением выражения партиционирования. Замечание: это означает, что нежелательно делать слишком гранулированное партиционирование (более порядка тысячи партиций), иначе производительность SELECT будет неудовлетворительной.
 
 Чтобы указать партицию в командах ALTER PARTITION, нужно указать значение выражения партиционирования (или кортежа). Поддерживаются константы и константные выражения. Пример:
-``` sql
+```sql
 ALTER TABLE table DROP PARTITION (toMonday(today()), 1)
 ```
 удалит партицию за текущую неделю с типом события 1. То же самое для запроса OPTIMIZE. Чтобы указать единственную партицию непартиционированной таблицы, укажите `PARTITION tuple()`.
@@ -41,5 +41,3 @@ ALTER TABLE table DROP PARTITION (toMonday(today()), 1)
 ID партиции - это её строковый идентификатор (по возможности человекочитаемый), используемый для имён кусков на файловой системе и в ZooKeeper. Его можно указывать в запросах ALTER вместо значения ключа партиционирования. Пример: ключ партиционирования `toYYYYMM(EventDate)`, в ALTER можно указывать либо `PARTITION 201710`, либо `PARTITION ID '201710'`.
 
 Больше примеров в тестах [`00502_custom_partitioning_local`](https://github.com/yandex/ClickHouse/blob/master/dbms/tests/queries/0_stateless/00502_custom_partitioning_local.sql) и [`00502_custom_partitioning_replicated_zookeeper`](https://github.com/yandex/ClickHouse/blob/master/dbms/tests/queries/0_stateless/00502_custom_partitioning_replicated_zookeeper.sql).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/custom_partitioning_key/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/dictionary.md b/docs/ru/operations/table_engines/dictionary.md
index bc5e8e409..7d6aedf39 100644
--- a/docs/ru/operations/table_engines/dictionary.md
+++ b/docs/ru/operations/table_engines/dictionary.md
@@ -2,7 +2,7 @@
 
 # Dictionary
 
-Движок `Dictionary` отображает данные [словаря](../../query_language/dicts/external_dicts.md#dicts-external_dicts) как таблицу ClickHouse.
+Движок `Dictionary` отображает данные [словаря](../../query_language/dicts/external_dicts.md) как таблицу ClickHouse.
 
 Рассмотрим для примера словарь `products` со следующей конфигурацией:
 
@@ -39,7 +39,7 @@
 
 Запрос данных словаря:
 
-``` sql
+```sql
 select name, type, key, attribute.names, attribute.types, bytes_allocated, element_count,source from system.dictionaries where name = 'products';                     
 
 SELECT
@@ -73,7 +73,7 @@ CREATE TABLE %table_name% (%fields%) engine = Dictionary(%dictionary_name%)`
 
 Пример использования:
 
-``` sql
+```sql
 create table products (product_id UInt64, title String) Engine = Dictionary(products);
 
 CREATE TABLE products
@@ -91,7 +91,7 @@ Ok.
 
 Проверим что у нас в таблице?
 
-``` sql
+```sql
 select * from products limit 1;
 
 SELECT *
@@ -106,5 +106,3 @@ LIMIT 1
 
 1 rows in set. Elapsed: 0.006 sec.
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/dictionary/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/distributed.md b/docs/ru/operations/table_engines/distributed.md
index a483e6de8..a79767769 100644
--- a/docs/ru/operations/table_engines/distributed.md
+++ b/docs/ru/operations/table_engines/distributed.md
@@ -7,7 +7,7 @@
 Движок Distributed принимает параметры: имя кластера в конфигурационном файле сервера, имя удалённой базы данных, имя удалённой таблицы, а также (не обязательно) ключ шардирования.
 Пример:
 
-```
+```text
 Distributed(logs, default, hits[, sharding_key])
 ```
 
@@ -120,5 +120,3 @@ logs - имя кластера в конфигурационном файле с
 Если после INSERT-а в Distributed таблицу, сервер перестал существовать или был грубо перезапущен (например, в следствие аппаратного сбоя), то записанные данные могут быть потеряны. Если в директории таблицы обнаружен повреждённый кусок данных, то он переносится в поддиректорию broken и больше не используется.
 
 При выставлении опции max_parallel_replicas выполнение запроса распараллеливается по всем репликам внутри одного шарда. Подробнее смотрите раздел "Настройки, max_parallel_replicas".
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/distributed/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/external_data.md b/docs/ru/operations/table_engines/external_data.md
index 430f90a82..fc61c7767 100644
--- a/docs/ru/operations/table_engines/external_data.md
+++ b/docs/ru/operations/table_engines/external_data.md
@@ -59,5 +59,3 @@ curl -F 'passwd=@passwd.tsv;' 'http://localhost:8123/?query=SELECT+shell,+count(
 ```
 
 При распределённой обработке запроса, временные таблицы передаются на все удалённые серверы.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/external_data/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/file.md b/docs/ru/operations/table_engines/file.md
index 2cf9f3ff7..f65736768 100644
--- a/docs/ru/operations/table_engines/file.md
+++ b/docs/ru/operations/table_engines/file.md
@@ -31,7 +31,7 @@ File(Format)
 
 **1.** Создадим на сервере таблицу `file_engine_table`:
 
-``` sql
+```sql
 CREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(TabSeparated)
 ```
 
@@ -47,11 +47,11 @@ two	2
 
 **3.** Запросим данные:
 
-``` sql
+```sql
 SELECT * FROM file_engine_table
 ```
 
-```
+```text
 ┌─name─┬─value─┐
 │ one  │     1 │
 │ two  │     2 │
@@ -75,5 +75,3 @@ $ echo -e "1,2\n3,4" | clickhouse-local -q "CREATE TABLE table (a Int64, b Int64
     - использование операций `ALTER` и `SELECT...SAMPLE`;
     - индексы;
     - репликация.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/file/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/graphitemergetree.md b/docs/ru/operations/table_engines/graphitemergetree.md
index fa7898577..3617fe408 100644
--- a/docs/ru/operations/table_engines/graphitemergetree.md
+++ b/docs/ru/operations/table_engines/graphitemergetree.md
@@ -27,7 +27,7 @@ Graphite хранит в ClickHouse полные данные, а получат
 
 Шаблон правил rollup:
 
-```
+```text
 pattern
     regexp
     function
@@ -83,5 +83,3 @@ default
     </default>
 </graphite_rollup>
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/graphitemergetree/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/index.md b/docs/ru/operations/table_engines/index.md
index a5e42c21e..90b14f700 100644
--- a/docs/ru/operations/table_engines/index.md
+++ b/docs/ru/operations/table_engines/index.md
@@ -14,5 +14,3 @@
 При чтении, движок обязан лишь выдать запрошенные столбцы, но в некоторых случаях движок может частично обрабатывать данные при ответе на запрос.
 
 Для большинства серьёзных задач, следует использовать движки семейства `MergeTree`.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/join.md b/docs/ru/operations/table_engines/join.md
index d9ce49665..2acabefc4 100644
--- a/docs/ru/operations/table_engines/join.md
+++ b/docs/ru/operations/table_engines/join.md
@@ -2,7 +2,7 @@
 
 Представляет собой подготовленную структуру данных для JOIN-а, постоянно находящуюся в оперативке.
 
-```
+```text
 Join(ANY|ALL, LEFT|INNER, k1[, k2, ...])
 ```
 
@@ -14,5 +14,3 @@ Join(ANY|ALL, LEFT|INNER, k1[, k2, ...])
 В таблицу можно вставлять данные INSERT-ом, аналогично движку Set. В случае ANY, данные для дублирующихся ключей будут проигнорированы; в случае ALL - будут учитываться. Из таблицы нельзя, непосредственно, делать SELECT. Единственная возможность чтения - использование в качестве "правой" таблицы для JOIN.
 
 Хранение данных на диске аналогично движку Set.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/join/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/kafka.md b/docs/ru/operations/table_engines/kafka.md
index 8dc7b55e3..282140d43 100644
--- a/docs/ru/operations/table_engines/kafka.md
+++ b/docs/ru/operations/table_engines/kafka.md
@@ -43,7 +43,7 @@ Kafka SETTINGS
 
 Примеры:
 
-``` sql
+```sql
   CREATE TABLE queue (
     timestamp UInt64,
     level String,
@@ -85,7 +85,7 @@ Kafka SETTINGS
 
 Пример:
 
-``` sql
+```sql
   CREATE TABLE queue (
     timestamp UInt64,
     level String,
@@ -136,5 +136,3 @@ Kafka SETTINGS
 ```
 
 В документе [librdkafka configuration reference](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) можно увидеть список возможных опций конфигурации. Используйте подчёркивания (`_`) вместо точек в конфигурации ClickHouse, например, `check.crcs=true` будет соответствовать `<check_crcs>true</check_crcs>`.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/kafka/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/log.md b/docs/ru/operations/table_engines/log.md
index 58aab57f5..cfb050f16 100644
--- a/docs/ru/operations/table_engines/log.md
+++ b/docs/ru/operations/table_engines/log.md
@@ -3,5 +3,3 @@
 Отличается от TinyLog тем, что вместе с файлами столбцов лежит небольшой файл "засечек". Засечки пишутся на каждый блок данных и содержат смещение - с какого места нужно читать файл, чтобы пропустить заданное количество строк. Это позволяет читать данные из таблицы в несколько потоков.
 При конкуррентном доступе к данным, чтения могут выполняться одновременно, а записи блокируют чтения и друг друга.
 Движок Log не поддерживает индексы. Также, если при записи в таблицу произошёл сбой, то таблица станет битой, и чтения из неё будут возвращать ошибку. Движок Log подходит для временных данных, write-once таблиц, а также для тестовых и демонстрационных целей.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/log/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/materializedview.md b/docs/ru/operations/table_engines/materializedview.md
index 843f0678a..8c44eb40c 100644
--- a/docs/ru/operations/table_engines/materializedview.md
+++ b/docs/ru/operations/table_engines/materializedview.md
@@ -1,5 +1,3 @@
 # MaterializedView
 
 Используется для реализации материализованных представлений (подробнее см. запрос [CREATE TABLE](../../query_language/create.md#query_language-queries-create_table)). Для хранения данных, использует другой движок, который был указан при создании представления. При чтении из таблицы, просто использует этот движок.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/materializedview/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/memory.md b/docs/ru/operations/table_engines/memory.md
index 70cf0c8db..7552a2709 100644
--- a/docs/ru/operations/table_engines/memory.md
+++ b/docs/ru/operations/table_engines/memory.md
@@ -8,5 +8,3 @@
 Обычно, использование этого движка таблиц является неоправданным. Тем не менее, он может использоваться для тестов, а также в задачах, где важно достичь максимальной скорости на не очень большом количестве строк (примерно до 100 000 000).
 
 Движок Memory используется системой для временных таблиц - внешних данных запроса (смотрите раздел "Внешние данные для обработки запроса"), для реализации `GLOBAL IN` (смотрите раздел "Операторы IN").
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/memory/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/merge.md b/docs/ru/operations/table_engines/merge.md
index 3b2174d52..7aa5ebd33 100644
--- a/docs/ru/operations/table_engines/merge.md
+++ b/docs/ru/operations/table_engines/merge.md
@@ -39,5 +39,3 @@ Merge(hits, '^WatchLog')
 Таблица типа `Merge` содержит виртуальный столбец `_table` типа `String`. (Если в таблице уже есть столбец `_table`, то виртуальный столбец называется `_table1`; если уже есть `_table1`, то `_table2` и т. п.) Он содержит имя таблицы, из которой были прочитаны данные.
 
 Если секция `WHERE/PREWHERE` содержит (в качестве одного из элементов конъюнкции или в качестве всего выражения) условия на столбец `_table`, не зависящие от других столбцов таблицы, то эти условия используются как индекс: условия выполняются над множеством имён таблиц, из которых нужно читать данные, и чтение будет производиться только из тех таблиц, для которых условия сработали.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/merge/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/mergetree.md b/docs/ru/operations/table_engines/mergetree.md
index a0a528561..f41e165f7 100644
--- a/docs/ru/operations/table_engines/mergetree.md
+++ b/docs/ru/operations/table_engines/mergetree.md
@@ -159,7 +159,7 @@ ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDa
 
 В этом случае в запросах:
 
-``` sql
+```sql
 SELECT count() FROM table WHERE EventDate = toDate(now()) AND CounterID = 34
 SELECT count() FROM table WHERE EventDate = toDate(now()) AND (CounterID = 34 OR CounterID = 42)
 SELECT count() FROM table WHERE ((EventDate >= toDate('2014-01-01') AND EventDate <= toDate('2014-01-31')) OR EventDate = toDate('2014-05-01')) AND CounterID IN (101500, 731962, 160656) AND (CounterID = 101500 OR EventDate != toDate('2014-05-01'))
@@ -171,7 +171,7 @@ ClickHouse будет использовать индекс по первичн
 
 В примере ниже индекс не может использоваться.
 
-``` sql
+```sql
 SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'
 ```
 
@@ -185,5 +185,3 @@ SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'
 Для конкурентного доступа к таблице используется мультиверсионность. То есть, при одновременном чтении и обновлении таблицы, данные будут читаться из набора кусочков, актуального на момент запроса. Длинных блокировок нет. Вставки никак не мешают чтениям.
 
 Чтения из таблицы автоматически распараллеливаются.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/mergetree/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/mysql.md b/docs/ru/operations/table_engines/mysql.md
index d4444dfe2..e08edc4e1 100644
--- a/docs/ru/operations/table_engines/mysql.md
+++ b/docs/ru/operations/table_engines/mysql.md
@@ -25,5 +25,3 @@ MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_
 Остальные условия и ограничение выборки `LIMIT` будут выполнены в ClickHouse только после выполнения запроса к MySQL.
 
 Движок `MySQL` не поддерживает тип данных [Nullable](../../data_types/nullable.md#data_type-nullable), поэтому при чтении данных из таблиц MySQL `NULL` преобразуются в значения по умолчанию для заданного типа столбца, обычно это 0 или пустая строка.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/mysql/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/null.md b/docs/ru/operations/table_engines/null.md
index a67b9bb16..252517b1a 100644
--- a/docs/ru/operations/table_engines/null.md
+++ b/docs/ru/operations/table_engines/null.md
@@ -3,5 +3,3 @@
 При записи в таблицу типа Null, данные игнорируются. При чтении из таблицы типа Null, возвращается пустота.
 
 Тем не менее, есть возможность создать материализованное представление над таблицей типа Null. Тогда данные, записываемые в таблицу, будут попадать в представление.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/null/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/replacingmergetree.md b/docs/ru/operations/table_engines/replacingmergetree.md
index 3c5658bae..8e09810ca 100644
--- a/docs/ru/operations/table_engines/replacingmergetree.md
+++ b/docs/ru/operations/table_engines/replacingmergetree.md
@@ -6,7 +6,7 @@
 
 Столбец с версией должен иметь тип из семейства `UInt`, или `Date`, или `DateTime`.
 
-``` sql
+```sql
 ReplacingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, ver)
 ```
 
@@ -15,5 +15,3 @@ ReplacingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, ver)
 Таким образом, `ReplacingMergeTree` подходит для фоновой чистки дублирующихся данных в целях экономии места, но не даёт гарантий отсутствия дубликатов.
 
 *Движок не используется в Яндекс.Метрике, но нашёл своё применение в других отделах Яндекса.*
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/replacingmergetree/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/replication.md b/docs/ru/operations/table_engines/replication.md
index ec40645a8..1dec0f4f4 100644
--- a/docs/ru/operations/table_engines/replication.md
+++ b/docs/ru/operations/table_engines/replication.md
@@ -78,7 +78,7 @@
 
 Пример:
 
-```
+```text
 ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/hits', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192)
 ```
 
@@ -180,5 +180,3 @@ sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
 ## Восстановление в случае потери или повреждения метаданных на ZooKeeper кластере
 
 Если данные в ZooKeeper оказались утеряны или повреждены, то вы можете сохранить данные, переместив их в нереплицируемую таблицу, как описано в пункте выше.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/replication/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/set.md b/docs/ru/operations/table_engines/set.md
index b4371e1a9..0697b32b4 100644
--- a/docs/ru/operations/table_engines/set.md
+++ b/docs/ru/operations/table_engines/set.md
@@ -8,5 +8,3 @@
 Данные постоянно находятся в оперативке. При INSERT-е, в директорию таблицы на диске, также пишутся блоки вставленных данных. При запуске сервера, эти данные считываются в оперативку. То есть, после перезапуска, данные остаются на месте.
 
 При грубом перезапуске сервера, блок данных на диске может быть потерян или повреждён. В последнем случае, может потребоваться вручную удалить файл с повреждёнными данными.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/set/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/summingmergetree.md b/docs/ru/operations/table_engines/summingmergetree.md
index 6b9d14973..6aa2f116d 100644
--- a/docs/ru/operations/table_engines/summingmergetree.md
+++ b/docs/ru/operations/table_engines/summingmergetree.md
@@ -4,13 +4,13 @@
 
 Отличается от `MergeTree` тем, что суммирует данные при слиянии.
 
-``` sql
+```sql
 SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192)
 ```
 
 Столбцы для суммирования заданы неявно. При слиянии, для всех строчек с одинаковым значением первичного ключа (в примере - OrderID, EventDate, BannerID, ...), производится суммирование значений в числовых столбцах, не входящих в первичный ключ.
 
-``` sql
+```sql
 SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, (Shows, Clicks, Cost, ...))
 ```
 
@@ -32,7 +32,7 @@ SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, (Shows, C
 
 Примеры:
 
-```
+```text
 [(1, 100)] + [(2, 150)] -> [(1, 100), (2, 150)]
 [(1, 100)] + [(1, 150)] -> [(1, 250)]
 [(1, 100)] + [(1, 150), (2, 150)] -> [(1, 250), (2, 150)]
@@ -44,5 +44,3 @@ SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, (Shows, C
 Для вложенных структур данных не нужно указывать её столбцы в качестве списка столбцов для суммирования.
 
 Этот движок таблиц разработан по просьбе БК, и является мало полезным. Помните, что при хранении лишь предагрегированных данных, вы теряете часть преимуществ системы.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/summingmergetree/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/tinylog.md b/docs/ru/operations/table_engines/tinylog.md
index 3e0966850..813eaa568 100644
--- a/docs/ru/operations/table_engines/tinylog.md
+++ b/docs/ru/operations/table_engines/tinylog.md
@@ -16,5 +16,3 @@
 **Индексы не поддерживаются.**
 
 В Яндекс.Метрике таблицы типа TinyLog используются для промежуточных данных, обрабатываемых маленькими пачками.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/tinylog/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/url.md b/docs/ru/operations/table_engines/url.md
index a4a0d511e..b3daae061 100644
--- a/docs/ru/operations/table_engines/url.md
+++ b/docs/ru/operations/table_engines/url.md
@@ -23,7 +23,7 @@
 
 **1.** Создадим на сервере таблицу `url_engine_table`:
 
-``` sql
+```sql
 CREATE TABLE url_engine_table (word String, value UInt64)
 ENGINE=URL('http://127.0.0.1:12345/', CSV)
 ```
@@ -53,11 +53,11 @@ python3 server.py
 
 **3.** Запросим данные:
 
-``` sql
+```sql
 SELECT * FROM url_engine_table
 ```
 
-```
+```text
 ┌─word──┬─value─┐
 │ Hello │     1 │
 │ World │     2 │
@@ -72,5 +72,3 @@ SELECT * FROM url_engine_table
     - индексы;
     - репликация.
 
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/url/) <!--hide-->
diff --git a/docs/ru/operations/table_engines/view.md b/docs/ru/operations/table_engines/view.md
index 874569e3a..128986eb7 100644
--- a/docs/ru/operations/table_engines/view.md
+++ b/docs/ru/operations/table_engines/view.md
@@ -1,5 +1,3 @@
 # View
 
 Используется для реализации представлений (подробнее см. запрос `CREATE VIEW`). Не хранит данные, а хранит только указанный запрос `SELECT`. При чтении из таблицы, выполняет его (с удалением из запроса всех ненужных столбцов).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/table_engines/view/) <!--hide-->
diff --git a/docs/ru/operations/tips.md b/docs/ru/operations/tips.md
index af5ab00c3..9927d78dc 100644
--- a/docs/ru/operations/tips.md
+++ b/docs/ru/operations/tips.md
@@ -178,7 +178,7 @@ dynamicConfigFile=/etc/zookeeper-{{ cluster['name'] }}/conf/zoo.cfg.dynamic
 
 Версия Java:
 
-```
+```text
 Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
 Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
 ```
@@ -226,7 +226,7 @@ JAVA_OPTS="-Xms{{ cluster.get('xms','128M') }} \
 
 Salt init:
 
-```
+```text
 description "zookeeper-{{ cluster['name'] }} centralized coordination service"
 
 start on runlevel [2345]
@@ -254,5 +254,3 @@ script
         -Dzookeeper.root.logger=${ZOO_LOG4J_PROP} $ZOOMAIN $ZOOCFG
 end script
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/tips/) <!--hide-->
diff --git a/docs/ru/operations/utils/clickhouse-copier.md b/docs/ru/operations/utils/clickhouse-copier.md
index d114c826d..849fa532d 100644
--- a/docs/ru/operations/utils/clickhouse-copier.md
+++ b/docs/ru/operations/utils/clickhouse-copier.md
@@ -157,5 +157,3 @@ clickhouse-copier copier --daemon --config zookeeper.xml --task-path /task/path
 ```
 
 `clickhouse-copier` отслеживает изменения `/task/path/description` и применяет их "на лету". Если вы поменяете, например, значение `max_workers`, то количество процессов, выполняющих задания, также изменится.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/utils/clickhouse-copier/) <!--hide-->
diff --git a/docs/ru/operations/utils/clickhouse-local.md b/docs/ru/operations/utils/clickhouse-local.md
index 4edbb3abb..ddaa64e0a 100644
--- a/docs/ru/operations/utils/clickhouse-local.md
+++ b/docs/ru/operations/utils/clickhouse-local.md
@@ -66,5 +66,3 @@ Read 186 rows, 4.15 KiB in 0.035 sec., 5302 rows/sec., 118.34 KiB/sec.
 ├──────────┼──────────┤
 ...
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/utils/clickhouse-local/) <!--hide-->
diff --git a/docs/ru/operations/utils/index.md b/docs/ru/operations/utils/index.md
index aa07954d8..75bd574a7 100644
--- a/docs/ru/operations/utils/index.md
+++ b/docs/ru/operations/utils/index.md
@@ -2,5 +2,3 @@
 
 * [clickhouse-local](clickhouse-local.md#utils-clickhouse-local) - позволяет выполнять SQL-запросы над данными без установки сервера ClickHouse подобно тому, как это делает `awk`.
 * [clickhouse-copier](clickhouse-copier.md#utils-clickhouse-copier) - копирует (и перешардирует) данные с одного кластера на другой.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/operations/utils/) <!--hide-->
diff --git a/docs/ru/query_language/agg_functions/combinators.md b/docs/ru/query_language/agg_functions/combinators.md
index 0e4c1dd87..ba08f46e4 100644
--- a/docs/ru/query_language/agg_functions/combinators.md
+++ b/docs/ru/query_language/agg_functions/combinators.md
@@ -38,5 +38,3 @@
 ## -ForEach
 
 Преобразует агрегатную функцию для таблиц в агрегатную функцию для массивов, которая применяет агрегирование для соответствующих элементов массивов и возвращает массив результатов. Например, `sumForEach` для массивов `[1, 2]`, `[3, 4, 5]` и `[6, 7]` даст результат `[10, 13, 5]`, сложив соответственные элементы массивов.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/agg_functions/combinators/) <!--hide-->
diff --git a/docs/ru/query_language/agg_functions/index.md b/docs/ru/query_language/agg_functions/index.md
index e89934fde..261c3c370 100644
--- a/docs/ru/query_language/agg_functions/index.md
+++ b/docs/ru/query_language/agg_functions/index.md
@@ -61,5 +61,3 @@ FROM t_null_big
 ```
 
 `groupArray` не включает `NULL` в результирующий массив.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/agg_functions/) <!--hide-->
diff --git a/docs/ru/query_language/agg_functions/parametric_functions.md b/docs/ru/query_language/agg_functions/parametric_functions.md
index 7f7432fb6..b86b75baf 100644
--- a/docs/ru/query_language/agg_functions/parametric_functions.md
+++ b/docs/ru/query_language/agg_functions/parametric_functions.md
@@ -23,7 +23,7 @@
 
 Это вырожденный пример. Его можно записать с помощью других агрегатных функций:
 
-```
+```text
 minIf(EventTime, URL LIKE '%company%') < maxIf(EventTime, URL LIKE '%cart%').
 ```
 
@@ -123,9 +123,7 @@ ORDER BY level
 
 Пример применения:
 
-```
+```text
 Задача: показывать в отчёте только поисковые фразы, по которым было хотя бы 5 уникальных посетителей.
 Решение: пишем в запросе GROUP BY SearchPhrase HAVING uniqUpTo(4)(UserID) >= 5
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/agg_functions/parametric_functions/) <!--hide-->
diff --git a/docs/ru/query_language/agg_functions/reference.md b/docs/ru/query_language/agg_functions/reference.md
index 1c700851e..1ad00d767 100644
--- a/docs/ru/query_language/agg_functions/reference.md
+++ b/docs/ru/query_language/agg_functions/reference.md
@@ -37,7 +37,7 @@ anyHeavy(column)
 
 Возьмем набор данных [OnTime](../../getting_started/example_datasets/ontime.md#example_datasets-ontime) и выберем произвольное часто встречающееся значение в столбце `AirlineID`.
 
-``` sql
+```sql
 SELECT anyHeavy(AirlineID) AS res
 FROM ontime
 ```
@@ -109,7 +109,7 @@ SELECT argMin(user, salary) FROM salary
 
 Пример:
 
-``` sql
+```sql
 CREATE TABLE sum_map(
     date Date,
     timeslot DateTime,
@@ -130,7 +130,7 @@ FROM sum_map
 GROUP BY timeslot
 ```
 
-```
+```text
 ┌────────────timeslot─┬─sumMap(statusMap.status, statusMap.requests)─┐
 │ 2000-01-01 00:00:00 │ ([1,2,3,4,5],[10,10,20,10,10])               │
 │ 2000-01-01 00:01:00 │ ([4,5,6,7,8],[10,10,20,10,10])               │
@@ -352,7 +352,7 @@ topK(N)(column)
 
 Возьмем набор данных [OnTime](../../getting_started/example_datasets/ontime.md#example_datasets-ontime) и выберем 3 наиболее часто встречающихся значения в столбце `AirlineID`.
 
-``` sql
+```sql
 SELECT topK(3)(AirlineID) AS res
 FROM ontime
 ```
@@ -377,5 +377,3 @@ FROM ontime
 ## corr(x, y)
 
 Вычисляет коэффициент корреляции Пирсона: `Σ((x - x̅)(y - y̅)) / sqrt(Σ((x - x̅)^2) * Σ((y - y̅)^2))`.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/agg_functions/reference/) <!--hide-->
diff --git a/docs/ru/query_language/alter.md b/docs/ru/query_language/alter.md
index 268e7baa9..4a39af65b 100644
--- a/docs/ru/query_language/alter.md
+++ b/docs/ru/query_language/alter.md
@@ -7,7 +7,7 @@
 
 Изменение структуры таблицы.
 
-``` sql
+```sql
 ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...
 ```
 
@@ -16,7 +16,7 @@ ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...
 
 Существуют следующие действия:
 
-``` sql
+```sql
 ADD COLUMN name [type] [default_expr] [AFTER name_after]
 ```
 
@@ -26,14 +26,14 @@ ADD COLUMN name [type] [default_expr] [AFTER name_after]
 
 Такая схема позволяет добиться мгновенной работы запроса ALTER и отсутствия необходимости увеличивать объём старых данных.
 
-``` sql
+```sql
 DROP COLUMN name
 ```
 
 Удаляет столбец с именем name.
 Удаляет данные из файловой системы. Так как это представляет собой удаление целых файлов, запрос выполняется почти мгновенно.
 
-``` sql
+```sql
 MODIFY COLUMN name [type] [default_expr]
 ```
 
@@ -84,7 +84,7 @@ MODIFY COLUMN name [type] [default_expr]
 
 Чтобы посмотреть набор кусков и партиций таблицы, можно воспользоваться системной таблицей `system.parts`:
 
-``` sql
+```sql
 SELECT * FROM system.parts WHERE active
 ```
 
@@ -122,7 +122,7 @@ drwxrwxrwx 2 clickhouse clickhouse  4096 May  5 02:55 detached
 
 Директория `detached` содержит куски, не используемые сервером - отцепленные от таблицы с помощью запроса `ALTER ... DETACH`. Также в эту директорию переносятся куски, признанные повреждёнными, вместо их удаления. Вы можете в любое время добавлять, удалять, модифицировать данные в директории detached - сервер не будет об этом знать, пока вы не сделаете запрос `ALTER TABLE ... ATTACH`.
 
-``` sql
+```sql
 ALTER TABLE [db.]table DETACH PARTITION 'name'
 ```
 
@@ -133,13 +133,13 @@ ALTER TABLE [db.]table DETACH PARTITION 'name'
 
 Запрос реплицируется - данные будут перенесены в директорию detached и забыты на всех репликах. Запрос может быть отправлен только на реплику-лидер. Вы можете узнать, является ли реплика лидером, сделав SELECT в системную таблицу system.replicas. Или, проще, вы можете выполнить запрос на всех репликах, и на всех кроме одной, он кинет исключение.
 
-``` sql
+```sql
 ALTER TABLE [db.]table DROP PARTITION 'name'
 ```
 
 Аналогично операции `DETACH`. Удалить данные из таблицы. Куски с данными будут помечены как неактивные и будут полностью удалены примерно через 10 минут. Запрос реплицируется - данные будут удалены на всех репликах.
 
-``` sql
+```sql
 ALTER TABLE [db.]table ATTACH PARTITION|PART 'name'
 ```
 
@@ -151,7 +151,7 @@ ALTER TABLE [db.]table ATTACH PARTITION|PART 'name'
 
 То есть, вы можете разместить данные в директории detached на одной реплике и, с помощью запроса ALTER ... ATTACH добавить их в таблицу на всех репликах.
 
-``` sql
+```sql
 ALTER TABLE [db.]table FREEZE PARTITION 'name'
 ```
 
@@ -195,7 +195,7 @@ ALTER TABLE [db.]table FREEZE PARTITION 'name'
 Бэкапы защищают от человеческих ошибок (случайно удалили данные, удалили не те данные или не на том кластере, испортили данные).
 Для баз данных большого объёма, бывает затруднительно копировать бэкапы на удалённые серверы. В этих случаях, для защиты от человеческой ошибки, можно держать бэкап на том же сервере (он будет лежать в `/var/lib/clickhouse/shadow/`).
 
-``` sql
+```sql
 ALTER TABLE [db.]table FETCH PARTITION 'name' FROM 'path-in-zookeeper'
 ```
 
@@ -231,13 +231,13 @@ ALTER TABLE [db.]table FETCH PARTITION 'name' FROM 'path-in-zookeeper'
 
 На данный момент доступны команды:
 
-``` sql
+```sql
 ALTER TABLE [db.]table DELETE WHERE filter_expr
 ```
 
 Выражение `filter_expr` должно иметь тип UInt8. Запрос удаляет строки таблицы, для которых это выражение принимает ненулевое значение.
 
-``` sql
+```sql
 ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr
 ```
 
@@ -270,5 +270,3 @@ ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr
 **parts_to_do** - Количество кусков таблицы, которые ещё предстоит изменить.
 
 **is_done** - Завершена ли мутация. Замечание: даже если `parts_to_do = 0`, для реплицированной таблицы возможна ситуация, когда мутация ещё не завершена из-за долго выполняющейся вставки, которая добавляет данные, которые нужно будет мутировать.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/alter/) <!--hide-->
diff --git a/docs/ru/query_language/create.md b/docs/ru/query_language/create.md
index 83a137c7b..a33692d12 100644
--- a/docs/ru/query_language/create.md
+++ b/docs/ru/query_language/create.md
@@ -1,7 +1,7 @@
 ## CREATE DATABASE
 Создание базы данных db_name
 
-``` sql
+```sql
 CREATE DATABASE [IF NOT EXISTS] db_name
 ```
 
@@ -14,7 +14,7 @@ CREATE DATABASE [IF NOT EXISTS] db_name
 ## CREATE TABLE
 Запрос `CREATE TABLE` может иметь несколько форм.
 
-``` sql
+```sql
 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
 (
     name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
@@ -29,13 +29,13 @@ CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
 Описание столбца, это `name type`, в простейшем случае. Пример: `RegionID UInt32`.
 Также могут быть указаны выражения для значений по умолчанию - смотрите ниже.
 
-``` sql
+```sql
 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name AS [db2.]name2 [ENGINE = engine]
 ```
 
 Создаёт таблицу с такой же структурой, как другая таблица. Можно указать другой движок для таблицы. Если движок не указан, то будет выбран такой же движок, как у таблицы `db2.name2`.
 
-``` sql
+```sql
 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name ENGINE = engine AS SELECT ...
 ```
 
@@ -97,7 +97,7 @@ CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name ENGINE = engine AS SELECT ...
 Запросы `CREATE`, `DROP`, `ALTER`, `RENAME` поддерживают возможность распределенного выполнения на кластере.
 Например, следующий запрос создает `Distributed`-таблицу `all_hits` на каждом хосте кластера `cluster`:
 
-``` sql
+```sql
 CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE = Distributed(cluster, default, hits)
 ```
 
@@ -107,7 +107,7 @@ CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE
 
 ## CREATE VIEW
 
-``` sql
+```sql
 CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...
 ```
 
@@ -121,19 +121,19 @@ CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]name [TO[db.]name] [ENGINE = eng
 
 Для примера, пусть вы создали представление:
 
-``` sql
+```sql
 CREATE VIEW view AS SELECT ...
 ```
 
 и написали запрос:
 
-``` sql
+```sql
 SELECT a, b, c FROM view
 ```
 
 Этот запрос полностью эквивалентен использованию подзапроса:
 
-``` sql
+```sql
 SELECT a, b, c FROM (SELECT ...)
 ```
 
@@ -153,5 +153,3 @@ SELECT a, b, c FROM (SELECT ...)
 
 Отсутствует отдельный запрос для удаления представлений. Чтобы удалить представление, следует использовать `DROP TABLE`.
 
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/create/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/external_dicts.md b/docs/ru/query_language/dicts/external_dicts.md
index d1df2c499..6fdd4d2d2 100644
--- a/docs/ru/query_language/dicts/external_dicts.md
+++ b/docs/ru/query_language/dicts/external_dicts.md
@@ -48,5 +48,3 @@ ClickHouse:
 - [Источники внешних словарей](external_dicts_dict_sources.md#dicts-external_dicts_dict_sources)
 - [Ключ и поля словаря](external_dicts_dict_structure.md#dicts-external_dicts_dict_structure)
 - [Функции для работы с внешними словарями](../functions/ext_dict_functions.md#ext_dict_functions)
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/external_dicts/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/external_dicts_dict.md b/docs/ru/query_language/dicts/external_dicts_dict.md
index 0c56593fd..7ba8588dc 100644
--- a/docs/ru/query_language/dicts/external_dicts_dict.md
+++ b/docs/ru/query_language/dicts/external_dicts_dict.md
@@ -31,5 +31,3 @@
 -  [layout](external_dicts_dict_layout.md#dicts-external_dicts_dict_layout) - Размещение словаря в памяти.
 -  [structure](external_dicts_dict_structure.md#dicts-external_dicts_dict_structure) - Структура словаря. Ключ и атрибуты, которые можно получить по ключу.
 -  [lifetime](external_dicts_dict_lifetime.md#dicts-external_dicts_dict_lifetime) - Периодичность обновления словарей.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/external_dicts_dict/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/external_dicts_dict_layout.md b/docs/ru/query_language/dicts/external_dicts_dict_layout.md
index 199b6926f..d2ee91d5d 100644
--- a/docs/ru/query_language/dicts/external_dicts_dict_layout.md
+++ b/docs/ru/query_language/dicts/external_dicts_dict_layout.md
@@ -290,5 +290,3 @@ dictGetString('prefix', 'asn', tuple(IPv6StringToNum('2001:db8::1')))
 Никакие другие типы не поддерживаются. Функция возвращает атрибут для префикса, соответствующего данному IP-адресу. Если есть перекрывающиеся префиксы, возвращается наиболее специфический.
 
 Данные хранятся в побитовом дереве (`trie`), он должены полностью помещаться в оперативной памяти.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/external_dicts_dict_layout/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/external_dicts_dict_lifetime.md b/docs/ru/query_language/dicts/external_dicts_dict_lifetime.md
index 4e78a8381..f88986843 100644
--- a/docs/ru/query_language/dicts/external_dicts_dict_lifetime.md
+++ b/docs/ru/query_language/dicts/external_dicts_dict_lifetime.md
@@ -56,5 +56,3 @@ ClickHouse периодически обновляет словари. Инте
     ...
 </dictionary>
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/external_dicts_dict_lifetime/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/external_dicts_dict_sources.md b/docs/ru/query_language/dicts/external_dicts_dict_sources.md
index e1fa4578a..6b2c455b5 100644
--- a/docs/ru/query_language/dicts/external_dicts_dict_sources.md
+++ b/docs/ru/query_language/dicts/external_dicts_dict_sources.md
@@ -111,7 +111,7 @@
 ```xml
 <odbc>
     <db>DatabaseName</db>
-    <table>ShemaName.TableName</table>
+    <table>TableName</table>
     <connection_string>DSN=some_parameters</connection_string>
     <invalidate_query>SQL_QUERY</invalidate_query>
 </odbc>
@@ -119,13 +119,11 @@
 
 Поля настройки:
 
--   `db` - имя базы данных. Не указывать, если имя базы задано в параметрах. `<connection_string>`.
--   `table` - имя таблицы и схемы, если она есть.
+-   `db` - имя базы данных. Не указывать, если имя базы задано в параметрах `<connection_string>`.
+-   `table` - имя таблицы.
 -   `connection_string` - строка соединения.
 -   `invalidate_query` - запрос для проверки статуса словаря. Необязательный параметр. Читайте подробнее в разделе [Обновление словарей](external_dicts_dict_lifetime.md#dicts-external_dicts_dict_lifetime).
 
-ClickHouse получает от ODBC-драйвера информацию о квотировании и квотирует настройки в запросах к драйверу, поэтому имя таблицы нужно указывать в соответствии с регистром имени таблицы в базе данных.
-
 ### Выявленная уязвимость в функционировании ODBC словарей
 
 !!! attention
@@ -423,5 +421,3 @@ MySQL можно подключить на локальном хосте чер
 -   `password` - пароль пользователя MongoDB.
 -   `db` - имя базы данных.
 -   `collection` - имя коллекции.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/external_dicts_dict_sources/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/external_dicts_dict_structure.md b/docs/ru/query_language/dicts/external_dicts_dict_structure.md
index e26eb0282..15952024c 100644
--- a/docs/ru/query_language/dicts/external_dicts_dict_structure.md
+++ b/docs/ru/query_language/dicts/external_dicts_dict_structure.md
@@ -116,5 +116,3 @@ ClickHouse поддерживает следующие виды ключей:
 -   `injective` - Признак инъективности отображения `id -> attribute`. Если `true`, то можно оптимизировать `GROUP BY`. По умолчанию, `false`.
 -   `is_object_id` - Признак того, что запрос выполняется к документу MongoDB по `ObjectID`.
 
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/external_dicts_dict_structure/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/index.md b/docs/ru/query_language/dicts/index.md
index 2ea9f67ce..30d5d705b 100644
--- a/docs/ru/query_language/dicts/index.md
+++ b/docs/ru/query_language/dicts/index.md
@@ -10,5 +10,3 @@ ClickHouse поддерживает:
 
 - [Встроенные словари](internal_dicts.md#internal_dicts) со специфическим [набором функций](../functions/ym_dict_functions.md#ym_dict_functions).
 - [Подключаемые (внешние) словари](external_dicts.md#dicts-external_dicts) с [набором функций](../functions/ext_dict_functions.md#ext_dict_functions).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/) <!--hide-->
diff --git a/docs/ru/query_language/dicts/internal_dicts.md b/docs/ru/query_language/dicts/internal_dicts.md
index da4e9846e..0a61652dd 100644
--- a/docs/ru/query_language/dicts/internal_dicts.md
+++ b/docs/ru/query_language/dicts/internal_dicts.md
@@ -46,5 +46,3 @@ ClickHouse содержит встроенную возможность рабо
 Рекомендуется периодически обновлять словари с геобазой. При обновлении, генерируйте новые файлы, записывая их в отдельное место, а только когда всё готово - переименовывайте в файлы, которые использует сервер.
 
 Также имеются функции для работы с идентификаторами операционных систем и поисковых систем Яндекс.Метрики, пользоваться которыми не нужно.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/dicts/internal_dicts/) <!--hide-->
diff --git a/docs/ru/query_language/functions/arithmetic_functions.md b/docs/ru/query_language/functions/arithmetic_functions.md
index 8707e0dda..70c6826d8 100644
--- a/docs/ru/query_language/functions/arithmetic_functions.md
+++ b/docs/ru/query_language/functions/arithmetic_functions.md
@@ -4,11 +4,11 @@
 
 Пример:
 
-``` sql
+```sql
 SELECT toTypeName(0), toTypeName(0 + 0), toTypeName(0 + 0 + 0), toTypeName(0 + 0 + 0 + 0)
 ```
 
-```
+```text
 ┌─toTypeName(0)─┬─toTypeName(plus(0, 0))─┬─toTypeName(plus(plus(0, 0), 0))─┬─toTypeName(plus(plus(plus(0, 0), 0), 0))─┐
 │ UInt8         │ UInt16                 │ UInt32                          │ UInt64                                   │
 └───────────────┴────────────────────────┴─────────────────────────────────┴──────────────────────────────────────────┘
@@ -71,5 +71,3 @@ SELECT toTypeName(0), toTypeName(0 + 0), toTypeName(0 + 0 + 0), toTypeName(0 + 0
 ## lcm(a, b)
 Вычисляет наименьшее общее кратное чисел.
 При делении на ноль или при делении минимального отрицательного числа на минус единицу, кидается исключение.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/arithmetic_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/array_functions.md b/docs/ru/query_language/functions/array_functions.md
index 53bf3b94a..4fab96f32 100644
--- a/docs/ru/query_language/functions/array_functions.md
+++ b/docs/ru/query_language/functions/array_functions.md
@@ -53,7 +53,7 @@ arrayConcat(arrays)
 
 **Пример**
 
-``` sql
+```sql
 SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res
 ```
 ```
@@ -123,7 +123,7 @@ SELECT countEqual([1, 2, NULL, NULL], NULL)
 
 Эта функция обычно используется совместно с ARRAY JOIN. Она позволяет, после применения ARRAY JOIN, посчитать что-либо только один раз для каждого массива. Пример:
 
-``` sql
+```sql
 SELECT
     count() AS Reaches,
     countIf(num = 1) AS Hits
@@ -135,7 +135,7 @@ WHERE CounterID = 160656
 LIMIT 10
 ```
 
-```
+```text
 ┌─Reaches─┬──Hits─┐
 │   95606 │ 31406 │
 └─────────┴───────┘
@@ -143,7 +143,7 @@ LIMIT 10
 
 В этом примере, Reaches - число достижений целей (строк, получившихся после применения ARRAY JOIN), а Hits - число хитов (строк, которые были до ARRAY JOIN). В данном случае, тот же результат можно получить проще:
 
-``` sql
+```sql
 SELECT
     sum(length(GoalsReached)) AS Reaches,
     count() AS Hits
@@ -151,7 +151,7 @@ FROM test.hits
 WHERE (CounterID = 160656) AND notEmpty(GoalsReached)
 ```
 
-```
+```text
 ┌─Reaches─┬──Hits─┐
 │   95606 │ 31406 │
 └─────────┴───────┘
@@ -166,7 +166,7 @@ WHERE (CounterID = 160656) AND notEmpty(GoalsReached)
 Эта функция полезна при использовании ARRAY JOIN и агрегации по элементам массива.
 Пример:
 
-``` sql
+```sql
 SELECT
     Goals.ID AS GoalID,
     sum(Sign) AS Reaches,
@@ -181,7 +181,7 @@ ORDER BY Reaches DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌──GoalID─┬─Reaches─┬─Visits─┐
 │   53225 │    3214 │   1097 │
 │ 2825062 │    3188 │   1097 │
@@ -200,11 +200,11 @@ LIMIT 10
 
 Функция arrayEnumerateUniq может принимать несколько аргументов - массивов одинаковых размеров. В этом случае, уникальность считается для кортежей элементов на одинаковых позициях всех массивов.
 
-``` sql
+```sql
 SELECT arrayEnumerateUniq([1, 1, 1, 2, 2, 2], [1, 1, 2, 1, 1, 2]) AS res
 ```
 
-```
+```text
 ┌─res───────────┐
 │ [1,2,1,1,2,1] │
 └───────────────┘
@@ -226,7 +226,7 @@ arrayPopBack(array)
 
 **Пример**
 
-``` sql
+```sql
 SELECT arrayPopBack([1, 2, 3]) AS res
 ```
 ```
@@ -249,7 +249,7 @@ arrayPopFront(array)
 
 **Пример**
 
-``` sql
+```sql
 SELECT arrayPopFront([1, 2, 3]) AS res
 ```
 ```
@@ -273,7 +273,7 @@ arrayPushBack(array, single_value)
 
 **Пример**
 
-``` sql
+```sql
 SELECT arrayPushBack(['a'], 'b') AS res
 ```
 ```
@@ -297,7 +297,7 @@ arrayPushFront(array, single_value)
 
 **Пример**
 
-``` sql
+```sql
 SELECT arrayPushBack(['b'], 'a') AS res
 ```
 ```
@@ -359,7 +359,7 @@ arraySlice(array, offset[, length])
 
 **Пример**
 
-``` sql
+```sql
 SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res
 ```
 ```
@@ -378,5 +378,3 @@ SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res
 
 ## arrayJoin(arr)
 Особенная функция. Смотрите раздел ["Функция arrayJoin"](array_join.md#functions_arrayjoin).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/array_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/array_join.md b/docs/ru/query_language/functions/array_join.md
index 57bdbe94c..40878eee0 100644
--- a/docs/ru/query_language/functions/array_join.md
+++ b/docs/ru/query_language/functions/array_join.md
@@ -17,16 +17,14 @@
 
 Пример:
 
-``` sql
+```sql
 SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src
 ```
 
-```
+```text
 ┌─dst─┬─\'Hello\'─┬─src─────┐
 │   1 │ Hello     │ [1,2,3] │
 │   2 │ Hello     │ [1,2,3] │
 │   3 │ Hello     │ [1,2,3] │
 └─────┴───────────┴─────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/array_join/) <!--hide-->
diff --git a/docs/ru/query_language/functions/bit_functions.md b/docs/ru/query_language/functions/bit_functions.md
index 7ba32ad6b..b2fc83b6e 100644
--- a/docs/ru/query_language/functions/bit_functions.md
+++ b/docs/ru/query_language/functions/bit_functions.md
@@ -15,5 +15,3 @@
 ## bitShiftLeft(a, b)
 
 ## bitShiftRight(a, b)
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/bit_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/comparison_functions.md b/docs/ru/query_language/functions/comparison_functions.md
index 392c6c257..99879e2dc 100644
--- a/docs/ru/query_language/functions/comparison_functions.md
+++ b/docs/ru/query_language/functions/comparison_functions.md
@@ -29,5 +29,3 @@
 ## lessOrEquals, оператор `<=`
 
 ## greaterOrEquals, оператор `>=`
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/comparison_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/conditional_functions.md b/docs/ru/query_language/functions/conditional_functions.md
index 0140a0081..0393094fb 100644
--- a/docs/ru/query_language/functions/conditional_functions.md
+++ b/docs/ru/query_language/functions/conditional_functions.md
@@ -46,5 +46,3 @@ multiIf(cond_1, then_1, cond_2, then_2...else)
 │                                       ᴺᵁᴸᴸ │
 └────────────────────────────────────────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/conditional_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/date_time_functions.md b/docs/ru/query_language/functions/date_time_functions.md
index c0a654a05..40f12cea0 100644
--- a/docs/ru/query_language/functions/date_time_functions.md
+++ b/docs/ru/query_language/functions/date_time_functions.md
@@ -4,7 +4,7 @@
 
 Все функции по работе с датой и временем, для которых это имеет смысл, могут принимать второй, необязательный аргумент - имя часового пояса. Пример: Asia/Yekaterinburg. В этом случае, они используют не локальный часовой пояс (по умолчанию), а указанный.
 
-``` sql
+```sql
 SELECT
     toDateTime('2016-06-15 23:00:00') AS time,
     toDate(time) AS date_local,
@@ -12,7 +12,7 @@ SELECT
     toString(time, 'US/Samoa') AS time_samoa
 ```
 
-```
+```text
 ┌────────────────time─┬─date_local─┬─date_yekat─┬─time_samoa──────────┐
 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-16 │ 2016-06-15 09:00:00 │
 └─────────────────────┴────────────┴────────────┴─────────────────────┘
@@ -153,5 +153,3 @@ SELECT
 |%y|год, последние 2 цифры (00-99)|18|
 |%Y|год, 4 цифры|2018|
 |%%|символ %|%|
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/date_time_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/encoding_functions.md b/docs/ru/query_language/functions/encoding_functions.md
index c3825cd22..f0961822e 100644
--- a/docs/ru/query_language/functions/encoding_functions.md
+++ b/docs/ru/query_language/functions/encoding_functions.md
@@ -19,5 +19,3 @@
 
 ## bitmaskToArray(num)
 Принимает целое число. Возвращает массив чисел типа UInt64, содержащий степени двойки, в сумме дающих исходное число; числа в массиве идут по возрастанию.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/encoding_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/ext_dict_functions.md b/docs/ru/query_language/functions/ext_dict_functions.md
index d80d309a0..67702c538 100644
--- a/docs/ru/query_language/functions/ext_dict_functions.md
+++ b/docs/ru/query_language/functions/ext_dict_functions.md
@@ -38,5 +38,3 @@
 ## dictHas
 `dictHas('dict_name', id)`
 - проверить наличие ключа в словаре. Возвращает значение типа UInt8, равное 0, если ключа нет и 1, если ключ есть.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/ext_dict_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/functions_for_nulls.md b/docs/ru/query_language/functions/functions_for_nulls.md
index 9173b466f..5d5314efb 100644
--- a/docs/ru/query_language/functions/functions_for_nulls.md
+++ b/docs/ru/query_language/functions/functions_for_nulls.md
@@ -287,5 +287,3 @@ SELECT toTypeName(toNullable(10))
 │ Nullable(UInt8)            │
 └────────────────────────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/functions_for_nulls/) <!--hide-->
diff --git a/docs/ru/query_language/functions/geo.md b/docs/ru/query_language/functions/geo.md
index f1b460a4c..0b0c6fa24 100644
--- a/docs/ru/query_language/functions/geo.md
+++ b/docs/ru/query_language/functions/geo.md
@@ -25,11 +25,11 @@ greatCircleDistance(lon1Deg, lat1Deg, lon2Deg, lat2Deg)
 
 **Пример**
 
-``` sql
+```sql
 SELECT greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)
 ```
 
-```
+```text
 ┌─greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)─┐
 │                                                14132374.194975413 │
 └───────────────────────────────────────────────────────────────────┘
@@ -59,11 +59,11 @@ pointInEllipses(x, y, x₀, y₀, a₀, b₀,...,xₙ, yₙ, aₙ, bₙ)
 
 **Пример**
 
-``` sql
+```sql
 SELECT pointInEllipses(55.755831, 37.617673, 55.755831, 37.617673, 1.0, 2.0)
 ```
 
-```
+```text
 ┌─pointInEllipses(55.755831, 37.617673, 55.755831, 37.617673, 1., 2.)─┐
 │                                                                   1 │
 └─────────────────────────────────────────────────────────────────────┘
@@ -91,7 +91,7 @@ pointInPolygon((x, y), [(a, b), (c, d) ...], ...)
 
 **Пример**
 
-``` sql
+```sql
 SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res
 ```
 ```
@@ -99,5 +99,3 @@ SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res
 │   1 │
 └─────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/geo/) <!--hide-->
diff --git a/docs/ru/query_language/functions/hash_functions.md b/docs/ru/query_language/functions/hash_functions.md
index 62699d38d..905b14182 100644
--- a/docs/ru/query_language/functions/hash_functions.md
+++ b/docs/ru/query_language/functions/hash_functions.md
@@ -55,5 +55,3 @@ SipHash - криптографическая хэш-функция. Работа
 `URLHash(s)` - вычислить хэш от строки без одного завершающего символа `/`, `?` или `#` на конце, если такой там есть.
 `URLHash(s, N)` - вычислить хэш от строки до N-го уровня в иерархии URL, без одного завершающего символа `/`, `?` или `#` на конце, если такой там есть.
 Уровни аналогичные URLHierarchy. Функция специфична для Яндекс.Метрики.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/hash_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/higher_order_functions.md b/docs/ru/query_language/functions/higher_order_functions.md
index e9cb1d0de..2dfe08066 100644
--- a/docs/ru/query_language/functions/higher_order_functions.md
+++ b/docs/ru/query_language/functions/higher_order_functions.md
@@ -22,17 +22,17 @@
 
 Примеры:
 
-``` sql
+```sql
 SELECT arrayFilter(x -> x LIKE '%World%', ['Hello', 'abc World']) AS res
 ```
 
-```
+```text
 ┌─res───────────┐
 │ ['abc World'] │
 └───────────────┘
 ```
 
-``` sql
+```sql
 SELECT
     arrayFilter(
         (i, x) -> x LIKE '%World%',
@@ -41,7 +41,7 @@ SELECT
     AS res
 ```
 
-```
+```text
 ┌─res─┐
 │ [2] │
 └─────┘
@@ -71,11 +71,11 @@ SELECT
 
 Пример:
 
-``` sql
+```sql
 SELECT arrayCumSum([1, 1, 1, 1]) AS res
 ```
 
-```
+```text
 ┌─res──────────┐
 │ [1, 2, 3, 4] │
 └──────────────┘
@@ -90,11 +90,11 @@ SELECT arrayCumSum([1, 1, 1, 1]) AS res
 
 Пример:
 
-``` sql
+```sql
 SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]);
 ```
 
-```
+```text
 ┌─res────────────────┐
 │ ['world', 'hello'] │
 └────────────────────┘
@@ -103,5 +103,3 @@ SELECT arraySort((x, y) -> y, ['hello', 'world'], [2, 1]);
 ### arrayReverseSort(\[func,\] arr1, ...)
 
 Возвращает отсортированный в нисходящем порядке массив `arr1`. Если задана функция `func`, то порядок сортировки определяется результатом применения функции `func` на элементы массива (массивов).  
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/higher_order_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/in_functions.md b/docs/ru/query_language/functions/in_functions.md
index 606867c85..71aa8187e 100644
--- a/docs/ru/query_language/functions/in_functions.md
+++ b/docs/ru/query_language/functions/in_functions.md
@@ -12,5 +12,3 @@
 Функция, позволяющая достать столбец из кортежа.
 N - индекс столбца начиная с 1. N должно быть константой. N должно быть целым строго положительным числом не большим размера кортежа.
 Выполнение функции ничего не стоит.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/in_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/index.md b/docs/ru/query_language/functions/index.md
index c24e02009..870eccb24 100644
--- a/docs/ru/query_language/functions/index.md
+++ b/docs/ru/query_language/functions/index.md
@@ -61,5 +61,3 @@
 Другой пример - функция `hostName` вернёт имя сервера, на котором она выполняется, и это можно использовать для служебных целей - чтобы в запросе `SELECT` сделать `GROUP BY` по серверам.
 
 Если функция в запросе выполняется на сервере-инициаторе запроса, а вам нужно, чтобы она выполнялась на удалённых серверах, вы можете обернуть её в агрегатную функцию any или добавить в ключ в `GROUP BY`.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/ip_address_functions.md b/docs/ru/query_language/functions/ip_address_functions.md
index a0fe39a77..5d1945d55 100644
--- a/docs/ru/query_language/functions/ip_address_functions.md
+++ b/docs/ru/query_language/functions/ip_address_functions.md
@@ -11,7 +11,7 @@
 
 Пример:
 
-``` sql
+```sql
 SELECT
     IPv4NumToStringClassC(ClientIP) AS k,
     count() AS c
@@ -21,7 +21,7 @@ ORDER BY c DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─k──────────────┬─────c─┐
 │ 83.149.9.xxx   │ 26238 │
 │ 217.118.81.xxx │ 26074 │
@@ -42,17 +42,17 @@ LIMIT 10
 Принимает значение типа FixedString(16), содержащее IPv6-адрес в бинарном виде. Возвращает строку, содержащую этот адрес в текстовом виде.
 IPv6-mapped IPv4 адреса выводится в формате ::ffff:111.222.33.44. Примеры:
 
-``` sql
+```sql
 SELECT IPv6NumToString(toFixedString(unhex('2A0206B8000000000000000000000011'), 16)) AS addr
 ```
 
-```
+```text
 ┌─addr─────────┐
 │ 2a02:6b8::11 │
 └──────────────┘
 ```
 
-``` sql
+```sql
 SELECT
     IPv6NumToString(ClientIP6 AS k),
     count() AS c
@@ -63,7 +63,7 @@ ORDER BY c DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─IPv6NumToString(ClientIP6)──────────────┬─────c─┐
 │ 2a02:2168:aaa:bbbb::2                   │ 24695 │
 │ 2a02:2698:abcd:abcd:abcd:abcd:8888:5555 │ 22408 │
@@ -78,7 +78,7 @@ LIMIT 10
 └─────────────────────────────────────────┴───────┘
 ```
 
-``` sql
+```sql
 SELECT
     IPv6NumToString(ClientIP6 AS k),
     count() AS c
@@ -89,7 +89,7 @@ ORDER BY c DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─IPv6NumToString(ClientIP6)─┬──────c─┐
 │ ::ffff:94.26.111.111       │ 747440 │
 │ ::ffff:37.143.222.4        │ 529483 │
@@ -107,5 +107,3 @@ LIMIT 10
 ## IPv6StringToNum(s)
 Функция, обратная к IPv6NumToString. Если IPv6 адрес в неправильном формате, то возвращает строку из нулевых байт.
 HEX может быть в любом регистре.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/ip_address_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/json_functions.md b/docs/ru/query_language/functions/json_functions.md
index 6958f33e2..302bed008 100644
--- a/docs/ru/query_language/functions/json_functions.md
+++ b/docs/ru/query_language/functions/json_functions.md
@@ -29,7 +29,7 @@
 
 Примеры:
 
-```
+```text
 visitParamExtractRaw('{"abc":"\\n\\u0000"}', 'abc') = '"\\n\\u0000"'
 visitParamExtractRaw('{"abc":{"def":[1,2,3]}}', 'abc') = '{"def":[1,2,3]}'
 ```
@@ -39,7 +39,7 @@ visitParamExtractRaw('{"abc":{"def":[1,2,3]}}', 'abc') = '{"def":[1,2,3]}'
 
 Примеры:
 
-```
+```text
 visitParamExtractString('{"abc":"\\n\\u0000"}', 'abc') = '\n\0'
 visitParamExtractString('{"abc":"\\u263a"}', 'abc') = '☺'
 visitParamExtractString('{"abc":"\\u263"}', 'abc') = ''
@@ -47,5 +47,3 @@ visitParamExtractString('{"abc":"hello}', 'abc') = ''
 ```
 
 На данный момент, не поддерживаются записанные в формате `\uXXXX\uYYYY` кодовые точки не из basic multilingual plane (они переводятся не в UTF-8, а в CESU-8).
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/json_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/logical_functions.md b/docs/ru/query_language/functions/logical_functions.md
index 6fa2504f9..458356b5c 100644
--- a/docs/ru/query_language/functions/logical_functions.md
+++ b/docs/ru/query_language/functions/logical_functions.md
@@ -11,5 +11,3 @@
 ## not, оператор NOT
 
 ## xor
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/logical_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/math_functions.md b/docs/ru/query_language/functions/math_functions.md
index 0470a4697..bd75e4819 100644
--- a/docs/ru/query_language/functions/math_functions.md
+++ b/docs/ru/query_language/functions/math_functions.md
@@ -39,11 +39,11 @@
 
 Пример (правило трёх сигм):
 
-``` sql
+```sql
 SELECT erf(3 / sqrt(2))
 ```
 
-```
+```text
 ┌─erf(divide(3, sqrt(2)))─┐
 │      0.9973002039367398 │
 └─────────────────────────┘
@@ -78,5 +78,3 @@ SELECT erf(3 / sqrt(2))
 
 ## pow(x, y)
 Принимает два числовых аргумента x и y. Возвращает число типа Float64, близкое к x в степени y.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/math_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/other_functions.md b/docs/ru/query_language/functions/other_functions.md
index d7c70d61d..fb41049f2 100644
--- a/docs/ru/query_language/functions/other_functions.md
+++ b/docs/ru/query_language/functions/other_functions.md
@@ -72,7 +72,7 @@ SELECT visibleWidth(NULL)
 
 Пример:
 
-``` sql
+```sql
 SELECT
     toHour(EventTime) AS h,
     count() AS c,
@@ -82,7 +82,7 @@ GROUP BY h
 ORDER BY h ASC
 ```
 
-```
+```text
 ┌──h─┬──────c─┬─bar────────────────┐
 │  0 │ 292907 │ █████████▋         │
 │  1 │ 180563 │ ██████             │
@@ -141,7 +141,7 @@ ORDER BY h ASC
 
 Пример:
 
-``` sql
+```sql
 SELECT
     transform(SearchEngineID, [2, 3], ['Yandex', 'Google'], 'Other') AS title,
     count() AS c
@@ -151,7 +151,7 @@ GROUP BY title
 ORDER BY c DESC
 ```
 
-```
+```text
 ┌─title─────┬──────c─┐
 │ Yandex    │ 498635 │
 │ Google    │ 229872 │
@@ -170,7 +170,7 @@ ORDER BY c DESC
 
 Пример:
 
-``` sql
+```sql
 SELECT
     transform(domain(Referer), ['yandex.ru', 'google.ru', 'vk.com'], ['www.yandex', 'example.com']) AS s,
     count() AS c
@@ -180,7 +180,7 @@ ORDER BY count() DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─s──────────────┬───────c─┐
 │                │ 2906259 │
 │ www.yandex     │  867767 │
@@ -199,13 +199,13 @@ LIMIT 10
 
 Пример:
 
-``` sql
+```sql
 SELECT
     arrayJoin([1, 1024, 1024*1024, 192851925]) AS filesize_bytes,
     formatReadableSize(filesize_bytes) AS filesize
 ```
 
-```
+```text
 ┌─filesize_bytes─┬─filesize───┐
 │              1 │ 1.00 B     │
 │           1024 │ 1.00 KiB   │
@@ -238,7 +238,7 @@ SELECT
 
 Пример:
 
-``` sql
+```sql
 SELECT
     EventID,
     EventTime,
@@ -255,7 +255,7 @@ FROM
 )
 ```
 
-```
+```text
 ┌─EventID─┬───────────EventTime─┬─delta─┐
 │    1106 │ 2016-11-24 00:00:04 │     0 │
 │    1107 │ 2016-11-24 00:00:05 │     1 │
@@ -540,5 +540,3 @@ SELECT replicate(1, ['a', 'b', 'c'])
 │ [1,1,1]                       │
 └───────────────────────────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/other_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/random_functions.md b/docs/ru/query_language/functions/random_functions.md
index b29f50dfd..5e74ac10b 100644
--- a/docs/ru/query_language/functions/random_functions.md
+++ b/docs/ru/query_language/functions/random_functions.md
@@ -13,5 +13,3 @@
 ## rand64
 Возвращает псевдослучайное число типа UInt64, равномерно распределённое среди всех чисел типа UInt64.
 Используется linear congruential generator.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/random_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/rounding_functions.md b/docs/ru/query_language/functions/rounding_functions.md
index 269beccff..849c35013 100644
--- a/docs/ru/query_language/functions/rounding_functions.md
+++ b/docs/ru/query_language/functions/rounding_functions.md
@@ -31,7 +31,7 @@ N может быть отрицательным.
 
 **Пример**
 
-``` sql
+```sql
 SELECT
     number / 2 AS x,
     round(x)
@@ -61,5 +61,3 @@ LIMIT 10
 
 ## roundAge(num)
 Принимает число. Если число меньше 18 - возвращает 0. Иначе округляет число вниз до чисел из набора: 18, 25, 35, 45, 55. Эта функция специфична для Яндекс.Метрики и предназначена для реализации отчёта по возрасту посетителей.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/rounding_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/splitting_merging_functions.md b/docs/ru/query_language/functions/splitting_merging_functions.md
index 8561048b8..1f4d38ac7 100644
--- a/docs/ru/query_language/functions/splitting_merging_functions.md
+++ b/docs/ru/query_language/functions/splitting_merging_functions.md
@@ -25,5 +25,4 @@ SELECT alphaTokens('abca1abc')
 ┌─alphaTokens('abca1abc')─┐
 │ ['abca','abc']          │
 └─────────────────────────┘
-```
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/splitting_merging_functions/) <!--hide-->
+```
\ No newline at end of file
diff --git a/docs/ru/query_language/functions/string_functions.md b/docs/ru/query_language/functions/string_functions.md
index a917fb4d9..43842b594 100644
--- a/docs/ru/query_language/functions/string_functions.md
+++ b/docs/ru/query_language/functions/string_functions.md
@@ -58,5 +58,3 @@
 
 ## convertCharset(s, from, to)
 Возвращает сконвертированную из кодировки from в кодировку to строку s.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/string_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/string_replace_functions.md b/docs/ru/query_language/functions/string_replace_functions.md
index 498b321fb..7669b598b 100644
--- a/docs/ru/query_language/functions/string_replace_functions.md
+++ b/docs/ru/query_language/functions/string_replace_functions.md
@@ -17,7 +17,7 @@
 
 Пример 1. Переведём дату в американский формат:
 
-``` sql
+```sql
 SELECT DISTINCT
     EventDate,
     replaceRegexpOne(toString(EventDate), '(\\d{4})-(\\d{2})-(\\d{2})', '\\2/\\3/\\1') AS res
@@ -26,7 +26,7 @@ LIMIT 7
 FORMAT TabSeparated
 ```
 
-```
+```text
 2014-03-17      03/17/2014
 2014-03-18      03/18/2014
 2014-03-19      03/19/2014
@@ -38,11 +38,11 @@ FORMAT TabSeparated
 
 Пример 2. Размножить строку десять раз:
 
-``` sql
+```sql
 SELECT replaceRegexpOne('Hello, World!', '.*', '\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0') AS res
 ```
 
-```
+```text
 ┌─res────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
 │ Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World! │
 └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
@@ -51,11 +51,11 @@ SELECT replaceRegexpOne('Hello, World!', '.*', '\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0')
 ## replaceRegexpAll(haystack, pattern, replacement)
 То же самое, но делается замена всех вхождений. Пример:
 
-``` sql
+```sql
 SELECT replaceRegexpAll('Hello, World!', '.', '\\0\\0') AS res
 ```
 
-```
+```text
 ┌─res────────────────────────┐
 │ HHeelllloo,,  WWoorrlldd!! │
 └────────────────────────────┘
@@ -64,14 +64,12 @@ SELECT replaceRegexpAll('Hello, World!', '.', '\\0\\0') AS res
 В качестве исключения, если регулярное выражение сработало на пустой подстроке, то замена делается не более одного раза.
 Пример:
 
-``` sql
+```sql
 SELECT replaceRegexpAll('Hello, World!', '^', 'here: ') AS res
 ```
 
-```
+```text
 ┌─res─────────────────┐
 │ here: Hello, World! │
 └─────────────────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/string_replace_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/string_search_functions.md b/docs/ru/query_language/functions/string_search_functions.md
index 031082c66..125c57a61 100644
--- a/docs/ru/query_language/functions/string_search_functions.md
+++ b/docs/ru/query_language/functions/string_search_functions.md
@@ -44,5 +44,3 @@
 
 ## notLike(haystack, pattern), оператор haystack NOT LIKE pattern
 То же, что like, но с отрицанием.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/string_search_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/type_conversion_functions.md b/docs/ru/query_language/functions/type_conversion_functions.md
index 6b512574f..ea39c4f61 100644
--- a/docs/ru/query_language/functions/type_conversion_functions.md
+++ b/docs/ru/query_language/functions/type_conversion_functions.md
@@ -27,7 +27,7 @@
 
 Форматы даты и даты-с-временем для функций toDate/toDateTime определены следующим образом:
 
-```
+```text
 YYYY-MM-DD
 YYYY-MM-DD hh:mm:ss
 ```
@@ -40,13 +40,13 @@ YYYY-MM-DD hh:mm:ss
 
 Дополнительно, функция toString от аргумента типа DateTime может принимать второй аргумент String - имя тайм-зоны. Пример: `Asia/Yekaterinburg` В этом случае, форматирование времени производится согласно указанной тайм-зоне.
 
-``` sql
+```sql
 SELECT
     now() AS now_local,
     toString(now(), 'Asia/Yekaterinburg') AS now_yekat
 ```
 
-```
+```text
 ┌───────────now_local─┬─now_yekat───────────┐
 │ 2016-06-15 00:11:21 │ 2016-06-15 02:11:21 │
 └─────────────────────┴─────────────────────┘
@@ -63,21 +63,21 @@ SELECT
 
 Пример:
 
-``` sql
+```sql
 SELECT toFixedString('foo', 8) AS s, toStringCutToZero(s) AS s_cut
 ```
 
-```
+```text
 ┌─s─────────────┬─s_cut─┐
 │ foo\0\0\0\0\0 │ foo   │
 └───────────────┴───────┘
 ```
 
-``` sql
+```sql
 SELECT toFixedString('foo\0bar', 8) AS s, toStringCutToZero(s) AS s_cut
 ```
 
-```
+```text
 ┌─s──────────┬─s_cut─┐
 │ foo\0bar\0 │ foo   │
 └────────────┴───────┘
@@ -101,7 +101,7 @@ SELECT toFixedString('foo\0bar', 8) AS s, toStringCutToZero(s) AS s_cut
 
 Пример:
 
-``` sql
+```sql
 SELECT
     '2016-06-15 23:00:00' AS timestamp,
     CAST(timestamp AS DateTime) AS datetime,
@@ -110,7 +110,7 @@ SELECT
     CAST(timestamp, 'FixedString(22)') AS fixed_string
 ```
 
-```
+```text
 ┌─timestamp───────────┬────────────datetime─┬───────date─┬─string──────────────┬─fixed_string──────────────┐
 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00\0\0\0 │
 └─────────────────────┴─────────────────────┴────────────┴─────────────────────┴───────────────────────────┘
@@ -135,5 +135,3 @@ SELECT toTypeName(CAST(x, 'Nullable(UInt16)')) FROM t_null
 │ Nullable(UInt16)                        │
 └─────────────────────────────────────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/type_conversion_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/url_functions.md b/docs/ru/query_language/functions/url_functions.md
index 4b4fdc9ad..f4697b953 100644
--- a/docs/ru/query_language/functions/url_functions.md
+++ b/docs/ru/query_language/functions/url_functions.md
@@ -57,7 +57,7 @@
 То же самое, но без протокола и хоста в результате. Элемент / (корень) не включается. Пример:
 Функция используется для реализации древовидных отчётов по URL в Яндекс.Метрике.
 
-```
+```text
 URLPathHierarchy('https://example.com/browse/CONV-6788') =
 [
     '/browse/',
@@ -69,11 +69,11 @@ URLPathHierarchy('https://example.com/browse/CONV-6788') =
 Возвращает декодированный URL.
 Пример:
 
-``` sql
+```sql
 SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS DecodedURL;
 ```
 
-```
+```text
 ┌─DecodedURL─────────────────────────────┐
 │ http://127.0.0.1:8123/?query=SELECT 1; │
 └────────────────────────────────────────┘
@@ -97,5 +97,3 @@ SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS Decod
 
 ### cutURLParameter(URL, name)
 Удаляет параметр URL с именем name, если такой есть. Функция работает при допущении, что имя параметра закодировано в URL в точности таким же образом, что и в переданном аргументе.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/url_functions/) <!--hide-->
diff --git a/docs/ru/query_language/functions/ym_dict_functions.md b/docs/ru/query_language/functions/ym_dict_functions.md
index 7d37e1b0c..1761a21a7 100644
--- a/docs/ru/query_language/functions/ym_dict_functions.md
+++ b/docs/ru/query_language/functions/ym_dict_functions.md
@@ -23,7 +23,7 @@ ClickHouse поддерживает работу одновременно с н
 Во все функции по работе с регионами, в конце добавлен один необязательный аргумент - ключ словаря. Далее он обозначен как geobase.
 Пример:
 
-```
+```text
 regionToCountry(RegionID) - использует словарь по умолчанию: /opt/geo/regions_hierarchy.txt;
 regionToCountry(RegionID, '') - использует словарь по умолчанию: /opt/geo/regions_hierarchy.txt;
 regionToCountry(RegionID, 'ua') - использует словарь для ключа ua: /opt/geo/regions_hierarchy_ua.txt;
@@ -35,13 +35,13 @@ regionToCountry(RegionID, 'ua') - использует словарь для к
 ### regionToArea(id\[, geobase\])
 Переводит регион в область (тип в геобазе - 5). В остальном, аналогично функции regionToCity.
 
-``` sql
+```sql
 SELECT DISTINCT regionToName(regionToArea(toUInt32(number), 'ua'))
 FROM system.numbers
 LIMIT 15
 ```
 
-```
+```text
 ┌─regionToName(regionToArea(toUInt32(number), \'ua\'))─┐
 │                                                      │
 │ Москва и Московская область                          │
@@ -64,13 +64,13 @@ LIMIT 15
 ### regionToDistrict(id\[, geobase\])
 Переводит регион в федеральный округ (тип в геобазе - 4). В остальном, аналогично функции regionToCity.
 
-``` sql
+```sql
 SELECT DISTINCT regionToName(regionToDistrict(toUInt32(number), 'ua'))
 FROM system.numbers
 LIMIT 15
 ```
 
-```
+```text
 ┌─regionToName(regionToDistrict(toUInt32(number), \'ua\'))─┐
 │                                                          │
 │ Центральный федеральный округ                            │
@@ -116,5 +116,3 @@ LIMIT 15
 Принимает число типа UInt32 - идентификатор региона из геобазы Яндекса. Вторым аргументом может быть передана строка - название языка. Поддерживаются языки ru, en, ua, uk, by, kz, tr. Если второй аргумент отсутствует - используется язык ru. Если язык не поддерживается - кидается исключение. Возвращает строку - название региона на соответствующем языке. Если региона с указанным идентификатором не существует - возвращается пустая строка.
 
 `ua` и `uk` обозначают одно и то же - украинский язык.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/functions/ym_dict_functions/) <!--hide-->
diff --git a/docs/ru/query_language/index.md b/docs/ru/query_language/index.md
index 59313b15f..01666d3e0 100644
--- a/docs/ru/query_language/index.md
+++ b/docs/ru/query_language/index.md
@@ -5,5 +5,3 @@
 * [CREATE](create.md#create-database)
 * [ALTER](alter.md#query_language_queries_alter)
 * [Прочие виды запросов](misc.md#miscellaneous-queries)
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/) <!--hide-->
diff --git a/docs/ru/query_language/insert_into.md b/docs/ru/query_language/insert_into.md
index 70e4d5529..9d2e6280e 100644
--- a/docs/ru/query_language/insert_into.md
+++ b/docs/ru/query_language/insert_into.md
@@ -6,7 +6,7 @@
 
 Базовый формат запроса:
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...
 ```
 
@@ -19,13 +19,13 @@ INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), .
 
 В INSERT можно передавать данные любого [формата](../interfaces/formats.md#formats), который поддерживает ClickHouse. Для этого формат необходимо указать в запросе в явном виде:
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] FORMAT format_name data_set
 ```
 
 Например, следующий формат запроса идентичен базовому варианту INSERT ... VALUES:
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] FORMAT Values (v11, v12, v13), (v21, v22, v23), ...
 ```
 
@@ -33,7 +33,7 @@ ClickHouse отсекает все пробелы и один перенос с
 
 Пример:
 
-``` sql
+```sql
 INSERT INTO t FORMAT TabSeparated
 11  Hello, world!
 22  Qwerty
@@ -43,7 +43,7 @@ INSERT INTO t FORMAT TabSeparated
 
 ### Вставка результатов `SELECT`
 
-``` sql
+```sql
 INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...
 ```
 
@@ -65,5 +65,3 @@ INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...
 
 -   Данные поступают в режиме реального времени.
 -   Вы загружаете данные, которые как правило отсортированы по времени.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/insert_into/) <!--hide-->
diff --git a/docs/ru/query_language/misc.md b/docs/ru/query_language/misc.md
index 5bfddb64b..86080a736 100644
--- a/docs/ru/query_language/misc.md
+++ b/docs/ru/query_language/misc.md
@@ -12,7 +12,7 @@
 
 Если таблица перед этим была отсоединена (`DETACH`), т.е. её структура известна, то можно использовать сокращенную форму записи без определения структуры.
 
-``` sql
+```sql
 ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
 ```
 
@@ -21,14 +21,14 @@ ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]
 ## DROP
 Запрос имеет два вида: `DROP DATABASE` и `DROP TABLE`.
 
-``` sql
+```sql
 DROP DATABASE [IF EXISTS] db [ON CLUSTER cluster]
 ```
 
 Удаляет все таблицы внутри базы данных db, а затем саму базу данных db.
 Если указано `IF EXISTS` - не выдавать ошибку, если база данных не существует.
 
-``` sql
+```sql
 DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
 ```
 
@@ -38,7 +38,7 @@ DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
 ## DETACH
 Удаляет из сервера информацию о таблице name. Сервер перестаёт знать о существовании таблицы.
 
-``` sql
+```sql
 DETACH TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
 ```
 
@@ -50,7 +50,7 @@ DETACH TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]
 ## RENAME
 Переименовывает одну или несколько таблиц.
 
-``` sql
+```sql
 RENAME TABLE [db11.]name11 TO [db12.]name12, [db21.]name21 TO [db22.]name22, ... [ON CLUSTER cluster]
 ```
 
@@ -58,7 +58,7 @@ RENAME TABLE [db11.]name11 TO [db12.]name12, [db21.]name21 TO [db22.]name22, ...
 
 ## SHOW DATABASES
 
-``` sql
+```sql
 SHOW DATABASES [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -69,7 +69,7 @@ SHOW DATABASES [INTO OUTFILE filename] [FORMAT format]
 
 ## SHOW TABLES
 
-``` sql
+```sql
 SHOW [TEMPORARY] TABLES [FROM db] [LIKE 'pattern'] [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -84,7 +84,7 @@ SHOW [TEMPORARY] TABLES [FROM db] [LIKE 'pattern'] [INTO OUTFILE filename] [FORM
 
 ## SHOW PROCESSLIST
 
-``` sql
+```sql
 SHOW PROCESSLIST [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -116,7 +116,7 @@ watch -n1 "clickhouse-client --query='SHOW PROCESSLIST'"
 
 ## SHOW CREATE TABLE
 
-``` sql
+```sql
 SHOW CREATE [TEMPORARY] TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -124,7 +124,7 @@ SHOW CREATE [TEMPORARY] TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
 
 ## DESCRIBE TABLE
 
-``` sql
+```sql
 DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -134,7 +134,7 @@ DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format]
 
 ## EXISTS
 
-``` sql
+```sql
 EXISTS [TEMPORARY] TABLE [db.]name [INTO OUTFILE filename] [FORMAT format]
 ```
 
@@ -142,7 +142,7 @@ EXISTS [TEMPORARY] TABLE [db.]name [INTO OUTFILE filename] [FORMAT format]
 
 ## USE
 
-``` sql
+```sql
 USE db
 ```
 
@@ -152,7 +152,7 @@ USE db
 
 ## SET
 
-``` sql
+```sql
 SET param = value
 ```
 
@@ -165,7 +165,7 @@ SET param = value
 
 ## OPTIMIZE
 
-``` sql
+```sql
 OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition] [FINAL]
 ```
 
@@ -179,7 +179,7 @@ OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition] [FINAL]
 
 ## KILL QUERY
 
-``` sql
+```sql
 KILL QUERY [ON CLUSTER cluster]
   WHERE <where expression to SELECT FROM system.processes query>
   [SYNC|ASYNC|TEST]
@@ -190,7 +190,7 @@ KILL QUERY [ON CLUSTER cluster]
 Запросы для принудительной остановки выбираются из таблицы system.processes с помощью условия, указанного в секции `WHERE` запроса `KILL`.
 
 Примеры:
-``` sql
+```sql
 -- Принудительно останавливает все запросы с указанным query_id:
 KILL QUERY WHERE query_id='2-857d-4a57-9ee0-327da5d60a90'
 
@@ -210,5 +210,3 @@ Readonly-пользователи могут останавливать толь
 3.  остальные значения описывают причину невозможности остановки запроса.
 
 Тестовый вариант запроса (`TEST`) только проверяет права пользователя и выводит список запросов для остановки.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/misc/) <!--hide-->
diff --git a/docs/ru/query_language/operators.md b/docs/ru/query_language/operators.md
index 3814e1699..c5f90588f 100644
--- a/docs/ru/query_language/operators.md
+++ b/docs/ru/query_language/operators.md
@@ -87,7 +87,7 @@
 
 ## Условное выражение
 
-``` sql
+```sql
 CASE [x]
     WHEN a THEN b
     [WHEN ... THEN ...]
@@ -173,5 +173,3 @@ WHERE isNotNull(y)
 
 1 rows in set. Elapsed: 0.002 sec.
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/operators/) <!--hide-->
diff --git a/docs/ru/query_language/select.md b/docs/ru/query_language/select.md
index 4bf5e1020..81537d157 100644
--- a/docs/ru/query_language/select.md
+++ b/docs/ru/query_language/select.md
@@ -2,7 +2,7 @@
 
 `SELECT` осуществляет выборку данных.
 
-``` sql
+```sql
 SELECT [DISTINCT] expr_list
     [FROM [db.]table | (subquery) | table_function] [FINAL]
     [SAMPLE sample_coeff]
@@ -59,7 +59,7 @@ SELECT [DISTINCT] expr_list
 
 Пример:
 
-``` sql
+```sql
 SELECT
     Title,
     count() * 10 AS PageViews
@@ -90,7 +90,7 @@ ORDER BY PageViews DESC LIMIT 1000
 
 `ARRAY JOIN` - это, по сути, `INNER JOIN` с массивом. Пример:
 
-```
+```text
 :) CREATE TABLE arrays_test (s String, arr Array(UInt8)) ENGINE = Memory
 
 CREATE TABLE arrays_test
@@ -143,7 +143,7 @@ ARRAY JOIN arr
 
 Для массива в секции ARRAY JOIN может быть указан алиас. В этом случае, элемент массива будет доступен под этим алиасом, а сам массив - под исходным именем. Пример:
 
-```
+```text
 :) SELECT s, arr, a FROM arrays_test ARRAY JOIN arr AS a
 
 SELECT s, arr, a
@@ -163,7 +163,7 @@ ARRAY JOIN arr AS a
 
 В секции ARRAY JOIN может быть указано несколько массивов одинаковых размеров через запятую. В этом случае, JOIN делается с ними одновременно (прямая сумма, а не прямое произведение). Пример:
 
-```
+```text
 :) SELECT s, arr, a, num, mapped FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -> x + 1, arr) AS mapped
 
 SELECT s, arr, a, num, mapped
@@ -199,7 +199,7 @@ ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num
 
 ARRAY JOIN также работает с вложенными структурами данных. Пример:
 
-```
+```text
 :) CREATE TABLE nested_test (s String, nest Nested(x UInt8, y UInt32)) ENGINE = Memory
 
 CREATE TABLE nested_test
@@ -254,7 +254,7 @@ ARRAY JOIN nest
 
 При указании имени вложенной структуры данных в ARRAY JOIN, смысл такой же, как ARRAY JOIN со всеми элементами-массивами, из которых она состоит. Пример:
 
-```
+```text
 :) SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x, nest.y
 
 SELECT s, `nest.x`, `nest.y`
@@ -274,7 +274,7 @@ ARRAY JOIN `nest.x`, `nest.y`
 
 Такой вариант тоже имеет смысл:
 
-```
+```text
 :) SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x
 
 SELECT s, `nest.x`, `nest.y`
@@ -294,7 +294,7 @@ ARRAY JOIN `nest.x`
 
 Алиас для вложенной структуры данных можно использовать, чтобы выбрать как результат JOIN-а, так и исходный массив. Пример:
 
-```
+```text
 :) SELECT s, n.x, n.y, nest.x, nest.y FROM nested_test ARRAY JOIN nest AS n
 
 SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`
@@ -314,7 +314,7 @@ ARRAY JOIN nest AS n
 
 Пример использования функции arrayEnumerate:
 
-```
+```text
 :) SELECT s, n.x, n.y, nest.x, nest.y, num FROM nested_test ARRAY JOIN nest AS n, arrayEnumerate(nest.x) AS num
 
 SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`, num
@@ -343,7 +343,7 @@ ARRAY JOIN nest AS n, arrayEnumerate(`nest.x`) AS num
 
 Обычный JOIN, не имеет отношения к ARRAY JOIN, который описан выше.
 
-``` sql
+```sql
 [GLOBAL] ANY|ALL INNER|LEFT [OUTER] JOIN (subquery)|table USING columns_list
 ```
 
@@ -380,7 +380,7 @@ JOIN-ы бывают нескольких видов:
 
 Пример:
 
-``` sql
+```sql
 SELECT
     CounterID,
     hits,
@@ -404,7 +404,7 @@ ORDER BY hits DESC
 LIMIT 10
 ```
 
-```
+```text
 ┌─CounterID─┬───hits─┬─visits─┐
 │   1143050 │ 523264 │  13665 │
 │    731962 │ 475698 │ 102716 │
@@ -500,7 +500,7 @@ WHERE isNull(y)
 
 Пример:
 
-``` sql
+```sql
 SELECT
     count(),
     median(FetchTiming > 60 ? 60 : FetchTiming),
@@ -514,7 +514,7 @@ FROM hits
 
 Пример:
 
-``` sql
+```sql
 SELECT
     domainWithoutWWW(URL) AS domain,
     count(),
@@ -610,7 +610,7 @@ GROUP BY вычисляет для каждого встретившегося 
 
 Пример:
 
-``` sql
+```sql
 SELECT
     domainWithoutWWW(URL) AS domain,
     domainWithoutWWW(REFERRER_URL) AS referrer,
@@ -731,7 +731,7 @@ n и m должны быть неотрицательными целыми чи
 
 Произвольное количество запросов может быть объединено с помощью `UNION ALL`. Пример:
 
-``` sql
+```sql
 SELECT CounterID, 1 AS table, toInt64(count()) AS c
     FROM test.hits
     GROUP BY CounterID
@@ -779,7 +779,7 @@ SELECT CounterID, 2 AS table, sum(Sign) AS c
 
 Примеры:
 
-``` sql
+```sql
 SELECT UserID IN (123, 456) FROM ...
 SELECT (CounterID, UserID) IN ((34, 123), (101500, 456)) FROM ...
 ```
@@ -798,7 +798,7 @@ SELECT (CounterID, UserID) IN ((34, 123), (101500, 456)) FROM ...
 В подзапросе может быть указано более одного столбца для фильтрации кортежей.
 Пример:
 
-``` sql
+```sql
 SELECT (CounterID, UserID) IN (SELECT CounterID, UserID FROM ...) FROM ...
 ```
 
@@ -807,7 +807,7 @@ SELECT (CounterID, UserID) IN (SELECT CounterID, UserID FROM ...) FROM ...
 Оператор IN и подзапрос могут встречаться в любой части запроса, в том числе в агрегатных и лямбда функциях.
 Пример:
 
-``` sql
+```sql
 SELECT
     EventDate,
     avg(UserID IN
@@ -821,7 +821,7 @@ GROUP BY EventDate
 ORDER BY EventDate ASC
 ```
 
-```
+```text
 ┌──EventDate─┬────ratio─┐
 │ 2014-03-17 │        1 │
 │ 2014-03-18 │ 0.807696 │
@@ -893,13 +893,13 @@ FROM t_null
 
 Например, запрос
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table
 ```
 
 будет отправлен на все удалённые серверы в виде
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table
 ```
 
@@ -907,7 +907,7 @@ SELECT uniq(UserID) FROM local_table
 
 Теперь рассмотрим запрос с IN-ом:
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
 ```
 
@@ -915,7 +915,7 @@ SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID I
 
 Этот запрос будет отправлен на все удалённые серверы в виде
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)
 ```
 
@@ -925,19 +925,19 @@ SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SEL
 
 Чтобы исправить работу запроса, когда данные размазаны по серверам кластера произвольным образом, можно было бы указать **distributed_table** внутри подзапроса. Запрос будет выглядеть так:
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
 ```
 
 Этот запрос будет отправлен на все удалённые серверы в виде
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
 ```
 
 На каждом удалённом сервере начнёт выполняться подзапрос. Так как в подзапросе используется распределённая таблица, то подзапрос будет, на каждом удалённом сервере, снова отправлен на каждый удалённый сервер, в виде
 
-``` sql
+```sql
 SELECT UserID FROM local_table WHERE CounterID = 34
 ```
 
@@ -945,19 +945,19 @@ SELECT UserID FROM local_table WHERE CounterID = 34
 
 В таких случаях всегда следует использовать GLOBAL IN вместо IN. Рассмотрим его работу для запроса
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)
 ```
 
 На сервере-инициаторе запроса будет выполнен подзапрос
 
-``` sql
+```sql
 SELECT UserID FROM distributed_table WHERE CounterID = 34
 ```
 
 , и результат будет сложен во временную таблицу в оперативке. Затем запрос будет отправлен на каждый удалённый сервер в виде
 
-``` sql
+```sql
 SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL IN _data1
 ```
 
@@ -999,5 +999,3 @@ SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL
 -   в подзапросах (так как из подзапросов выкидываются столбцы, не нужные для внешнего запроса).
 
 В других случаях использование звёздочки является издевательством над системой, так как вместо преимуществ столбцовой СУБД вы получаете недостатки. То есть использовать звёздочку не рекомендуется.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/select/) <!--hide-->
diff --git a/docs/ru/query_language/syntax.md b/docs/ru/query_language/syntax.md
index a09074a56..95609b371 100644
--- a/docs/ru/query_language/syntax.md
+++ b/docs/ru/query_language/syntax.md
@@ -4,7 +4,7 @@
 Во всех случаях кроме запроса INSERT, используется только полноценный парсер SQL.
 В запросе INSERT используется оба парсера:
 
-``` sql
+```sql
 INSERT INTO t VALUES (1, 'Hello, world'), (2, 'abc'), (3, 'def')
 ```
 
@@ -100,7 +100,7 @@ INSERT INTO t VALUES (1, 'Hello, world'), (2, 'abc'), (3, 'def')
 
 В запросе SELECT, в выражениях могут быть указаны синонимы с помощью ключевого слова AS. Слева от AS стоит любое выражение. Справа от AS стоит идентификатор - имя для синонима. В отличие от стандартного SQL, синонимы могут объявляться не только на верхнем уровне выражений:
 
-``` sql
+```sql
 SELECT (1 AS n) + 2, n
 ```
 
@@ -115,5 +115,3 @@ SELECT (1 AS n) + 2, n
 Выражение представляет собой функцию, идентификатор, литерал, применение оператора, выражение в скобках, подзапрос, звёздочку; и может содержать синоним.
 Список выражений - одно выражение или несколько выражений через запятую.
 Функции и операторы, в свою очередь, в качестве аргументов, могут иметь произвольные выражения.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/syntax/) <!--hide-->
diff --git a/docs/ru/query_language/table_functions/file.md b/docs/ru/query_language/table_functions/file.md
index f0fe3d468..9e029a6b7 100644
--- a/docs/ru/query_language/table_functions/file.md
+++ b/docs/ru/query_language/table_functions/file.md
@@ -34,7 +34,7 @@ $ cat /var/lib/clickhouse/user_files/test.csv
 
 Таблица из `test.csv` и выборка первых двух строк из неё:
 
-``` sql
+```sql
 SELECT *
 FROM file('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
 LIMIT 2
@@ -45,5 +45,3 @@ LIMIT 2
 │       3 │       2 │       1 │
 └─────────┴─────────┴─────────┘
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/file/) <!--hide-->
diff --git a/docs/ru/query_language/table_functions/index.md b/docs/ru/query_language/table_functions/index.md
index 704c9fa71..6649ae397 100644
--- a/docs/ru/query_language/table_functions/index.md
+++ b/docs/ru/query_language/table_functions/index.md
@@ -3,5 +3,3 @@
 Табличные функции могут указываться в секции FROM вместо имени БД и таблицы.
 Табличные функции можно использовать только если не выставлена настройка readonly.
 Табличные функции не имеют отношения к другим функциям.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/) <!--hide-->
diff --git a/docs/ru/query_language/table_functions/jdbc.md b/docs/ru/query_language/table_functions/jdbc.md
index 77b1da780..8a7ca2c4a 100644
--- a/docs/ru/query_language/table_functions/jdbc.md
+++ b/docs/ru/query_language/table_functions/jdbc.md
@@ -5,21 +5,19 @@
 `jdbc(jdbc_connection_uri, schema, table)` - возвращает таблицу, соединение с которой происходит через JDBC-драйвер.
 
 Для работы этой табличной функциии требуется отдельно запускать приложение clickhouse-jdbc-bridge.
-Данная функция поддерживает Nullable типы (на основании DDL таблицы к которой происходит запрос).
+В отличии от табличной функции `odbc`, данная функция поддерживает Nullable типы (на основании DDL таблицы к которой происходит запрос).
 
 
 **Пример**
 
-``` sql
+```sql
 SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
 ```
 
-``` sql
+```sql
 SELECT * FROM jdbc('mysql://localhost:3306/?user=root&password=root', 'schema', 'table')
 ```
 
-``` sql
+```sql
 SELECT * FROM jdbc('datasource://mysql-local', 'schema', 'table')
-```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/jdbc/) <!--hide-->
+```
\ No newline at end of file
diff --git a/docs/ru/query_language/table_functions/merge.md b/docs/ru/query_language/table_functions/merge.md
index 04a3ba982..092c9243f 100644
--- a/docs/ru/query_language/table_functions/merge.md
+++ b/docs/ru/query_language/table_functions/merge.md
@@ -3,5 +3,3 @@
 `merge(db_name, 'tables_regexp')` - создаёт временную таблицу типа Merge. Подробнее смотрите раздел "Движки таблиц, Merge".
 
 Структура таблицы берётся из первой попавшейся таблицы, подходящей под регулярное выражение.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/merge/) <!--hide-->
diff --git a/docs/ru/query_language/table_functions/numbers.md b/docs/ru/query_language/table_functions/numbers.md
index a5e4ba58e..8ff5ae3ac 100644
--- a/docs/ru/query_language/table_functions/numbers.md
+++ b/docs/ru/query_language/table_functions/numbers.md
@@ -7,15 +7,13 @@
 
 Следующие запросы эквивалентны:
 
-``` sql
+```sql
 SELECT * FROM numbers(10);
 SELECT * FROM numbers(0,10);
 SELECT * FROM system.numbers LIMIT 10;
 ```
 Примеры:
-``` sql
+```sql
 -- генерация последовательности всех дат от 2010-01-01 до 2010-12-31
 select toDate('2010-01-01') + number as d FROM numbers(365);
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/numbers/) <!--hide-->
diff --git a/docs/ru/query_language/table_functions/remote.md b/docs/ru/query_language/table_functions/remote.md
index acd623a58..a2366607f 100644
--- a/docs/ru/query_language/table_functions/remote.md
+++ b/docs/ru/query_language/table_functions/remote.md
@@ -6,7 +6,7 @@
 
 Сигнатуры:
 
-``` sql
+```sql
 remote('addresses_expr', db, table[, 'user'[, 'password']])
 remote('addresses_expr', db.table[, 'user'[, 'password']])
 ```
@@ -18,7 +18,7 @@ remote('addresses_expr', db.table[, 'user'[, 'password']])
 
 Примеры:
 
-```
+```text
 example01-01-1
 example01-01-1:9000
 localhost
@@ -31,19 +31,19 @@ localhost
 
 Пример:
 
-```
+```text
 example01-01-1,example01-02-1
 ```
 
 Часть выражения может быть указана в фигурных скобках. Предыдущий пример может быть записан следующим образом:
 
-```
+```text
 example01-0{1,2}-1
 ```
 
 В фигурных скобках может быть указан диапазон (неотрицательных целых) чисел через две точки. В этом случае, диапазон раскрывается в множество значений, генерирующих адреса шардов. Если запись первого числа начинается с нуля, то значения формируются с таким же выравниванием нулями. Предыдущий пример может быть записан следующим образом:
 
-```
+```text
 example01-{01..02}-1
 ```
 
@@ -53,7 +53,7 @@ example01-{01..02}-1
 
 Пример:
 
-```
+```text
 example01-{01..02}-{1|2}
 ```
 
@@ -72,5 +72,3 @@ example01-{01..02}-{1|2}
 
 Если пользователь не задан,то используется `default`.
 Если пароль не задан, то используется пустой пароль.
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/remote/) <!--hide-->
diff --git a/docs/ru/query_language/table_functions/url.md b/docs/ru/query_language/table_functions/url.md
index fccaf1fe1..7c5068b3c 100644
--- a/docs/ru/query_language/table_functions/url.md
+++ b/docs/ru/query_language/table_functions/url.md
@@ -14,9 +14,7 @@ structure - структура таблицы в форме `'UserID UInt64, Nam
 
 **Пример**
 
-``` sql
+```sql
 -- получение 3-х строк таблицы, состоящей из двух колонк типа String и UInt32 от сервера, отдающего данные в формате CSV
 SELECT * FROM url('http://127.0.0.1:12345/', CSV, 'column1 String, column2 UInt32') LIMIT 3
 ```
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/query_language/table_functions/url/) <!--hide-->
diff --git a/docs/ru/roadmap.md b/docs/ru/roadmap.md
index 5418cf8a8..afd492c4b 100644
--- a/docs/ru/roadmap.md
+++ b/docs/ru/roadmap.md
@@ -1,14 +1,20 @@
 # Roadmap
 
+## Q3 2018
+
+- `ALTER UPDATE` для массового изменения данных с использованием подхода, аналогичного `ALTER DELETE`
+- Добавление Protobuf и Parquet к ассортименту поддерживаемых форматов ввода-вывода
+- Улучшением совместимости с Tableau и другими инструментами бизнес-аналитики
+
 ## Q4 2018
 
 - Соответствующий SQL стандарту синтаксис JOIN:
     - Несколько `JOIN`ов в одном `SELECT`
+    - Указание связи между таблицами через `ON`
+    - Возможность сослаться на имя таблицы вместо обязательного использования подзапроса
 
 - Улучшения в исполнении JOIN:
     - Распределённый JOIN, не ограниченный оперативной памятью
+    - Перенос зависящих только от одной стороны предикатов сквозь JOIN
 
-- Добавление Protobuf и Parquet к ассортименту поддерживаемых форматов ввода-вывода
 - Пулы ресурсов для более точного распределения мощностей кластера между его пользователями
-
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/roadmap/) <!--hide-->
diff --git a/docs/ru/security_changelog.md b/docs/ru/security_changelog.md
index 7bf286a4e..de3e3b2cb 100644
--- a/docs/ru/security_changelog.md
+++ b/docs/ru/security_changelog.md
@@ -18,5 +18,4 @@
 
 Некоррректная конфигурация в deb пакете могла привести к неавторизованному доступу к базе данных.
 
-Обнаружено благодаря: the UK's National Cyber Security Centre (NCSC)
-[Оригинальная статья](https://clickhouse.yandex/docs/ru/security_changelog/) <!--hide-->
+Обнаружено благодаря: the UK's National Cyber Security Centre (NCSC)
\ No newline at end of file
diff --git a/docs/toc_en.yml b/docs/toc_en.yml
index e6e63095d..8b191b165 100644
--- a/docs/toc_en.yml
+++ b/docs/toc_en.yml
@@ -100,7 +100,6 @@ nav:
     - 'numbers': 'query_language/table_functions/numbers.md'
     - 'remote': 'query_language/table_functions/remote.md'
     - 'url': 'query_language/table_functions/url.md'
-    - 'jdbc': 'query_language/table_functions/jdbc.md'
   - 'Dictionaries':
     - 'Introduction': 'query_language/dicts/index.md'
     - 'External dictionaries':
diff --git a/docs/toc_fa.yml b/docs/toc_fa.yml
index 780d97479..9970325f4 100644
--- a/docs/toc_fa.yml
+++ b/docs/toc_fa.yml
@@ -97,7 +97,6 @@ nav:
     - 'numbers': 'query_language/table_functions/numbers.md'
     - 'remote': 'query_language/table_functions/remote.md'
     - 'url': 'query_language/table_functions/url.md'
-    - 'jdbc': 'query_language/table_functions/jdbc.md'
   - 'Dictionaries':
     - 'Introduction': 'query_language/dicts/index.md'
     - 'External dictionaries':
diff --git a/docs/toc_ru.yml b/docs/toc_ru.yml
index 9c5ee2db7..8272e1b11 100644
--- a/docs/toc_ru.yml
+++ b/docs/toc_ru.yml
@@ -101,7 +101,6 @@ nav:
     - 'numbers': 'query_language/table_functions/numbers.md'
     - 'remote': 'query_language/table_functions/remote.md'
     - 'url': 'query_language/table_functions/url.md'
-    - 'jdbc': 'query_language/table_functions/jdbc.md'
   - 'Словари':
     - 'Введение': 'query_language/dicts/index.md'
     - 'Внешние словари':
diff --git a/docs/toc_zh.yml b/docs/toc_zh.yml
index 7a3bf5de7..8b191b165 100644
--- a/docs/toc_zh.yml
+++ b/docs/toc_zh.yml
@@ -1,15 +1,15 @@
 nav:
 
-- '介绍':
-  - '概貌': 'index.md'
-  - 'ClickHouse的独特功能': 'introduction/distinctive_features.md'
-  - 'ClickHouse功能可被视为缺点': 'introduction/features_considered_disadvantages.md'
-  - '性能': 'introduction/performance.md'
-  - 'Yandex.Metrica使用案例': 'introduction/ya_metrika_task.md'
+- 'Introduction':
+  - 'Overview': 'index.md'
+  - 'Distinctive features of ClickHouse': 'introduction/distinctive_features.md'
+  - 'ClickHouse features that can be considered disadvantages': 'introduction/features_considered_disadvantages.md'
+  - 'Performance': 'introduction/performance.md'
+  - 'The Yandex.Metrica task': 'introduction/ya_metrika_task.md'
 
-- '起步':
-  - '部署运行': 'getting_started/index.md'
-  - '示例数据集':
+- 'Getting started':
+  - 'Deploying and running': 'getting_started/index.md'
+  - 'Example datasets':
     - 'OnTime': 'getting_started/example_datasets/ontime.md'
     - 'New York Taxi data': 'getting_started/example_datasets/nyc_taxi.md'
     - 'AMPLab Big Data Benchmark': 'getting_started/example_datasets/amplab_benchmark.md'
@@ -17,18 +17,18 @@ nav:
     - 'Terabyte click logs from Criteo': 'getting_started/example_datasets/criteo.md'
     - 'Star Schema Benchmark': 'getting_started/example_datasets/star_schema.md'
 
-- '客户端':
-  - '介绍': 'interfaces/index.md'
-  - '命令行客户端接口': 'interfaces/cli.md'
-  - 'HTTP 客户端接口': 'interfaces/http_interface.md'
-  - 'JDBC 驱动': 'interfaces/jdbc.md'
-  - '原生客户端接口 (TCP)': 'interfaces/tcp.md'
-  - '第三方开发的库': 'interfaces/third-party_client_libraries.md'
-  - '第三方开发的可视化界面': 'interfaces/third-party_gui.md'
-  - '输入输出格式': 'interfaces/formats.md'
+- 'Interfaces':
+  - 'Introduction': 'interfaces/index.md'
+  - 'Command-line client': 'interfaces/cli.md'
+  - 'HTTP interface': 'interfaces/http_interface.md'
+  - 'JDBC driver': 'interfaces/jdbc.md'
+  - 'Native interface (TCP)': 'interfaces/tcp.md'
+  - 'Libraries from third-party developers': 'interfaces/third-party_client_libraries.md'
+  - 'Visual interfaces from third-party developers': 'interfaces/third-party_gui.md'
+  - 'Input and output formats': 'interfaces/formats.md'
 
-- '数据类型':
-  - '介绍': 'data_types/index.md'
+- 'Data types':
+  - 'Introduction': 'data_types/index.md'
   - 'UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64': 'data_types/int_uint.md'
   - 'Float32, Float64': 'data_types/float.md'
   - 'Decimal': 'data_types/decimal.md'
@@ -51,7 +51,7 @@ nav:
     - 'Set': 'data_types/special_data_types/set.md'
     - 'Nothing': 'data_types/special_data_types/nothing.md'
 
-- 'SQL语法':
+- 'SQL reference':
   - 'hidden': 'query_language/index.md'
   - 'SELECT': 'query_language/select.md'
   - 'INSERT INTO': 'query_language/insert_into.md'
@@ -100,7 +100,6 @@ nav:
     - 'numbers': 'query_language/table_functions/numbers.md'
     - 'remote': 'query_language/table_functions/remote.md'
     - 'url': 'query_language/table_functions/url.md'
-    - 'jdbc': 'query_language/table_functions/jdbc.md'
   - 'Dictionaries':
     - 'Introduction': 'query_language/dicts/index.md'
     - 'External dictionaries':
@@ -114,7 +113,7 @@ nav:
   - 'Operators': 'query_language/operators.md'
   - 'General syntax': 'query_language/syntax.md'
 
-- '运维':
+- 'Operations':
   - 'hidden': 'operations/index.md'
   - 'Table engines':
     - 'Introduction': 'operations/table_engines/index.md'
@@ -166,10 +165,10 @@ nav:
     - 'clickhouse-copier': 'operations/utils/clickhouse-copier.md'
     - 'clickhouse-local': 'operations/utils/clickhouse-local.md'
 
-- '常见问题':
-  - '一般的问题': 'faq/general.md'
+- 'F.A.Q.':
+  - 'General questions': 'faq/general.md'
 
-- '开发者指南':
+- 'Development':
   - 'hidden': 'development/index.md'
   - 'Overview of ClickHouse architecture': 'development/architecture.md'
   - 'How to build ClickHouse on Linux': 'development/build.md'
@@ -177,7 +176,7 @@ nav:
   - 'How to write  C++ code': 'development/style.md'
   - 'How to run ClickHouse tests': 'development/tests.md'
 
-- '新功能特性':
-  - '路线图': 'roadmap.md'
-  - '更新日志': 'changelog.md'
-  - '安全更改日志': 'security_changelog.md'
+- 'What''s new':
+  - 'Roadmap': 'roadmap.md'
+  - 'Changelog': 'changelog.md'
+  - 'Security changelog': 'security_changelog.md'
diff --git a/docs/tools/README.md b/docs/tools/README.md
index a0b853c03..1017aa93e 100644
--- a/docs/tools/README.md
+++ b/docs/tools/README.md
@@ -16,7 +16,7 @@ Usually those have some way to preview how Markdown will look like, which allows
 
 It'll take some effort to go through, but the result will be very close to production documentation.
 
-For the first time you'll need to install [wkhtmltopdf](https://wkhtmltopdf.org/) and set up virtualenv:
+For the first time you'll need to set up virtualenv:
 
 ``` bash
 $ cd ClickHouse/docs/tools
diff --git a/docs/tools/build.py b/docs/tools/build.py
index 96a2a74ea..f3b05be97 100755
--- a/docs/tools/build.py
+++ b/docs/tools/build.py
@@ -8,13 +8,8 @@ import datetime
 import logging
 import os
 import shutil
-import subprocess
 import sys
 import tempfile
-import time
-
-import markdown.extensions
-import markdown.util
 
 from mkdocs import config
 from mkdocs import exceptions
@@ -39,17 +34,6 @@ def autoremoved_file(path):
     finally:
         os.unlink(path)
 
-class ClickHouseMarkdown(markdown.extensions.Extension):
-    class ClickHousePreprocessor(markdown.util.Processor):
-        def run(self, lines):
-            for line in lines:
-                if '<!--hide-->' not in line:
-                    yield line
-
-    def extendMarkdown(self, md):
-        md.preprocessors.register(self.ClickHousePreprocessor(), 'clickhouse_preprocessor', 31)
-
-markdown.extensions.ClickHouseMarkdown = ClickHouseMarkdown
 
 def build_for_lang(lang, args):
     logging.info('Building %s docs' % lang)
@@ -77,8 +61,10 @@ def build_for_lang(lang, args):
             'static_templates': ['404.html'],
             'extra': {
                 'single_page': False,
-                'now': int(time.mktime(datetime.datetime.now().timetuple())) # TODO better way to avoid caching
+                'opposite_lang': 'ru' if lang == 'en' else 'en',
+                'now': datetime.datetime.now() # TODO better way to avoid caching
             }
+
         }
 
         site_names = {
@@ -103,7 +89,6 @@ def build_for_lang(lang, args):
             edit_uri='edit/master/docs/%s' % lang,
             extra_css=['assets/stylesheets/custom.css'],
             markdown_extensions=[
-                'clickhouse',
                 'admonition',
                 'attr_list',
                 'codehilite',
@@ -149,7 +134,8 @@ def build_single_page_version(lang, args, cfg):
                     'docs_dir': docs_temp_lang,
                     'site_dir': site_temp,
                     'extra': {
-                        'single_page': True
+                        'single_page': True,
+                        'opposite_lang': 'en' if lang == 'ru' else 'ru'
                     },
                     'nav': [
                         {cfg.data.get('site_name'): 'single.md'}
@@ -168,12 +154,6 @@ def build_single_page_version(lang, args, cfg):
                     single_page_output_path
                 )
 
-                single_page_index_html = os.path.abspath(os.path.join(single_page_output_path, 'index.html'))
-                single_page_pdf = single_page_index_html.replace('index.html', 'clickhouse_%s.pdf' % lang)
-                create_pdf_command = ['wkhtmltopdf', '--print-media-type', single_page_index_html, single_page_pdf]
-                logging.debug(' '.join(create_pdf_command))
-                subprocess.check_call(' '.join(create_pdf_command), shell=True)
-
 
 def build_redirects(args):
     lang_re_fragment = args.lang.replace(',', '|')
diff --git a/docs/tools/mdx_clickhouse.py b/docs/tools/mdx_clickhouse.py
deleted file mode 100755
index c38d6ddcf..000000000
--- a/docs/tools/mdx_clickhouse.py
+++ /dev/null
@@ -1,22 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-from __future__ import unicode_literals
-
-import markdown.extensions
-import markdown.util
-
-
-class ClickHousePreprocessor(markdown.util.Processor):
-    def run(self, lines):
-        for line in lines:
-            if '<!--hide-->' not in line:
-                yield line
-
-class ClickHouseMarkdown(markdown.extensions.Extension):
-
-    def extendMarkdown(self, md, md_globals):
-        md.preprocessors['clickhouse'] = ClickHousePreprocessor()
-
-
-def makeExtension(**kwargs):
-    return ClickHouseMarkdown(**kwargs)
diff --git a/docs/tools/mkdocs-material-theme/assets/flags/en.svg b/docs/tools/mkdocs-material-theme/assets/flags/en.svg
deleted file mode 100644
index 7cabb5e90..000000000
--- a/docs/tools/mkdocs-material-theme/assets/flags/en.svg
+++ /dev/null
@@ -1,10 +0,0 @@
-<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 60 30" preserveAspectRatio="none">
-<clipPath id="t">
-<path d="M30,15 h30 v15 z v15 h-30 z h-30 v-15 z v-15 h30 z"/>
-</clipPath>
-<path d="M0,0 v30 h60 v-30 z" fill="#00247d"/>
-<path d="M0,0 L60,30 M60,0 L0,30" stroke="#fff" stroke-width="6"/>
-<path d="M0,0 L60,30 M60,0 L0,30" clip-path="url(#t)" stroke="#cf142b" stroke-width="4"/>
-<path d="M30,0 v30 M0,15 h60" stroke="#fff" stroke-width="10"/>
-<path d="M30,0 v30 M0,15 h60" stroke="#cf142b" stroke-width="6"/>
-</svg>
diff --git a/docs/tools/mkdocs-material-theme/assets/flags/fa.svg b/docs/tools/mkdocs-material-theme/assets/flags/fa.svg
deleted file mode 100644
index de467f133..000000000
--- a/docs/tools/mkdocs-material-theme/assets/flags/fa.svg
+++ /dev/null
@@ -1,31 +0,0 @@
-<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 630 360" preserveAspectRatio="none">
-<rect width="630" height="360" fill="#da0000"/>
-<rect width="630" height="240" fill="#fff"/>
-<rect width="630" height="120" fill="#239f40"/>
-<g transform="translate(8.4,100.4)">
-<g id="tb4">
-<g id="tb1" fill="none" stroke="#fff" stroke-width="2">
-<path id="tbp1" d="M0,1H26M1,10V5H9V9H17V5H12M4,9H6M26,9H21V5H29M29,0V9H37V0M33,0V9" transform="scale(1.4)"/>
-<path id="tbp2" d="M0,7H9M10,7H19" transform="scale(2.8)"/>
-<use xlink:href="#tbp2" y="120"/>
-<use xlink:href="#tbp1" y="145.2"/>
-</g>
-<g id="tb3">
-<use xlink:href="#tb1" x="56"/>
-<use xlink:href="#tb1" x="112"/>
-<use xlink:href="#tb1" x="168"/>
-</g>
-</g>
-<use xlink:href="#tb3" x="168"/>
-<use xlink:href="#tb4" x="392"/>
-</g>
-<g fill="#da0000" transform="matrix(45,0,0,45,315,180)">
-<g id="emblem_half">
-<path d="M-0.54815,0.83638A0.912046,0.912046 0 0,0 0.328544,-0.722384A1,1 0 0,1 -0.54815,0.83638"/>
-<path d="M0.618339,0.661409A0.763932,0.763932 0 0,0 0.421644,-0.741049A1,1 0 0,1 0.618339,0.661409"/>
-<path d="M0,1 -0.05,0 0,-0.787278A0.309995,0.309995 0 0,0 0.118034,-0.688191V-0.100406L0.077809,0.892905z"/>
-<path d="M-0.02,-0.85 0,-0.831217A0.14431,0.14431 0 0,0 0.252075,-0.967708A0.136408,0.136408 0 0,1 0,-0.924634"/>
-</g>
-<use xlink:href="#emblem_half" transform="scale(-1,1)"/>
-</g>
-</svg>
diff --git a/docs/tools/mkdocs-material-theme/assets/flags/ru.svg b/docs/tools/mkdocs-material-theme/assets/flags/ru.svg
deleted file mode 100644
index 76e92a101..000000000
--- a/docs/tools/mkdocs-material-theme/assets/flags/ru.svg
+++ /dev/null
@@ -1,5 +0,0 @@
-<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 9 6" preserveAspectRatio="none">
-<rect fill="#fff" width="9" height="3"/>
-<rect fill="#d52b1e" y="3" width="9" height="3"/>
-<rect fill="#0039a6" y="2" width="9" height="2"/>
-</svg>
diff --git a/docs/tools/mkdocs-material-theme/assets/flags/zh.svg b/docs/tools/mkdocs-material-theme/assets/flags/zh.svg
deleted file mode 100644
index 7681b3504..000000000
--- a/docs/tools/mkdocs-material-theme/assets/flags/zh.svg
+++ /dev/null
@@ -1,11 +0,0 @@
-<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 30 20" preserveAspectRatio="none">
-<defs>
-<path id="s" d="M0,-1 0.587785,0.809017 -0.951057,-0.309017H0.951057L-0.587785,0.809017z" fill="#ffde00"/>
-</defs>
-<rect width="30" height="20" fill="#de2910"/>
-<use xlink:href="#s" transform="translate(5,5) scale(3)"/>
-<use xlink:href="#s" transform="translate(10,2) rotate(23.036243)"/>
-<use xlink:href="#s" transform="translate(12,4) rotate(45.869898)"/>
-<use xlink:href="#s" transform="translate(12,7) rotate(69.945396)"/>
-<use xlink:href="#s" transform="translate(10,9) rotate(20.659808)"/>
-</svg>
diff --git a/docs/tools/mkdocs-material-theme/assets/javascripts/lunr/lunr.fa.js b/docs/tools/mkdocs-material-theme/assets/javascripts/lunr/lunr.fa.js
deleted file mode 100644
index e69de29bb..000000000
diff --git a/docs/tools/mkdocs-material-theme/assets/javascripts/lunr/lunr.zh.js b/docs/tools/mkdocs-material-theme/assets/javascripts/lunr/lunr.zh.js
deleted file mode 100644
index 1125b3efa..000000000
--- a/docs/tools/mkdocs-material-theme/assets/javascripts/lunr/lunr.zh.js
+++ /dev/null
@@ -1,146 +0,0 @@
-i/**
- * lunr对中文分词的支持
- */
-;
-(function(root, factory) {
-  if (typeof define === 'function' && define.amd) {
-    // AMD. Register as an anonymous module.
-    define(factory)
-  } else if (typeof exports === 'object') {
-    /**
-     * Node. Does not work with strict CommonJS, but
-     * only CommonJS-like environments that support module.exports,
-     * like Node.
-     */
-    module.exports = factory()
-  } else {
-    // Browser globals (root is window)
-    factory()(root.lunr);
-  }
-}(this, function() {
-  /**
-   * Just return a value to define the module export.
-   * This example returns an object, but the module
-   * can return a function as the exported value.
-   */
-  return function(lunr) { 
-    
-    /*
-    Thai tokenization is the same to Japanense, which does not take into account spaces.
-    So, it uses the same logic to assign tokenization function due to different Lunr versions.
-    */
-    var isLunr2 = lunr.version[0] == "2";
-
-    /* register specific locale function */
-    lunr.zhcn = function() {
-        this.pipeline.reset();
-        this.pipeline.add(
-            lunr.zhcn.trimmer,
-            lunr.zhcn.stopWordFilter,
-            lunr.zhcn.stemmer
-        );
-
-        if (isLunr2) { // for lunr version 2.0.0
-            this.tokenizer = lunr.zhcn.tokenizer;
-        } else {
-            if (lunr.tokenizer) { // for lunr version 0.6.0
-                lunr.tokenizer = lunr.zhcn.tokenizer;
-            }
-            if (this.tokenizerFn) { // for lunr version 0.7.0 -> 1.0.0
-                this.tokenizerFn = lunr.zhcn.tokenizer;
-            }
-        }
-    };
-
-
-    var segmenter = new lunr.TinySegmenter(); 
-
-    lunr.zhcn.tokenizer = function(obj) {
-      var i;
-      var str;
-      var len;
-      var segs;
-      var tokens;
-      var char;
-      var sliceLength;
-      var sliceStart;
-      var sliceEnd;
-      var segStart;
-
-      if (!arguments.length || obj == null || obj == undefined)
-        return [];
-
-      if (Array.isArray(obj)) {
-        return obj.map(
-          function(t) {
-            return isLunr2 ? new lunr.Token(t.toLowerCase()) : t.toLowerCase();
-          }
-        );
-      }
-
-      str = obj.toString().toLowerCase().replace(/^\s+/, '');
-      for (i = str.length - 1; i >= 0; i--) {
-        if (/\S/.test(str.charAt(i))) {
-          str = str.substring(0, i + 1);
-          break;
-        }
-      }
-
-      tokens = [];
-      len = str.length;
-      for (sliceEnd = 0, sliceStart = 0; sliceEnd <= len; sliceEnd++) {
-        char = str.charAt(sliceEnd);
-        sliceLength = sliceEnd - sliceStart;
-
-        if ((char.match(/\s/) || sliceEnd == len)) {
-          if (sliceLength > 0) {
-            segs = segmenter.segment(str.slice(sliceStart, sliceEnd)).filter(
-              function(token) {
-                return !!token;
-              }
-            );
-
-            segStart = sliceStart;
-            for (i = 0; i < segs.length; i++) {
-              if (isLunr2) {
-                tokens.push(
-                  new lunr.Token(
-                    segs[i], {
-                      position: [segStart, segs[i].length],
-                      index: tokens.length
-                    }
-                  )
-                );
-              } else {
-                tokens.push(segs[i]);
-              }
-              segStart += segs[i].length;
-            }
-          }
-
-          sliceStart = sliceEnd + 1;
-        }
-      }
-
-      return tokens;
-    }
-
-    lunr.zhcn.stemmer = (function(){
-      return function(word) {
-        return word;
-      }
-    })();
-    
-    lunr.Pipeline.registerFunction(lunr.zhcn.stemmer, 'stemmer-zhcn');
-
-    /* lunr trimmer function */
-    lunr.zhcn.wordCharacters = "一二三四五六七八九十百千万億兆一-龠々〆ヵヶぁ-んァ-ヴーｱ-ﾝﾞa-zA-Zａ-ｚＡ-Ｚ0-9０-９";
-    lunr.zhcn.trimmer = lunr.trimmerSupport.generateTrimmer(lunr.zhcn.wordCharacters);
-    lunr.Pipeline.registerFunction(lunr.zhcn.trimmer, 'trimmer-zhcn');
-
-
-    /* lunr stop word filter. see https://www.ranks.nl/stopwords/chinese-stopwords */
-    lunr.zhcn.stopWordFilter = lunr.generateStopWordFilter('的 一 不 在 人 有 是 为 以 于 上 他 而 后 之 来 及 了 因 下 可 到 由 这 与 也 此 但 并 个 其 已 无 小 我 们 起 最 再 今 去 好 只 又 或 很 亦 某 把 那 你 乃 它 吧 被 比 别 趁 当 从 到 得 打 凡 儿 尔 该 各 给 跟 和 何 还 即 几 既 看 据 距 靠 啦 了 另 么 每 们 嘛 拿 哪 那 您 凭 且 却 让 仍 啥 如 若 使 谁 虽 随 同 所 她 哇 嗡 往 哪 些 向 沿 哟 用 于 咱 则 怎 曾 至 致 着 诸 自'.split(' '));
-    lunr.Pipeline.registerFunction(lunr.zhcn.stopWordFilter, 'stopWordFilter-zhcn');
-  };
-}))
diff --git a/docs/tools/mkdocs-material-theme/assets/stylesheets/custom.css b/docs/tools/mkdocs-material-theme/assets/stylesheets/custom.css
index 002434555..2fcf11ce4 100644
--- a/docs/tools/mkdocs-material-theme/assets/stylesheets/custom.css
+++ b/docs/tools/mkdocs-material-theme/assets/stylesheets/custom.css
@@ -1,74 +1,70 @@
 @font-face {
-    font-family: 'Yandex Sans Text Web';
-    src: url(https://yastatic.net/adv-www/_/yy5JveR58JFkc97waf-xp0i6_jM.eot);
-    src: url(https://yastatic.net/adv-www/_/yy5JveR58JFkc97waf-xp0i6_jM.eot?#iefix) format('embedded-opentype'),
-    url(https://yastatic.net/adv-www/_/CYblzLEXzCqQIvrYs7QKQe2omRk.woff2) format('woff2'),
-    url(https://yastatic.net/adv-www/_/pUcnOdRwl83MvPPzrNomhyletnA.woff) format('woff'),
-    url(https://yastatic.net/adv-www/_/vNFEmXOcGYKJ4AAidUprHWoXrLU.ttf) format('truetype'),
-    url(https://yastatic.net/adv-www/_/0w7OcWZM_QLP8x-LQUXFOgXO6dE.svg#YandexSansTextWeb-Bold) format('svg');
-    font-weight: 700;
-    font-style: normal;
-    font-stretch: normal
+	font-family: 'Yandex Sans Text Web';
+	src: url(https://yastatic.net/adv-www/_/yy5JveR58JFkc97waf-xp0i6_jM.eot);
+	src: url(https://yastatic.net/adv-www/_/yy5JveR58JFkc97waf-xp0i6_jM.eot?#iefix) format('embedded-opentype'),
+	url(https://yastatic.net/adv-www/_/CYblzLEXzCqQIvrYs7QKQe2omRk.woff2) format('woff2'),
+	url(https://yastatic.net/adv-www/_/pUcnOdRwl83MvPPzrNomhyletnA.woff) format('woff'),
+	url(https://yastatic.net/adv-www/_/vNFEmXOcGYKJ4AAidUprHWoXrLU.ttf) format('truetype'),
+	url(https://yastatic.net/adv-www/_/0w7OcWZM_QLP8x-LQUXFOgXO6dE.svg#YandexSansTextWeb-Bold) format('svg');
+	font-weight: 700;
+	font-style: normal;
+	font-stretch: normal
 }
 
 @font-face {
-    font-family: 'Yandex Sans Text Web';
-    src: url(https://yastatic.net/adv-www/_/LI6l3L2RqcgxBe2pXmuUha37czQ.eot);
-    src: url(https://yastatic.net/adv-www/_/LI6l3L2RqcgxBe2pXmuUha37czQ.eot?#iefix) format('embedded-opentype'),
-    url(https://yastatic.net/adv-www/_/z3MYElcut0R2MF_Iw1RDNrstgYs.woff2) format('woff2'),
-    url(https://yastatic.net/adv-www/_/1jvKJ_-hCXl3s7gmFl-y_-UHTaI.woff) format('woff'),
-    url(https://yastatic.net/adv-www/_/9nzjfpCR2QHvK1EzHpDEIoVFGuY.ttf) format('truetype'),
-    url(https://yastatic.net/adv-www/_/gwyBTpxSwkFCF1looxqs6JokKls.svg#YandexSansTextWeb-Regular) format('svg');
-    font-weight: 400;
-    font-style: normal;
-    font-stretch: normal
+	font-family: 'Yandex Sans Text Web';
+	src: url(https://yastatic.net/adv-www/_/LI6l3L2RqcgxBe2pXmuUha37czQ.eot);
+	src: url(https://yastatic.net/adv-www/_/LI6l3L2RqcgxBe2pXmuUha37czQ.eot?#iefix) format('embedded-opentype'),
+	url(https://yastatic.net/adv-www/_/z3MYElcut0R2MF_Iw1RDNrstgYs.woff2) format('woff2'),
+	url(https://yastatic.net/adv-www/_/1jvKJ_-hCXl3s7gmFl-y_-UHTaI.woff) format('woff'),
+	url(https://yastatic.net/adv-www/_/9nzjfpCR2QHvK1EzHpDEIoVFGuY.ttf) format('truetype'),
+	url(https://yastatic.net/adv-www/_/gwyBTpxSwkFCF1looxqs6JokKls.svg#YandexSansTextWeb-Regular) format('svg');
+	font-weight: 400;
+	font-style: normal;
+	font-stretch: normal
 }
 
 @font-face {
-    font-family: 'Yandex Sans Text Web';
-    src: url(https://yastatic.net/adv-www/_/ayAFYoY8swgBLhq_I56tKj2JftU.eot);
-    src: url(https://yastatic.net/adv-www/_/ayAFYoY8swgBLhq_I56tKj2JftU.eot?#iefix) format('embedded-opentype'),
-    url(https://yastatic.net/adv-www/_/lGQcYklLVV0hyvz1HFmFsUTj8_0.woff2) format('woff2'),
-    url(https://yastatic.net/adv-www/_/f0AAJ9GJ4iiwEmhG-7PWMHk6vUY.woff) format('woff'),
-    url(https://yastatic.net/adv-www/_/4UDe4nlVvgEJ-VmLWNVq3SxCsA.ttf) format('truetype'),
-    url(https://yastatic.net/adv-www/_/EKLr1STNokPqxLAQa_RyN82pL98.svg#YandexSansTextWeb-Light) format('svg');
-    font-weight: 300;
-    font-style: normal;
-    font-stretch: normal
+	font-family: 'Yandex Sans Text Web';
+	src: url(https://yastatic.net/adv-www/_/ayAFYoY8swgBLhq_I56tKj2JftU.eot);
+	src: url(https://yastatic.net/adv-www/_/ayAFYoY8swgBLhq_I56tKj2JftU.eot?#iefix) format('embedded-opentype'),
+	url(https://yastatic.net/adv-www/_/lGQcYklLVV0hyvz1HFmFsUTj8_0.woff2) format('woff2'),
+	url(https://yastatic.net/adv-www/_/f0AAJ9GJ4iiwEmhG-7PWMHk6vUY.woff) format('woff'),
+	url(https://yastatic.net/adv-www/_/4UDe4nlVvgEJ-VmLWNVq3SxCsA.ttf) format('truetype'),
+	url(https://yastatic.net/adv-www/_/EKLr1STNokPqxLAQa_RyN82pL98.svg#YandexSansTextWeb-Light) format('svg');
+	font-weight: 300;
+	font-style: normal;
+	font-stretch: normal
 }
 
 @font-face {
-    font-family: 'Yandex Sans Display Web';
-    src: url(https://yastatic.net/adv-www/_/H63jN0veW07XQUIA2317lr9UIm8.eot);
-    src: url(https://yastatic.net/adv-www/_/H63jN0veW07XQUIA2317lr9UIm8.eot?#iefix) format('embedded-opentype'),
-    url(https://yastatic.net/adv-www/_/sUYVCPUAQE7ExrvMS7FoISoO83s.woff2) format('woff2'),
-    url(https://yastatic.net/adv-www/_/v2Sve_obH3rKm6rKrtSQpf-eB7U.woff) format('woff'),
-    url(https://yastatic.net/adv-www/_/PzD8hWLMunow5i3RfJ6WQJAL7aI.ttf) format('truetype'),
-    url(https://yastatic.net/adv-www/_/lF_KG5g4tpQNlYIgA0e77fBSZ5s.svg#YandexSansDisplayWeb-Regular) format('svg');
-    font-weight: 400;
-    font-style: normal;
-    font-stretch: normal
+	font-family: 'Yandex Sans Display Web';
+	src: url(https://yastatic.net/adv-www/_/H63jN0veW07XQUIA2317lr9UIm8.eot);
+	src: url(https://yastatic.net/adv-www/_/H63jN0veW07XQUIA2317lr9UIm8.eot?#iefix) format('embedded-opentype'),
+	url(https://yastatic.net/adv-www/_/sUYVCPUAQE7ExrvMS7FoISoO83s.woff2) format('woff2'),
+	url(https://yastatic.net/adv-www/_/v2Sve_obH3rKm6rKrtSQpf-eB7U.woff) format('woff'),
+	url(https://yastatic.net/adv-www/_/PzD8hWLMunow5i3RfJ6WQJAL7aI.ttf) format('truetype'),
+	url(https://yastatic.net/adv-www/_/lF_KG5g4tpQNlYIgA0e77fBSZ5s.svg#YandexSansDisplayWeb-Regular) format('svg');
+	font-weight: 400;
+	font-style: normal;
+	font-stretch: normal
 }
 
 @font-face {
-    font-family: 'Yandex Sans Display Web';
-    src: url(https://yastatic.net/adv-www/_/g8_MyyKVquSZ3xEL6tarK__V9Vw.eot);
-    src: url(https://yastatic.net/adv-www/_/g8_MyyKVquSZ3xEL6tarK__V9Vw.eot?#iefix) format('embedded-opentype'),
-    url(https://yastatic.net/adv-www/_/LGiRvlfqQHlWR9YKLhsw5e7KGNA.woff2) format('woff2'),
-    url(https://yastatic.net/adv-www/_/40vXwNl4eYYMgteIVgLP49dwmfc.woff) format('woff'),
-    url(https://yastatic.net/adv-www/_/X6zG5x_wO8-AtwJ-vDLJcKC5228.ttf) format('truetype'),
-    url(https://yastatic.net/adv-www/_/ZKhaR0m08c8CRRL77GtFKoHcLYA.svg#YandexSansDisplayWeb-Light) format('svg');
-    font-weight: 300;
-    font-style: normal;
-    font-stretch: normal
+	font-family: 'Yandex Sans Display Web';
+	src: url(https://yastatic.net/adv-www/_/g8_MyyKVquSZ3xEL6tarK__V9Vw.eot);
+	src: url(https://yastatic.net/adv-www/_/g8_MyyKVquSZ3xEL6tarK__V9Vw.eot?#iefix) format('embedded-opentype'),
+	url(https://yastatic.net/adv-www/_/LGiRvlfqQHlWR9YKLhsw5e7KGNA.woff2) format('woff2'),
+	url(https://yastatic.net/adv-www/_/40vXwNl4eYYMgteIVgLP49dwmfc.woff) format('woff'),
+	url(https://yastatic.net/adv-www/_/X6zG5x_wO8-AtwJ-vDLJcKC5228.ttf) format('truetype'),
+	url(https://yastatic.net/adv-www/_/ZKhaR0m08c8CRRL77GtFKoHcLYA.svg#YandexSansDisplayWeb-Light) format('svg');
+	font-weight: 300;
+	font-style: normal;
+	font-stretch: normal
 }
 
 body {
-    font: 400 14pt/200% 'Yandex Sans Text Web', Arial, sans-serif;
-}
-
-body.md-lang-zh {
-    font: 400 14pt/200% 'Yandex Sans Text Web', -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
+	font: 300 14pt/200% 'Yandex Sans Text Web', Arial, sans-serif;
 }
 
 a:link, a:visited {
@@ -77,7 +73,7 @@ a:link, a:visited {
 }
 
 .md-nav__link {
-    color: #000 !important;
+	color: #000 !important;
 }
 
 .md-nav__link[data-md-state='blur'] {
@@ -104,11 +100,11 @@ a:hover, a:active {
 
 h1, h2, h3, .md-logo {
     font-family: 'Yandex Sans Display Web', Arial, sans-serif;
-    color: #000 !important;
+	color: #000 !important;
 }
 
 .md-logo {
-    padding: 0;
+	padding: 0;
 }
 
 .md-header {
@@ -134,34 +130,10 @@ h1, h2, h3, .md-logo {
 }
 
 #md-extra-nav {
-    background: #efefef;
-    padding: 0.5rem 0;
+	background: #efefef;
+	padding-top: 0.5rem;
 }
 
 .grey {
-    color: #666;
-}
-
-.md-alt-lang:hover {
-    text-decoration: none;
-}
-
-.md-alt-lang>svg {
-    width: 36px;
-    height: 24px;
-    margin: 1.3rem 0;
-    filter: brightness(96%) grayscale(90%);
-}
-
-.md-alt-lang:hover>svg {
-    filter: brightness(96%) grayscale(5%);
-}
-
-.md-current-lang>svg {
-    filter: brightness(96%) grayscale(0%) !important;
-}
-@media only screen and (min-width: 60em) {
-    #md-sidebar-flags {
-        display: none;
-    }
+	color: #666;
 }
diff --git a/docs/tools/mkdocs-material-theme/base.html b/docs/tools/mkdocs-material-theme/base.html
index b3d3ae2d0..53e855a19 100644
--- a/docs/tools/mkdocs-material-theme/base.html
+++ b/docs/tools/mkdocs-material-theme/base.html
@@ -63,7 +63,7 @@
       <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
     {% endblock %}
     {% for path in extra_css %}
-      <link rel="stylesheet" href="{{ path }}?{{ config.theme.extra.now }}">
+      <link rel="stylesheet" href="{{ path }}">
     {% endfor %}
     {% block extrahead %}{% endblock %}
   </head>
@@ -71,9 +71,9 @@
   {% if palette.primary or palette.accent %}
     {% set primary = palette.primary | replace(" ", "-") | lower %}
     {% set accent  = palette.accent  | replace(" ", "-") | lower %}
-    <body dir="{{ direction }}" data-md-color-primary="{{ primary }}" data-md-color-accent="{{ accent }}" class="md-lang-{{ config.theme.language }}">
+    <body dir="{{ direction }}" data-md-color-primary="{{ primary }}" data-md-color-accent="{{ accent }}">
   {% else %}
-    <body dir="{{ direction }}" class="md-lang-{{ config.theme.language }}">
+    <body dir="{{ direction }}">
   {% endif %}
     <svg class="md-svg">
       <defs>
@@ -231,20 +231,6 @@
                   base: "{{ base_url }}"
               }
           });
-          var beforePrint = function() {
-            var details = document.getElementsByTagName("details");
-            for (var i = 0; i < details.length; ++i) {
-                details[i].open = 1;
-            }
-          };
-          if (window.matchMedia) {
-              window.matchMedia('print').addListener(function(q) {
-                  if (q.matches) {
-                      beforePrint();
-                  }
-              });
-          }
-          window.onbeforeprint = beforePrint;
       });
     </script>
     <!-- Yandex.Metrika counter -->
diff --git a/docs/tools/mkdocs-material-theme/partials/flags.html b/docs/tools/mkdocs-material-theme/partials/flags.html
deleted file mode 100644
index ae0200b60..000000000
--- a/docs/tools/mkdocs-material-theme/partials/flags.html
+++ /dev/null
@@ -1,9 +0,0 @@
-{% set alt_langs = [['en', 'English'], ['ru', 'Russian'], ['zh', 'Chinese'], ['fa', 'Farsi']] %}
-{% for alt_lang, alt_title in alt_langs %}
-  <a class="md-alt-lang{% if alt_lang == config.theme.language %} md-current-lang{% endif %}"
-     href="{{ base_url }}/../{{ alt_lang }}/{{ page.url or '' }}" class="md-flex__ellipsis md-header-nav__title"
-     title="{{ alt_title }}">
-    {% include "assets/flags/" + alt_lang + ".svg" %}
-  </a>
-{% endfor %}
-
diff --git a/docs/tools/mkdocs-material-theme/partials/header.html b/docs/tools/mkdocs-material-theme/partials/header.html
index 5c70a96ac..7adbbeb8d 100644
--- a/docs/tools/mkdocs-material-theme/partials/header.html
+++ b/docs/tools/mkdocs-material-theme/partials/header.html
@@ -43,7 +43,24 @@
       {% if page %}
         <div class="md-flex__cell md-flex__cell--shrink">
           <div id="md-language-switch" class="md-header-nav__source">
-            {% include "partials/flags.html" %}
+            {% if config.theme.extra.opposite_lang == 'en' %}
+              <a href="{{ base_url }}/../{{ config.theme.extra.opposite_lang }}/{{ page.url or '' }}" title="Switch to English" class="md-flex__ellipsis md-header-nav__title">
+                <svg xmlns="http://www.w3.org/2000/svg" width="50" height="30" viewBox="0,0 25,15" style="border:1px solid #eee;margin: .8rem 0 0 -1.25rem;">
+                  <rect width="25" height="15" fill="#00247d"></rect>
+                  <path d="M 0,0 L 25,15 M 25,0 L 0,15" stroke="#fff" stroke-width="3"></path>
+                  <path d="M 12.5,0 V 15 M 0,7.5 H 25" stroke="#fff" stroke-width="5"></path>
+                  <path d="M 12.5,0 V 15 M 0,7.5 H 25" stroke="#cf142b" stroke-width="3"></path>
+                </svg>
+              </a>
+            {% else %}
+              <a href="{{ base_url }}/../{{ config.theme.extra.opposite_lang }}/{{ page.url or '' }}" title="Переключить на русский язык" class="md-flex__ellipsis md-header-nav__title">
+                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 6" width="50" height="30" style="border: 1px solid #eee;margin: .8rem 0 0 -1.25rem">
+                  <rect fill="#fff" width="10" height="3"></rect>
+                  <rect fill="#d52b1e" y="3" width="10" height="3"></rect>
+                  <rect fill="#0039a6" y="2" width="10" height="2"></rect>
+                </svg>
+              </a>
+            {% endif %}
           </div>
         </div>
       {% endif %}
diff --git a/docs/tools/mkdocs-material-theme/partials/nav.html b/docs/tools/mkdocs-material-theme/partials/nav.html
index 14f5d507e..66366e6ad 100644
--- a/docs/tools/mkdocs-material-theme/partials/nav.html
+++ b/docs/tools/mkdocs-material-theme/partials/nav.html
@@ -29,13 +29,6 @@
         {% endif %}
       {% endif %}
     </li>
-    <li class="md-nav__item md-nav__item--active">
-      {% if config.theme.language == 'ru' %}
-          <a href="{{ base_url }}/single/clickhouse_{{ config.theme.language }}.pdf" class="md-nav__link md-nav__link--active">PDF версия</a>
-      {% else %}
-          <a href="{{ base_url }}/single/clickhouse_{{ config.theme.language }}.pdf" class="md-nav__link md-nav__link--active">PDF version</a>
-      {% endif %}
-    </li>
     {% if config.repo_url %}
     <li class="md-nav__item md-nav__item--active">
       <a href="{{ config.repo_url }}" rel="external nofollow" target="_blank" class="md-nav__link">
@@ -47,10 +40,5 @@
       </a>
     </li>
     {% endif %}
-    <li class="md-nav__item md-nav__item--active">
-        <span id="md-sidebar-flags" class="md-nav__link">
-        {% include "partials/flags.html" %}
-        </span>
-    </li>
   </ul>
 </nav>
diff --git a/docs/zh/faq/general.md b/docs/zh/faq/general.md
index adc0b5791..a45670232 100644
--- a/docs/zh/faq/general.md
+++ b/docs/zh/faq/general.md
@@ -8,5 +8,3 @@
 
 大多数MapReduce系统允许您在集群上执行任意代码。但是，声明性查询语言更适合OLAP，以便快速运行实验。例如，Hadoop包含Hive和Pig，Cloudera Impala或Shark（过时）for Spark，以及Spark SQL、Presto和Apache Drill。与专业系统相比，运行此类任务时的性能非常不理想，所以将这些系统用作Web接口的后端服务是不现实的，因为延迟相对较高。
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/faq/general/) <!--hide-->
diff --git a/docs/zh/index.md b/docs/zh/index.md
index 6c16c93ea..3eb9756a6 100644
--- a/docs/zh/index.md
+++ b/docs/zh/index.md
@@ -78,8 +78,9 @@ ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)
 
 例如，查询“统计每个广告平台的记录数量”需要读取“广告平台ID”这一列，它在未压缩的情况下需要1个字节进行存储。如果大部分流量不是来自广告平台，那么这一列至少可以以十倍的压缩率被压缩。当采用快速压缩算法，它的解压速度最少在十亿字节(未压缩数据)每秒。换句话说，这个查询可以在单个服务器上以每秒大约几十亿行的速度进行处理。这实际上是当前实现的速度。
 
-<details markdown="1"><summary>示例</summary>
-```
+<details><summary>示例</summary>
+<p>
+<pre>
 $ clickhouse-client
 ClickHouse client version 0.0.52053.
 Connecting to localhost:9000.
@@ -121,7 +122,8 @@ LIMIT 20
 20 rows in set. Elapsed: 0.153 sec. Processed 1.00 billion rows, 4.00 GB (6.53 billion rows/s., 26.10 GB/s.)
 
 :)
-```
+</pre>
+</p>
 </details>
 
 ### CPU
@@ -137,5 +139,3 @@ LIMIT 20
 这是不应该在一个通用数据库中实现的，因为这在运行简单查询时是没有意义的。但是也有例外，例如，MemSQL使用代码生成来减少处理SQL查询的延迟(只是为了比较，分析型数据库通常需要优化的是吞吐而不是延迟)。
 
 请注意，为了提高CPU效率，查询语言必须是声明型的(SQL或MDX)， 或者至少一个向量(J，K)。 查询应该只包含隐式循环，允许进行优化。
-
-[来源文章](https://clickhouse.yandex/docs/zh/) <!--hide-->
diff --git a/docs/zh/interfaces/cli.md b/docs/zh/interfaces/cli.md
index c9a287a1e..134132bf8 100644
--- a/docs/zh/interfaces/cli.md
+++ b/docs/zh/interfaces/cli.md
@@ -111,5 +111,3 @@ cat file.csv | clickhouse-client --database=test --query="INSERT INTO test FORMA
 </config>
 ```
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/cli/) <!--hide-->
diff --git a/docs/zh/interfaces/formats.md b/docs/zh/interfaces/formats.md
deleted file mode 100644
index ec905713b..000000000
--- a/docs/zh/interfaces/formats.md
+++ /dev/null
@@ -1,625 +0,0 @@
-<a name="formats"></a>
-
-# 输入输出格式
-
-ClickHouse 可以接受多种数据格式，可以在 (`INSERT`) 以及 (`SELECT`) 请求中使用。
-
-下列表格列出了支持的数据格式以及在 (`INSERT`) 以及 (`SELECT`) 请求中使用它们的方式。
-
-| 格式 | INSERT | SELECT |
-| ------- | -------- | -------- |
-| [TabSeparated](#tabseparated) | ✔ | ✔ |
-| [TabSeparatedRaw](#tabseparatedraw) | ✗ | ✔ |
-| [TabSeparatedWithNames](#tabseparatedwithnames) | ✔ | ✔ |
-| [TabSeparatedWithNamesAndTypes](#tabseparatedwithnamesandtypes) | ✔ | ✔ |
-| [CSV](#csv) | ✔ | ✔ |
-| [CSVWithNames](#csvwithnames) | ✔ | ✔ |
-| [Values](#values) | ✔ | ✔ |
-| [Vertical](#vertical) | ✗ | ✔ |
-| [VerticalRaw](#verticalraw) | ✗ | ✔ |
-| [JSON](#json) | ✗ | ✔ |
-| [JSONCompact](#jsoncompact) | ✗ | ✔ |
-| [JSONEachRow](#jsoneachrow) | ✔ | ✔ |
-| [TSKV](#tskv) | ✔ | ✔ |
-| [Pretty](#pretty) | ✗ | ✔ |
-| [PrettyCompact](#prettycompact) | ✗ | ✔ |
-| [PrettyCompactMonoBlock](#prettycompactmonoblock) | ✗ | ✔ |
-| [PrettyNoEscapes](#prettynoescapes) | ✗ | ✔ |
-| [PrettySpace](#prettyspace) | ✗ | ✔ |
-| [RowBinary](#rowbinary) | ✔ | ✔ |
-| [Native](#native) | ✔ | ✔ |
-| [Null](#null) | ✗ | ✔ |
-| [XML](#xml) | ✗ | ✔ |
-| [CapnProto](#capnproto) | ✔ | ✔ |
-
-<a name="tabseparated"></a>
-
-## TabSeparated
-
-在 TabSeparated 格式中，数据按行写入。每行包含由制表符分隔的值。除了行中的最后一个值（后面紧跟换行符）之外，每个值都跟随一个制表符。 在任何地方都可以使用严格的 Unix 命令行。最后一行还必须在最后包含换行符。值以文本格式编写，不包含引号，并且要转义特殊字符。
-
-这种格式也可以用 `TSV` 来表示。
-
-TabSeparated 格式非常方便用于自定义程序或脚本处理数据。HTTP 客户端接口默认会用这种格式，命令行客户端批量模式下也会用这种格式。这种格式允许在不同数据库之间传输数据。例如，从 MYSQL 中导出数据然后导入到 ClickHouse 中，反之亦然。
-
-TabSeparated 格式支持输出数据总值（当使用 WITH TOTALS） 以及极值（当 'extremes' 设置是1）。这种情况下，总值和极值输出在主数据的后面。主要的数据，总值，极值会以一个空行隔开，例如：
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated``
-```
-
-```
-2014-03-17      1406958
-2014-03-18      1383658
-2014-03-19      1405797
-2014-03-20      1353623
-2014-03-21      1245779
-2014-03-22      1031592
-2014-03-23      1046491
-
-0000-00-00      8873898
-
-2014-03-17      1031592
-2014-03-23      1406958
-```
-
-### 数据解析方式
-
-整数以十进制形式写入。数字在开头可以包含额外的 `+` 字符（解析时忽略，格式化时不记录）。非负数不能包含负号。 读取时，允许将空字符串解析为零，或者（对于带符号的类型）将仅包含负号的字符串解析为零。 不符合相应数据类型的数字可能会被解析为不同的数字，而不会显示错误消息。
-
-浮点数以十进制形式写入。点号用作小数点分隔符。支持指数等符号，如'inf'，'+ inf'，'-inf'和'nan'。 浮点数的输入可以以小数点开始或结束。
-格式化的时候，浮点数的精确度可能会丢失。
-解析的时候，没有严格需要去读取与机器可以表示的最接近的数值。
-
-日期会以 YYYY-MM-DD 格式写入和解析，但会以任何字符作为分隔符。
-带时间的日期会以 YYYY-MM-DD hh:mm:ss 格式写入和解析，但会以任何字符作为分隔符。
-这一切都发生在客户端或服务器启动时的系统时区（取决于哪一种格式的数据）。对于具有时间的日期，夏时制时间未指定。 因此，如果转储在夏令时中有时间，则转储不会明确地匹配数据，解析将选择两者之一。
-在读取操作期间，不正确的日期和具有时间的日期可以使用自然溢出或空日期和时间进行分析，而不会出现错误消息。
-
-有个例外情况，Unix 时间戳格式（10个十进制数字）也支持使用时间解析日期。结果不是时区相关的。格式 YYYY-MM-DD hh:mm:ss和 NNNNNNNNNN 会自动区分。
-
-字符串以反斜线转义的特殊字符输出。 以下转义序列用于输出：`\b`，`\f`，`\r`，`\n`，`\t`，`\0`，`\'`，`\\`。 解析还支持`\a`，`\v`和`\xHH`（十六进制转义字符）和任何`\c`字符，其中`c`是任何字符（这些序列被转换为`c`）。 因此，读取数据支持可以将换行符写为`\n`或`\`的格式，或者换行。例如，字符串 `Hello world` 在单词之间换行而不是空格可以解析为以下任何形式：
-
-```
-Hello\nworld
-
-Hello\
-world
-```
-
-第二种形式是支持的，因为 MySQL 读取 tab-separated 格式数据集的时候也会使用它。
-
-在 TabSeparated 格式中传递数据时需要转义的最小字符集为：Tab，换行符（LF）和反斜杠。
-
-只有一小组符号会被转义。你可以轻易地找到一个字符串值，但这不会正常在你的终端显示。
-
-数组写在方括号内的逗号分隔值列表中。 通常情况下，数组中的数字项目会被拼凑，但日期，带时间的日期以及字符串将使用与上面相同的转义规则用单引号引起来。
-
-[NULL](../query_language/syntax.md#null-literal) 将输出为 `\N`。
-
-<a name="tabseparatedraw"></a>
-
-## TabSeparatedRaw
-
-与 `TabSeparated` 格式不一样的是，行数据是不会被转义的。
-该格式仅适用于输出查询结果，但不适用于解析输入（将数据插入到表中）。
-
-这种格式也可以使用名称 `TSVRaw` 来表示。
-<a name="tabseparatedwithnames"></a>
-
-## TabSeparatedWithNames
-
-与 `TabSeparated` 格式不一样的是，第一行会显示列的名称。
-在解析过程中，第一行完全被忽略。您不能使用列名来确定其位置或检查其正确性。
-（未来可能会加入解析头行的功能）
-
-这种格式也可以使用名称 ` TSVWithNames` 来表示。
-<a name="tabseparatedwithnamesandtypes"></a>
-
-## TabSeparatedWithNamesAndTypes
-
-与 `TabSeparated` 格式不一样的是，第一行会显示列的名称，第二行会显示列的类型。
-在解析过程中，第一行和第二行完全被忽略。
-
-这种格式也可以使用名称 ` TSVWithNamesAndTypes` 来表示。
-<a name="tskv"></a>
-
-## TSKV
-
-与 `TabSeparated` 格式类似，但它输出的是 `name=value` 的格式。名称会和 `TabSeparated` 格式一样被转义，`=` 字符也会被转义。
-
-```
-SearchPhrase=   count()=8267016
-SearchPhrase=bathroom interior design    count()=2166
-SearchPhrase=yandex     count()=1655
-SearchPhrase=2014 spring fashion    count()=1549
-SearchPhrase=freeform photos       count()=1480
-SearchPhrase=angelina jolie    count()=1245
-SearchPhrase=omsk       count()=1112
-SearchPhrase=photos of dog breeds    count()=1091
-SearchPhrase=curtain designs        count()=1064
-SearchPhrase=baku       count()=1000
-```
-
-[NULL](../query_language/syntax.md#null-literal) 输出为 `\N`。
-
-``` sql
-SELECT * FROM t_null FORMAT TSKV
-```
-
-```
-x=1	y=\N
-```
-
-当有大量的小列时，这种格式是低效的，通常没有理由使用它。它被用于 Yandex 公司的一些部门。
-
-数据的输出和解析都支持这种格式。对于解析，任何顺序都支持不同列的值。可以省略某些值，用 `-` 表示， 它们被视为等于它们的默认值。在这种情况下，零和空行被用作默认值。作为默认值，不支持表中指定的复杂值。
-
-对于不带等号或值，可以用附加字段 `tskv` 来表示，这种在解析上是被允许的。这样的话该字段被忽略。
-<a name="csv"></a>
-
-## CSV
-
-按逗号分隔的数据格式([RFC](https://tools.ietf.org/html/rfc4180))。
-
-格式化的时候，行是用双引号括起来的。字符串中的双引号会以两个双引号输出，除此之外没有其他规则来做字符转义了。日期和时间也会以双引号包括。数字的输出不带引号。值由一个单独的字符隔开，这个字符默认是 `,`。行使用 Unix 换行符（LF）分隔。 数组序列化成 CSV 规则如下：首先将数组序列化为 TabSeparated 格式的字符串，然后将结果字符串用双引号包括输出到 CSV。CSV 格式的元组被序列化为单独的列（即它们在元组中的嵌套关系会丢失）。
-
-
-```
-clickhouse-client --format_csv_delimiter="|" --query="INSERT INTO test.csv FORMAT CSV" < data.csv
-```
-
-&ast;默认情况下间隔符是 `,` ，在[format_csv_delimiter](../operations/settings/settings.md#format_csv_delimiter)中可以了解更多间隔符配置。
-
-解析的时候，可以使用或不使用引号来解析所有值。支持双引号和单引号。行也可以不用引号排列。 在这种情况下，它们被解析为逗号或换行符（CR 或 LF）。在解析不带引号的行时，若违反 RFC 规则，会忽略前导和尾随的空格和制表符。 对于换行，全部支持 Unix（LF），Windows（CR LF）和 Mac OS Classic（CR LF）。
-
-`NULL` is formatted as `\N`.
-
-CSV 格式是和 TabSeparated 一样的方式输出总数和极值。
-
-## CSVWithNames
-
-会输出带头部行，和 `TabSeparatedWithNames` 一样。
-<a name="json"></a>
-
-## JSON
-
-以 JSON 格式输出数据。除了数据表之外，它还输出列名称和类型以及一些附加信息：输出行的总数以及在没有 LIMIT 时可以输出的行数。 例：
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON
-```
-
-```json
-{
-        "meta":
-        [
-                {
-                        "name": "SearchPhrase",
-                        "type": "String"
-                },
-                {
-                        "name": "c",
-                        "type": "UInt64"
-                }
-        ],
-
-        "data":
-        [
-                {
-                        "SearchPhrase": "",
-                        "c": "8267016"
-                },
-                {
-                        "SearchPhrase": "bathroom interior design",
-                        "c": "2166"
-                },
-                {
-                        "SearchPhrase": "yandex",
-                        "c": "1655"
-                },
-                {
-                        "SearchPhrase": "spring 2014 fashion",
-                        "c": "1549"
-                },
-                {
-                        "SearchPhrase": "freeform photos",
-                        "c": "1480"
-                }
-        ],
-
-        "totals":
-        {
-                "SearchPhrase": "",
-                "c": "8873898"
-        },
-
-        "extremes":
-        {
-                "min":
-                {
-                        "SearchPhrase": "",
-                        "c": "1480"
-                },
-                "max":
-                {
-                        "SearchPhrase": "",
-                        "c": "8267016"
-                }
-        },
-
-        "rows": 5,
-
-        "rows_before_limit_at_least": 141137
-}
-```
-
-JSON 与 JavaScript 兼容。为了确保这一点，一些字符被另外转义：斜线`/`被转义为`\/`; 替代的换行符 `U+2028` 和 `U+2029` 会打断一些浏览器解析，它们会被转义为 `\uXXXX`。 ASCII 控制字符被转义：退格，换页，换行，回车和水平制表符被替换为`\b`，`\f`，`\n`，`\r`，`\t` 作为使用`\uXXXX`序列的00-1F范围内的剩余字节。 无效的 UTF-8 序列更改为替换字符 ，因此输出文本将包含有效的 UTF-8 序列。 为了与 JavaScript 兼容，默认情况下，Int64 和 UInt64 整数用双引号引起来。要除去引号，可以将配置参数 output_format_json_quote_64bit_integers 设置为0。
-
-`rows` – 结果输出的行数。
-
-`rows_before_limit_at_least` 去掉 LIMIT 过滤后的最小行总数。 只会在查询包含 LIMIT 条件时输出。
-若查询包含 GROUP BY，rows_before_limit_at_least 就是去掉 LIMIT 后过滤后的准确行数。
-
-`totals` – 总值 （当使用 TOTALS 条件时）。
-
-`extremes` – 极值 （当 extremes 设置为 1时）。
-
-该格式仅适用于输出查询结果，但不适用于解析输入（将数据插入到表中）。
-
-ClickHouse 支持 [NULL](../query_language/syntax.md#null-literal), 在 JSON 格式中以 `null`  输出来表示.
-
-参考 JSONEachRow 格式。
-
-<a name="jsoncompact"></a>
-
-## JSONCompact
-
-与 JSON 格式不同的是它以数组的方式输出结果，而不是以结构体。
-
-示例：
-
-```json
-{
-        "meta":
-        [
-                {
-                        "name": "SearchPhrase",
-                        "type": "String"
-                },
-                {
-                        "name": "c",
-                        "type": "UInt64"
-                }
-        ],
-
-        "data":
-        [
-                ["", "8267016"],
-                ["bathroom interior design", "2166"],
-                ["yandex", "1655"],
-                ["fashion trends spring 2014", "1549"],
-                ["freeform photo", "1480"]
-        ],
-
-        "totals": ["","8873898"],
-
-        "extremes":
-        {
-                "min": ["","1480"],
-                "max": ["","8267016"]
-        },
-
-        "rows": 5,
-
-        "rows_before_limit_at_least": 141137
-}
-```
-
-这种格式仅仅适用于输出结果集，而不适用于解析（将数据插入到表中）。
-参考 `JSONEachRow` 格式。
-<a name="jsoneachrow"></a>
-
-## JSONEachRow
-
-将数据结果每一行以 JSON 结构体输出（换行分割 JSON 结构体）。
-
-```json
-{"SearchPhrase":"","count()":"8267016"}
-{"SearchPhrase": "bathroom interior design","count()": "2166"}
-{"SearchPhrase":"yandex","count()":"1655"}
-{"SearchPhrase":"2014 spring fashion","count()":"1549"}
-{"SearchPhrase":"freeform photo","count()":"1480"}
-{"SearchPhrase":"angelina jolie","count()":"1245"}
-{"SearchPhrase":"omsk","count()":"1112"}
-{"SearchPhrase":"photos of dog breeds","count()":"1091"}
-{"SearchPhrase":"curtain designs","count()":"1064"}
-{"SearchPhrase":"baku","count()":"1000"}
-```
-
-与 JSON 格式不同的是，没有替换无效的UTF-8序列。任何一组字节都可以在行中输出。这是必要的，因为这样数据可以被格式化而不会丢失任何信息。值的转义方式与JSON相同。
-
-对于解析，任何顺序都支持不同列的值。可以省略某些值 - 它们被视为等于它们的默认值。在这种情况下，零和空行被用作默认值。 作为默认值，不支持表中指定的复杂值。元素之间的空白字符被忽略。如果在对象之后放置逗号，它将被忽略。对象不一定必须用新行分隔。
-<a name="native"></a>
-
-## Native
-
-最高性能的格式。 据通过二进制格式的块进行写入和读取。对于每个块，该块中的行数，列数，列名称和类型以及列的部分将被相继记录。 换句话说，这种格式是 “列式”的 - 它不会将列转换为行。 这是用于在服务器之间进行交互的本地界面中使用的格式，用于使用命令行客户端和 C++ 客户端。
-
-您可以使用此格式快速生成只能由 ClickHouse DBMS 读取的格式。但自己处理这种格式是没有意义的。
-<a name="null"></a>
-
-## Null
-
-没有输出。但是，查询已处理完毕，并且在使用命令行客户端时，数据将传输到客户端。这仅用于测试，包括生产力测试。
-显然，这种格式只适用于输出，不适用于解析。
-<a name="pretty"></a>
-
-## Pretty
-
-将数据以表格形式输出，也可以使用 ANSI 转义字符在终端中设置颜色。
-它会绘制一个完整的表格，每行数据在终端中占用两行。
-每一个结果块都会以单独的表格输出。这是很有必要的，以便结果块不用缓冲结果输出（缓冲在可以预见结果集宽度的时候是很有必要的）。
-
-[NULL](../query_language/syntax.md#null-literal) 输出为 `ᴺᵁᴸᴸ`。
-
-``` sql
-SELECT * FROM t_null
-```
-
-```
-┌─x─┬────y─┐
-│ 1 │ ᴺᵁᴸᴸ │
-└───┴──────┘
-```
-
-为避免将太多数据传输到终端，只打印前10,000行。 如果行数大于或等于10,000，则会显示消息“Showed first 10 000”。
-该格式仅适用于输出查询结果，但不适用于解析输入（将数据插入到表中）。
-
-Pretty格式支持输出总值（当使用 WITH TOTALS 时）和极值（当 `extremes` 设置为1时）。 在这些情况下，总数值和极值在主数据之后以单独的表格形式输出。 示例（以 PrettyCompact 格式显示）：
-
-``` sql
-SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact
-```
-
-```
-┌──EventDate─┬───────c─┐
-│ 2014-03-17 │ 1406958 │
-│ 2014-03-18 │ 1383658 │
-│ 2014-03-19 │ 1405797 │
-│ 2014-03-20 │ 1353623 │
-│ 2014-03-21 │ 1245779 │
-│ 2014-03-22 │ 1031592 │
-│ 2014-03-23 │ 1046491 │
-└────────────┴─────────┘
-
-Totals:
-┌──EventDate─┬───────c─┐
-│ 0000-00-00 │ 8873898 │
-└────────────┴─────────┘
-
-Extremes:
-┌──EventDate─┬───────c─┐
-│ 2014-03-17 │ 1031592 │
-│ 2014-03-23 │ 1406958 │
-└────────────┴─────────┘
-```
-
-<a name="prettycompact"></a>
-
-## PrettyCompact
-
-与 `Pretty` 格式不一样的是，`PrettyCompact` 去掉了行之间的表格分割线，这样使得结果更加紧凑。这种格式会在交互命令行客户端下默认使用。
-<a name="prettycompactmonoblock"></a>
-
-## PrettyCompactMonoBlock
-
-与 `PrettyCompact` 格式不一样的是，它支持 10,000 行数据缓冲，然后输出在一个表格中，不会按照块来区分
-<a name="prettynoescapes"></a>
-
-## PrettyNoEscapes
-
-与 `Pretty` 格式不一样的是，它不使用 ANSI 字符转义， 这在浏览器显示数据以及在使用 `watch` 命令行工具是有必要的。
-
-示例：
-
-```bash
-watch -n1 "clickhouse-client --query='SELECT event, value FROM system.events FORMAT PrettyCompactNoEscapes'"
-```
-
-您可以使用 HTTP 接口来获取数据，显示在浏览器中。
-
-### PrettyCompactNoEscapes
-
-用法类似上述。
-
-### PrettySpaceNoEscapes
-
-用法类似上述。
-<a name="prettyspace"></a>
-
-## PrettySpace
-
-与 `PrettyCompact`(#prettycompact) 格式不一样的是，它使用空格来代替网格来显示数据。
-<a name="rowbinary"></a>
-
-## RowBinary
-
-以二进制格式逐行格式化和解析数据。行和值连续列出，没有分隔符。
-这种格式比 Native 格式效率低，因为它是基于行的。
-
-整数使用固定长度的小端表示法。 例如，UInt64 使用8个字节。
-DateTime 被表示为 UInt32 类型的Unix 时间戳值。
-Date 被表示为 UInt16 对象，它的值为 1970-01-01以来的天数。
-字符串表示为 varint 长度（无符号[LEB128](https://en.wikipedia.org/wiki/LEB128)），后跟字符串的字节数。
-FixedString 被简单地表示为一个字节序列。
-
-数组表示为 varint 长度（无符号[LEB128](https://en.wikipedia.org/wiki/LEB128)），后跟有序的数组元素。
-
-对于 [NULL](../query_language/syntax.md#null-literal) 的支持， 一个为 1 或 0 的字节会加在每个 [Nullable](../data_types/nullable.md#data_type-nullable) 值前面。如果为 1, 那么该值就是 `NULL`。 如果为 0，则不为 `NULL`。
-
-## Values
-
-在括号中打印每一行。行由逗号分隔。最后一行之后没有逗号。括号内的值也用逗号分隔。数字以十进制格式输出，不含引号。 数组以方括号输出。带有时间的字符串，日期和时间用引号包围输出。转义字符的解析规则与 [TabSeparated](#tabseparated) 格式类似。 在格式化过程中，不插入额外的空格，但在解析过程中，空格是被允许并跳过的（除了数组值之外的空格，这是不允许的）。[NULL](../query_language/syntax.md#null-literal) 为 `NULL`。
-
-以 Values 格式传递数据时需要转义的最小字符集是：单引号和反斜线。
-
-这是 `INSERT INTO t VALUES ...` 中可以使用的格式，但您也可以将其用于查询结果。
-
-<a name="vertical"></a>
-
-## Vertical
-
-使用指定的列名在单独的行上打印每个值。如果每行都包含大量列，则此格式便于打印一行或几行。
-
-[NULL](../query_language/syntax.md#null-literal) 输出为 `ᴺᵁᴸᴸ`。
-
-示例:
-
-``` sql
-SELECT * FROM t_null FORMAT Vertical
-```
-
-```
-Row 1:
-──────
-x: 1
-y: ᴺᵁᴸᴸ
-```
-
-该格式仅适用于输出查询结果，但不适用于解析输入（将数据插入到表中）。
-
-<a name="verticalraw"></a>
-
-## VerticalRaw
-
-和 `Vertical` 格式不同点在于，行是不会被转义的。
-这种格式仅仅适用于输出，但不适用于解析输入（将数据插入到表中）。
-
-示例:
-
-```
-:) SHOW CREATE TABLE geonames FORMAT VerticalRaw;
-Row 1:
-──────
-statement: CREATE TABLE default.geonames ( geonameid UInt32, date Date DEFAULT CAST('2017-12-08' AS Date)) ENGINE = MergeTree(date, geonameid, 8192)
-
-:) SELECT 'string with \'quotes\' and \t with some special \n characters' AS test FORMAT VerticalRaw;
-Row 1:
-──────
-test: string with 'quotes' and   with some special
- characters
-```
-
-Compare with the Vertical format:
-
-```
-:) SELECT 'string with \'quotes\' and \t with some special \n characters' AS test FORMAT Vertical;
-Row 1:
-──────
-test: string with \'quotes\' and \t with some special \n characters
-```
-
-<a name="xml"></a>
-
-## XML
-
-该格式仅适用于输出查询结果，但不适用于解析输入，示例：
-
-
-```xml
-<?xml version='1.0' encoding='UTF-8' ?>
-<result>
-        <meta>
-                <columns>
-                        <column>
-                                <name>SearchPhrase</name>
-                                <type>String</type>
-                        </column>
-                        <column>
-                                <name>count()</name>
-                                <type>UInt64</type>
-                        </column>
-                </columns>
-        </meta>
-        <data>
-                <row>
-                        <SearchPhrase></SearchPhrase>
-                        <field>8267016</field>
-                </row>
-                <row>
-                        <SearchPhrase>bathroom interior design</SearchPhrase>
-                        <field>2166</field>
-                </row>
-                <row>
-                        <SearchPhrase>yandex</SearchPhrase>
-                        <field>1655</field>
-                </row>
-                <row>
-                        <SearchPhrase>2014 spring fashion</SearchPhrase>
-                        <field>1549</field>
-                </row>
-                <row>
-                        <SearchPhrase>freeform photos</SearchPhrase>
-                        <field>1480</field>
-                </row>
-                <row>
-                        <SearchPhrase>angelina jolie</SearchPhrase>
-                        <field>1245</field>
-                </row>
-                <row>
-                        <SearchPhrase>omsk</SearchPhrase>
-                        <field>1112</field>
-                </row>
-                <row>
-                        <SearchPhrase>photos of dog breeds</SearchPhrase>
-                        <field>1091</field>
-                </row>
-                <row>
-                        <SearchPhrase>curtain designs</SearchPhrase>
-                        <field>1064</field>
-                </row>
-                <row>
-                        <SearchPhrase>baku</SearchPhrase>
-                        <field>1000</field>
-                </row>
-        </data>
-        <rows>10</rows>
-        <rows_before_limit_at_least>141137</rows_before_limit_at_least>
-</result>
-```
-
-如果列名称没有可接受的格式，则仅使用 `field` 作为元素名称。 通常，XML 结构遵循 JSON 结构。
-就像JSON一样，将无效的 UTF-8 字符都作替换，以便输出文本将包含有效的 UTF-8 字符序列。
-
-在字符串值中，字符 `<` 和 `＆` 被转义为 `<` 和 `＆`。
-
-数组输出为 `<array> <elem> Hello </ elem> <elem> World </ elem> ... </ array>`，元组输出为 `<tuple> <elem> Hello </ elem> <elem> World </ ELEM> ... </tuple>` 。
-
-<a name="format_capnproto"></a>
-
-## CapnProto
-
-Cap'n Proto 是一种二进制消息格式，类似 Protocol Buffers 和 Thriftis，但与 JSON 或 MessagePack 格式不一样。
-
-Cap'n Proto 消息格式是严格类型的，而不是自我描述，这意味着它们不需要外部的描述。这种格式可以实时地应用，并针对每个查询进行缓存。
-
-``` sql
-SELECT SearchPhrase, count() AS c FROM test.hits
-       GROUP BY SearchPhrase FORMAT CapnProto SETTINGS schema = 'schema:Message'
-```
-
-其中 `schema.capnp` 描述如下：
-
-```
-struct Message {
-  SearchPhrase @0 :Text;
-  c @1 :Uint64;
-}
-```
-
-格式文件存储的目录可以在服务配置中的[ format_schema_path ](../operations/server_settings/settings.md#server_settings-format_schema_path) 指定。
-
-Cap'n Proto 反序列化是很高效的，通常不会增加系统的负载。
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/formats/) <!--hide-->
diff --git a/docs/zh/interfaces/formats.md b/docs/zh/interfaces/formats.md
new file mode 120000
index 000000000..41a65ebe5
--- /dev/null
+++ b/docs/zh/interfaces/formats.md
@@ -0,0 +1 @@
+../../en/interfaces/formats.md
\ No newline at end of file
diff --git a/docs/zh/interfaces/http_interface.md b/docs/zh/interfaces/http_interface.md
index 8b150d059..853b31b45 100644
--- a/docs/zh/interfaces/http_interface.md
+++ b/docs/zh/interfaces/http_interface.md
@@ -1,4 +1,4 @@
-# HTTP 客户端
+# HTTP 接口
 
 HTTP 接口可以让你通过任何平台和编程语言来使用 ClickHouse。我们用 Java 和 Perl 以及 shell 脚本来访问它。在其他的部门中，HTTP 接口会用在 Perl，Python 以及 Go 中。HTTP 接口比 TCP 原生接口更为局限，但是却有更好的兼容性。
 
@@ -213,5 +213,3 @@ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&buffer_size=3000000&wa
 ```
 
 查询请求响应状态码和 HTTP 头被发送到客户端后，若发生查询处理出错，使用缓冲区可以避免这种情况的发生。在这种情况下，响应主体的结尾会写入一条错误消息，而在客户端，只能在解析阶段检测到该错误。
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/http_interface/) <!--hide-->
diff --git a/docs/zh/interfaces/index.md b/docs/zh/interfaces/index.md
index b5603f81e..4ff975527 100644
--- a/docs/zh/interfaces/index.md
+++ b/docs/zh/interfaces/index.md
@@ -1,9 +1,7 @@
 <a name="interfaces"></a>
 
-# 客户端
+# 接口
 
 为了探索 ClickHouse 的能力，如导入数据到表中，或做一些手动的查询，可以使用 clickhouse-client 命令行程序来完成
 
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/) <!--hide-->
diff --git a/docs/zh/interfaces/jdbc.md b/docs/zh/interfaces/jdbc.md
index 41bf39ab2..f4c8139ef 100644
--- a/docs/zh/interfaces/jdbc.md
+++ b/docs/zh/interfaces/jdbc.md
@@ -4,5 +4,3 @@
 
 - 三方提供的 JDBC 驱动 [ClickHouse-Native-JDBC](https://github.com/housepower/ClickHouse-Native-JDBC).
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/jdbc/) <!--hide-->
diff --git a/docs/zh/interfaces/tcp.md b/docs/zh/interfaces/tcp.md
index 742e252a2..1ff4270d6 100644
--- a/docs/zh/interfaces/tcp.md
+++ b/docs/zh/interfaces/tcp.md
@@ -1,6 +1,4 @@
-# 原生客户端接口（TCP）
+# 原生接口（TCP）
 
 TCP 原生接口用于 `clickhouse-client` 命令行，它可以在分布式查询执行中和服务器进行交互，并且可以用在 C++ 程序中。我们讲解只覆盖命令行客户端。
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/tcp/) <!--hide-->
diff --git a/docs/zh/interfaces/third-party_client_libraries.md b/docs/zh/interfaces/third-party_client_libraries.md
index c3cabeaca..7e9203beb 100644
--- a/docs/zh/interfaces/third-party_client_libraries.md
+++ b/docs/zh/interfaces/third-party_client_libraries.md
@@ -45,5 +45,3 @@
 - Nim
     - [nim-clickhouse](https://github.com/leonardoce/nim-clickhouse)
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/third-party_client_libraries/) <!--hide-->
diff --git a/docs/zh/interfaces/third-party_gui.md b/docs/zh/interfaces/third-party_gui.md
index e9f204052..db2a0f582 100644
--- a/docs/zh/interfaces/third-party_gui.md
+++ b/docs/zh/interfaces/third-party_gui.md
@@ -28,6 +28,7 @@ ClickHouse Web 界面 [Tabix](https://github.com/tabixio/tabix).
 - 快速查看列占用的空间。
 - 服务配置。
 
+The following features are planned for development:
 以下功能正在计划开发：
 - 数据库管理
 - 用户管理
@@ -36,5 +37,3 @@ ClickHouse Web 界面 [Tabix](https://github.com/tabixio/tabix).
 - 集群管理
 - 监控副本情况以及 Kafka 引擎表
 
-
-[来源文章](https://clickhouse.yandex/docs/zh/interfaces/third-party_gui/) <!--hide-->
diff --git a/docs/zh/query_language/table_functions/jdbc.md b/docs/zh/query_language/table_functions/jdbc.md
deleted file mode 120000
index 73bec80ca..000000000
--- a/docs/zh/query_language/table_functions/jdbc.md
+++ /dev/null
@@ -1 +0,0 @@
-../../../en/query_language/table_functions/jdbc.md
\ No newline at end of file
diff --git a/libs/libmemcpy/impl/README.md b/libs/libmemcpy/impl/README.md
index ac807ecd9..91e01d4a5 100644
--- a/libs/libmemcpy/impl/README.md
+++ b/libs/libmemcpy/impl/README.md
@@ -20,7 +20,7 @@ Reference
 
 [Using Block Prefetch for Optimized Memory Performance](http://files.rsdn.ru/23380/AMD_block_prefetch_paper.pdf)
 
-The article only focused on aligned huge memory copy. You need handle other conditions by your self.
+The artical only focused on aligned huge memory copy. You need handle other conditions by your self.
 
 
 Results
diff --git a/utils/travis/normal.sh b/utils/travis/normal.sh
index 7f45641d4..16482c3f1 100755
--- a/utils/travis/normal.sh
+++ b/utils/travis/normal.sh
@@ -32,7 +32,7 @@ cmake $CUR_DIR/../.. -DCMAKE_CXX_COMPILER=`which $DEB_CXX $CXX` -DCMAKE_C_COMPIL
     `# Use all possible contrib libs from system` \
     -DUNBUNDLED=1 \
     `# Disable all features` \
-    -DENABLE_CAPNP=0 -DENABLE_RDKAFKA=0 -DENABLE_EMBEDDED_COMPILER=0 -DUSE_INTERNAL_LLVM_LIBRARY=0 -DENABLE_JEMALLOC=0 -DENABLE_UNWIND=0 -DENABLE_MYSQL=0 -DENABLE_POCO_ODBC=0 -DENABLE_ODBC=0 $CMAKE_FLAGS
+    -DENABLE_CAPNP=0 -DENABLE_RDKAFKA=0 -DENABLE_EMBEDDED_COMPILER=0 -DENABLE_JEMALLOC=0 -DENABLE_UNWIND=0 -DENABLE_MYSQL=0 -DENABLE_POCO_ODBC=0 -DENABLE_ODBC=0 -DUSE_INTERNAL_LLVM_LIBRARY=0 $CMAKE_FLAGS
 
 ninja clickhouse-bundle
 
diff --git a/website/gulpfile.js b/website/gulpfile.js
index ca254bf68..09c617ecc 100644
--- a/website/gulpfile.js
+++ b/website/gulpfile.js
@@ -26,7 +26,6 @@ var paths = {
     docstxt: ['docs/**/*.txt', 'docs/redirects.conf'],
     docsjson: ['docs/**/*.json'],
     docsxml: ['docs/**/*.xml'],
-    docspdf: ['docs/**/*.pdf'],
     docssitemap: ['sitemap.xml', 'sitemap_static.xml'],
     scripts: [
         '**/*.js',
@@ -79,11 +78,6 @@ gulp.task('docsxml', ['docs'], function () {
         .pipe(gulp.dest(outputDir + '/docs'))
 });
 
-gulp.task('docspdf', ['docs'], function () {
-    return gulp.src(paths.docspdf)
-        .pipe(gulp.dest(outputDir + '/docs'))
-});
-
 gulp.task('docssitemap', [], function () {
     return gulp.src(paths.docssitemap)
         .pipe(gulp.dest(outputDir + '/docs'))
@@ -99,7 +93,7 @@ gulp.task('robotstxt', [], function () {
         .pipe(gulp.dest(outputDir))
 });
 
-gulp.task('htmls', ['docs', 'docstxt', 'docsjson', 'docsxml', 'docspdf', 'docssitemap'], function () {
+gulp.task('htmls', ['docs', 'docstxt', 'docsjson', 'docsxml', 'docssitemap'], function () {
     return gulp.src(paths.htmls)
         .pipe(htmlmin({collapseWhitespace: true}))
         .pipe(minifyInline())
diff --git a/website/index.css b/website/index.css
index 8a51c5de0..fb626a2d8 100644
--- a/website/index.css
+++ b/website/index.css
@@ -65,7 +65,7 @@
 
 body {
     background: #fff;
-    font: 400 14pt/200% 'Yandex Sans Text Web', Arial, sans-serif;
+    font: 300 14pt/200% 'Yandex Sans Text Web', Arial, sans-serif;
     margin: 0;
     padding: 0;
 }
diff --git a/website/nginx/nginx.conf b/website/nginx/nginx.conf
index 24bfc1cca..0227b8611 100644
--- a/website/nginx/nginx.conf
+++ b/website/nginx/nginx.conf
@@ -32,10 +32,6 @@ http {
     gzip_comp_level 5;
     gzip_min_length 256;
 
-    add_header X-Content-Type-Options nosniff;
-    add_header X-Frame-Options DENY;
-    add_header X-XSS-Protection "1; mode=block";
-
     include /etc/nginx/conf.d/*.conf;
     include /etc/nginx/sites-enabled/*;
 }
